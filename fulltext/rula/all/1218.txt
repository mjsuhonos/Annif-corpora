Ryerson University

Digital Commons @ Ryerson
Theses and dissertations

1-1-2011

Performance measurement and analytic modeling of a web application
Yasir Shoaib
Ryerson University

Follow this and additional works at: http://digitalcommons.ryerson.ca/dissertations Part of the Electrical and Computer Engineering Commons Recommended Citation
Shoaib, Yasir, "Performance measurement and analytic modeling of a web application" (2011). Theses and dissertations. Paper 748.

This Thesis is brought to you for free and open access by Digital Commons @ Ryerson. It has been accepted for inclusion in Theses and dissertations by an authorized administrator of Digital Commons @ Ryerson. For more information, please contact bcameron@ryerson.ca.

PERFORMANCE MEASUREMENT AND ANALYTIC MODELING OF A WEB APPLICATION
by

Yasir Shoaib B.A.Sc., University of Toronto, 2008

A thesis presented to Ryerson University in partial fulfillment of the requirements for the degree of Master of Applied Science (M.A.Sc.) in the Program of Electrical and Computer Engineering

Toronto, Ontario, Canada, 2011 © Copyright by Yasir Shoaib 2011 All Rights Reserved

Author's Declaration
I hereby declare that I am the sole author of this thesis or dissertation.

I authorize Ryerson University to lend this thesis or dissertation to other institutions or individuals for the purpose of scholarly research.

Yasir Shoaib

I further authorize Ryerson University to reproduce this thesis or dissertation by photocopying or by other means, in total or in part, at the request of other institutions or individuals for the purpose of scholarly research.

Yasir Shoaib

ii

PERFORMANCE MEASUREMENT AND ANALYTIC MODELING OF A WEB APPLICATION
Yasir Shoaib
Master of Applied Science (M.A.Sc.) Electrical and Computer Engineering Ryerson University, 2011

Abstract
The performance characteristics such as throughput, resource utilization and response time of a system can be determined through measurement, simulation modeling and analytic modeling. In this thesis, measurement and analytic modeling approaches are applied to study the performance of a Apache-PHP-PostgreSQL web application. Layered Queueing Network (LQN) analytic modeling has been used to represent the system's performance model. The measurements found from load testing are compared with model analysis results for model validation. This thesis aims to show that LQN performance models are versatile enough to allow development of highly granular and easily modifiable models of PHP-based web applications and furthermore are capable of performance prediction with sufficiently high accuracy. Lastly, the thesis also describes utilities and methods used for load testing and determination of service demand parameters in our research work which would aid in shortening time required in development and study of performance models of similar systems.

iii

Acknowledgements
Throughout the course of my graduate studies, my supervisor, Dr. Olivia Das, has been a source of guidance for me. I respect her kind efforts and would like to express my sincere thanks to her. Her mentorship role and overall support has been a contributing factor in the successful completion of my thesis. I would also like to thank the members of my thesis defence committee, Dr. Kaamran Raahemifar, Dr. Muhammad Jaseemuddin and Dr. Truman Yang, to have spent their valuable time reviewing my thesis and providing me with insightful comments. Thanks to anonymous reviewers for their feedback on the papers submitted, which form part of this research work. Also, thanks to Bruce Derwin for kindly helping me with the initial lab equipment setup, which I have used for most of the research. Furthermore, thanks to Ryerson University for their funding support both in the form of awards and graduate assistantship during my graduate work. Finally, I much appreciate the support I have received from each and every member of my family. Initially, they encouraged me when I considered graduate school. At times when the journey ahead was difficult, my parents provided me a sense of hopefulness and were a source of inspiration for me. Alongside, my brother and sister brought fun and happiness back to life, good enough to make matters appear optimistic. Thanks for being understanding and at the same time collectively making the process manageable for me.

I dedicate this thesis to Ammi, Papa, Nabila and Haris.

iv

Table of Contents
Chapter 1 Introduction....................................................................................................................................................... 1 1.1 1.2 1.3 Motivation ............................................................................................................................................................... 1 Apache Web Server and LAMP/LAPP Stack.............................................................................................. 2 Web Applications and Performance Modeling ........................................................................................ 3 Overview ........................................................................................................................................................ 4 Web Application Performance Evaluation & Challenges ........................................................... 4

1.3.1 1.3.2 1.4 1.5 1.6 1.7 2.1 2.2

Research Overview .............................................................................................................................................. 6 Related Works ....................................................................................................................................................... 7 Thesis Contributions ......................................................................................................................................... 11 Thesis Outline ...................................................................................................................................................... 11 Software Performance Engineering (SPE)............................................................................................... 13 Performance Evaluation .................................................................................................................................. 14 Measurements............................................................................................................................................ 15 Analytical Performance Modeling ..................................................................................................... 16 Markov Chains ...................................................................................................................................... 16 Queueing Networks ............................................................................................................................ 17 Petri Nets ................................................................................................................................................. 21 Layered Queueing Networks........................................................................................................... 23

Chapter 2 Background ...................................................................................................................................................... 13

2.2.1 2.2.2 2.2.2.1 2.2.2.2 2.2.2.3 2.2.2.4 2.3 3.1 3.2 3.3

Conclusions ........................................................................................................................................................... 25 MyBikeRoutes-OSM Web Application ....................................................................................................... 26 Process Flow Diagram ...................................................................................................................................... 27 LAPP Performance Aspects ............................................................................................................................ 28 Apache Configuration ............................................................................................................................. 28 PostgreSQL configuration ..................................................................................................................... 29

Chapter 3 Web Application............................................................................................................................................. 26

3.3.1 3.3.2 3.4 4.1

Conclusions ........................................................................................................................................................... 30 Load Testing ......................................................................................................................................................... 31 Remote-Test Setup and Machine Configuration .......................................................................... 33 Base-Scenario and Test Implementation ........................................................................................ 34 v

Chapter 4 Performance Evaluation ............................................................................................................................. 31 4.1.1 4.1.2

4.2

LQN Performance Modeling........................................................................................................................... 37 Base-Scenario Model ............................................................................................................................... 37 Discovering Service Demand Parameters ...................................................................................... 40 Service Demands from Utilization Law ...................................................................................... 40

4.2.1 4.2.2 4.2.2.1 4.3 5.1

Conclusions ........................................................................................................................................................... 44 Results and Model Validation........................................................................................................................ 46 Load Test Results: Base-Scenario ...................................................................................................... 46 LQN Results: Base-Scenario ................................................................................................................. 48 Base-Scenario model validation ......................................................................................................... 50 Overview ...................................................................................................................................................... 51 Performance Analysis ............................................................................................................................. 52 Overview ...................................................................................................................................................... 54 XDeb-Scenario LQN Model.................................................................................................................... 54 Service Demand Parameters using XDebug .................................................................................. 55 XDeb-Scenario Results ........................................................................................................................... 57

Chapter 5 Results and Analysis..................................................................................................................................... 46 5.1.1 5.1.2 5.1.3 5.2 5.2.1 5.2.2 5.3 5.3.1 5.3.2 5.3.3 5.3.4 5.4 6.1 6.2 6.3 6.4

Attaining Performance Objectives .............................................................................................................. 50

Performance Modeling and XDebug ........................................................................................................... 54

Conclusions ........................................................................................................................................................... 60 Summary ................................................................................................................................................................ 61 Limitations ............................................................................................................................................................ 62 Future Work ......................................................................................................................................................... 62 Conclusions ........................................................................................................................................................... 63

Chapter 6 Conclusions ...................................................................................................................................................... 61

References ................................................................................................................................................................................ 64 Appendices ............................................................................................................................................................................... 69

vi

List of Tables
Table 1: Load Test: Hardware and Software Specification .................................................................................. 34 Table 2: Request classes ..................................................................................................................................................... 36 Table 3: Service Demand Parameters for Base-Scenario Model ....................................................................... 44 Table 4: LQN entries/activities and request class demands ............................................................................... 44 Table 5: Measurement Results - Base-Scenario ........................................................................................................ 46 Table 6: LQN Model Results - Base-Scenario ............................................................................................................. 48 Table 7: Service Demand Parameters for Model1 (XDebug) ............................................................................... 57 Table 8: LQN Model Results ­ XDeb-Scenario ........................................................................................................... 58

vii

List of Figures
Figure 1: An Example Markov Chain ............................................................................................................................ 17 Figure 2: A Simple Open Queueing Network with Arriving and Departing Customers ........................... 18 Figure 3: A Simple Closed Queueing Network with 30 customers ................................................................... 18 Figure 4: PN Example1 - Representation of a Queueing System (Based on [45] and [51]).................... 22 Figure 5: PN Example1 - After transition "Service" fires (Based on [45] and [51]) .................................. 22 Figure 6: Example LQN Model .......................................................................................................................................... 23 Figure 7: MyBikeRoutes-OSM Process Flow Diagram ............................................................................................ 27 Figure 8: Performance Modeling Flow Chart ............................................................................................................. 32 Figure 9: Remote-Test Setup ............................................................................................................................................ 33 Figure 10: Base Scenario Sequence Diagram ............................................................................................................. 35 Figure 11: Base-Scenario LQN Model ............................................................................................................................ 37 Figure 12: Discovering Service Demands - Flow Chart.......................................................................................... 41 Figure 13: Measurements Base-Scenario (Throughput vs. Users) ................................................................... 47 Figure 14: Measurements Base-Scenario (Response Time vs. Users) ............................................................. 47 Figure 15: LQN Base-Scenario (Throughput vs. Users) ......................................................................................... 49 Figure 16: LQN Base-Scenario (Response Time vs. Users) .................................................................................. 49 Figure 17: LQN Base-Scenario (pApache Utilization) ............................................................................................ 50 Figure 18: Performance Analysis (Throughput vs. Users) ................................................................................... 52 Figure 19: Performance Analysis (Response Time vs. Users) ­ 60 users ...................................................... 52 Figure 20: Performance Analysis (pServer Utilization) ........................................................................................ 53 Figure 21: XDeb-Scenario LQN Model........................................................................................................................... 55 Figure 22: Call Graph from XDebug (Routing1)........................................................................................................ 56 Figure 23: Flat Profile from XDebug (reqRouting1) ............................................................................................... 57 Figure 24: LQN XDeb-Scenario (Throughput vs. Users)........................................................................................ 58 Figure 25: LQN XDeb-Scenario (Response Time vs. Users) ................................................................................. 59 Figure 26: LQN XDeb-Scenario (pApache Utilization vs. Users) ........................................................................ 59

viii

List of Appendices
Appendix 1: Base-Scenario LQN model ........................................................................................................................ 69 Appendix 2: XDeb-Scenario (XDebug parameters) ................................................................................................. 74 Appendix 3: SeparateDB-Scenario ................................................................................................................................. 78

ix

Chapter 1 Introduction
1.1 Motivation
Internet-users commonly interact with websites, many of which are dynamic in nature. These sites generate content to suit user requests instead of only serving static web pages. Due to the functionality and interactivity provided by these dynamic websites they are more appropriately considered as Web Applications [23]. Alongside with delivering the required functionality, these web applications need to be quick and responsive enough such that users do not find their web experience unpleasant. From one's personal experience, it is readily realized that sites that take long to respond are unpopular. With growing internet user population, which is expected to reach 2.2 billion in 2013, and the growing e-Commerce market [29], the future will likely see more of business web presence in the form of web applications. However, if only functional characteristics are considered then web applications will seriously suffer in performance. Based on a study of online buyers by Forrester Consulting, 40% of the customers would leave a site if the web page load is longer than three seconds and that poor performance is a contributing factor to "shopper dissatisfaction and site abandonment" [24], [52]. If the performance is poor, then customers are lost [30], which contributes to lost revenues and builds up a bad reputation for the organization [24], [52]. Thus, performance is a vital term in the success equation of a web application ­ and in general for any software. To directly assess if an application will meet the required performance objectives based on available resources, for the purposes of capacity planning [47], a measurement-based approach is adopted. The behavior of the system under given customer workload can provide results which can 1

help identify performance bottlenecks [31]. Performance modeling, which uses performance models, also finds use in capacity planning by means of predicting system's performance and by pinpointing system bottlenecks. Other uses of modeling include capacity provisioning ­ i.e. allocating and preparing resources to handle the demands ­ and finding application configuration parameters that meet the desired objectives [10]. In this thesis, we study the performance of a web application through measurement and modeling as the approaches of performance evaluation. In particular we concentrate our work on analytical performance modeling approach where mathematical equations are used for solving the models. The work of this thesis has been published partly in [53] and [32]. Following is an outline of this chapter. The Apache Web server, which hosts the web application under study, is introduced in section 1.2. A general introduction to web applications and challenges faced in evaluating performance of such systems is provided in section 1.3. This is followed by the research overview in section 1.4. The related works in section 1.5 lists important works done by researchers on web application performance modeling. The contributions of this work are presented in section 1.6. The thesis outline is provided in section 1.7.

1.2 Apache Web Server and LAMP/LAPP Stack
Apache1 is the most widely used HTTP Web server [41]. Based on the NetCraft February 2010 survey, as stated in [41], Apache server is used for hosting 54% of the web sites. This popular server is known more commonly in the context of "LAMP stack" [42]. LAMP refers to combination of Linux, Apache, MySQL and PHP/Perl/Python applications [41]. Each stack layer brings about its strengths and advantages, where the base foundation is formed by Linux, a robust Operating System (OS) on which other applications run. Linux has many flavors and few of those are Fedora,

1

http://httpd.apache.org/

2

openSUSE, Debian, and Ubuntu. MySQL is popular backend database and PHP/Perl/Python are powerful scripting languages. The LAMP stack can be modified by replacing its components. For example, WAMP would refer to the configuration on a Windows OS (a commercial OS). The LAPP stack would refer to a configuration with PostgreSQL2 database instead of MySQL. In this work, the LAPP stack running Ubuntu Linux with PHP3 scripting support is used. Most of the aforementioned and subsequent discussions for LAMP also apply to the LAPP stack. LAMP has mainly found use in modest web systems, however, recent trends has been geared towards their use in development of enterprise applications, which has seen infiltration of technologies from organizations like Microsoft and Sun Microsystems (Java) [42]. According to [42], firms such as Google, Yahoo, Lufthansa and Sabre have already embarked on a journey of largescale LAMP web application development. It is interesting to note that the popularity of such application configurations is not solely based on their functionality and robustness, although it is wise to assume that such traits do also significantly contribute to the reputation established. The reasons are also due to the fact that these applications are Open Source ­ allowing for customization ­ and are mostly Free [42]. It is the popularity of LAMP technologies (or LAPP) in web development that has been a significant motivation for studying the performance aspects of such systems through a web application in this thesis.

1.3 Web Applications and Performance Modeling
This section describes in brief the communication that occurs to access web applications with mention of different components that form a web system. Following this, the performance

2 3

http://www.postgresql.org/ http://www.php.net/

3

modeling aspects that relate to web application and the commonly used modeling approaches are outlined. 1.3.1 Overview Web applications are multi-tier distributed systems using the Internet as a medium to facilitate the communication between the Browser (client) and the Web server processes. The client and the server processes each execute on their dedicated hardware. When HTTP requests are sent via the Browser, it is first received by the Web Server. After parsing the request, the Web Server decides on the next processing steps. Static-content such as HTML, images and client-scripts (viz. JavaScript) do not need further processing at the server-end and if requested, are sent in an HTTP response back to the client from the Web server [23]. If further processing is required, as in the case of dynamic-content such as PHP, ASP .NET and Java EE, the Web Server furthers the request to the Application Server (or "Language Runtime" [36]). After the processing and generation of the dynamic content, the Application Server replies to the Web Server which sends the reply back to the Client. For data retrieval or modifications, the Application Server executes queries at the backend-Database server, then processes the received query results and finally sends the reply back. 1.3.2 Web Application Performance Evaluation & Challenges For performance modeling of web application and similar distributed systems, behaviours such as contention of resources and simultaneous resource possession need to be depicted in a performance model. Contention is caused due to the sharing of resources, where processes compete amongst themselves to receive service from a particular resource. Scheduling disciplines viz. First Come First Served (FCFS), Last Come First Served (LCFS) or Processing Sharing (PS) etc., define how processes/jobs shall gain access to the resources. When resource possession is not gained right away, the processes are placed in a queue and serviced based on the scheduling discipline.

4

Contention due to waiting for both hardware resources (hardware contention) and software processes (software contention) contribute to the response time of a web system [35]. Simultaneous resource possession is seen when a particular job requires access to multiple resources, as is the case with Remote Procedure Call (RPC), where client is blocked during the call but possesses both of self and server resources [12]. In RPC, a client requests service from a remote server process by issuing a procedure call. For performance modeling, use of well known Queueing Network (QN) models is very common. The latter are network of processing stations with their respective queues, where customers traverse through each node to receive service. Solving these models provide performance metrics such as response time, throughput and resource utilization and serve the purpose of helping comprehend the system behaviour for given user scenarios and a system under different workloads. However, basic QN models fall short in being able to account for software contention as seen in software servers [28]. If effects of queueing due to software contention are ignored then response times are understated leading to inaccurate results. The QNs depict software as customers only, whereas software servers behave at times as server processes when they are serving their client requests and behave other times as client processes themselves when requesting service from other servers (such as database) [28]. Furthermore, aspects such as parallel software execution that is seen through creation of a child process from an existing process ( fork) cannot be directly represented [39]. Layered Queueing Networks (LQN) [5] analytical models are an extended form of QNs and are designed to eliminate the aforementioned shortcomings of the latter. In case of RPC, the queueing that occurs at lower layers due to software contention is included in the upper layer response time of an LQN model [5]. Furthermore, they can also model nested RPCs [12]. The 5

parallel execution of software and servers which send "early reply" [12] (i.e. some processing at the server is to happen in second phase after reply is sent to the client) can also be modeled [28]. LQNs are ideal for representing the interactions and intricacies of multi-tier application and this thesis therefore uses LQN for performance modeling.

1.4 Research Overview
This thesis applies measurement and analytic modeling approaches to study the performance of a web application. The application runs on Apache HTTP Web server with PHP scripting support and PostgreSQL backend database. LQN analytic modeling has been used to represent the system's performance model. The measurements, which are found from load testing, are initially used for model parameterization. A performance objective is subsequently set such that users do not experience excessive delays when using the application. After carrying out model evaluation, the model validation is done by comparison of the results with those of the measurements done earlier. With average error of 3.77% for throughput and 12.15% for response times the model is shown to capture the web application's performance. From model results, the bottleneck resource is identified to be the Application server machine. To ease the bottleneck, model analysis for various configurations is performed such as to meet the performance objective. Analysis results predict that using a quad-processor is the best option and would meet the performance objective. Through this thesis it is seen that LQN performance models are versatile enough to allow development of highly granular and easily modifiable models of PHP-based web applications and furthermore are capable of performance prediction with sufficiently high accuracy. In addition, the thesis describes utilities and methods used for load testing and determination of service demand parameters in our research work which would aid in easing and shortening the time required in development and study of performance models of similar PHP web systems. 6

1.5 Related Works
One of the earlier works pertaining to web system performance analysis is done by Slothouber [33], where a four-station open QN model ­ consisting of Client, Web server and Network ­of a simple file Web server has been presented and analyzed. The purpose of the work was to have a model represent the server and network relationship. Furthermore, the model was extended to evaluate the effects of having multi-servers. Menasce [35] models the software and hardware architecture of single-tier Web server using Markov Chains and Queueing Networks, respectively. The article describes how software contention can influence web system performance and shows the benefits of performance models in dynamic configuration of such systems. The effects of the configuration of server software architecture based on processing model and the pool-size behaviour are also described. For further discussion on performance aspects relating to server software architecture, refer to section 3.3.1. Kounev and Buchmann [14] describe a closed queueing model of SPECjAppServer2002 (J2EE) benchmark comprising of Client, Application Server Cluster, Database Server and Production Line Stations. The paper explains how service demands ­ which represent the demands that requests place on the computing resources and serve as inputs to the model ­ were obtained through use of Operational Laws of QNs. The model was evaluated for low, medium and high workloads represented by 130, 260 and 350 concurrent users, respectively. Through measurements the model was validated and the results show a very high accuracy of performance prediction with average error of 2% for throughput, 6% for CPU utilization and 18% for response time. Urgaonkar et al. in [10] present a closed QN model of multi-tier internet applications which considers caching, concurrency limits and multiple session-based class requests. The work initially presents a basic model which is enhanced further to incorporate concurrency limits and other 7

features. Furthermore, the model is validated through two J2EE applications, RUBiS (similar to eBay) and RUBBoS (a bulletin-board benchmark) that execute on a Linux server cluster. The paper also describes how the model has been used in the context of capacity provisioning with fluctuating demands ("dynamic capacity provisioning") [10]. Liu et al. [38] describe a closed QN model of a 3-tiered web application comprising of Apache Web server, Tomcat Application server and MySQL Database server. To model the concurrency limits ­ such as maximum number of concurrently running threads/processes ­ of Apache and MySQL servers, multi-station queues are used. To solve the model, approximate MeanValue Analysis (MVA) algorithm is applied. TPC-W client emulator is used for load testing and the results are used for model validation. For further discussion on concurrency limits relating to Application and Database server, refer to section 3.3. The above works are very useful representations of web system modeling and their measurements; however, with respect to modeling they encounter the same drawbacks as those of QNs, which LQN overcomes through easily representing large software and hardware systems while also incorporating aspects such as software contention that affect the performance. LQN performance models for web systems have been studied in [6], [7], [8] and [9]. Tiwari and Mynampati [9] modeled the SPECjAppServer2001 EJB benchmark as a simple LQN model, validated by measurements obtained from an earlier work by Kounev and Buchmann [44]. Ufimtsev and Murphy [8] model a JavaEE ECPerf benchmark based on LQN EJB templates [7]. However, as Urgaonkar et al. [10] indicate, most of these works have focused on Java Enterprise applications. In this work we utilize the versatility of LQN and model an Apache-PHP web application with PostgreSQL backend-database to study the system's performance behaviour. Some other useful works based on LQN are mentioned below (e.g. [5], [48], [49], [46], [50]).

8

Franks et al. [5] present LQN model of an ecommerce web application with the purpose to explain the LQN formalism through examples. The model comprises of Bookstore and Database Server machines and the requests originate from the Customers and the Administrator. LQN activities [12], which are able to portray detailed precedence interactions including parallel executions, viz. And-forks, Or-forks and Join-forks, have been used in the model. The results from model evaluation are also displayed. Efforts have also been made to automate LQN model creation through system design UML diagrams, e.g. Tribastone et al. [48] use UML technologies to represent model of a mobile payment system, where the UML model is converted to an LQN performance model. The analysis of the latter shows one of the servers running on a uni-processor as the bottleneck resource. Through model evaluation, it is found that using a dual-processor system instead of the uni-processor alleviates the system bottleneck. Wu and Woodside [49] use a linkage of LQN component sub-models to represent a "Management Information System". The models are originally created through "Component-Based Modeling Language" (CBML) and converted to LQN [49]. The purpose is to have a repository of submodels to be used and integrated in creation of complete models. The system represented comprises of Clients, Web server, Application server (internally includes reporting and caching servers) and Database Server. Different configurations aspects such as number of threads and server copies (replicas) are varied and solved through the model. The response time results are used to determine which configuration in the most scalable, in order to support more clients with acceptable response times. Omari et al. [46] demonstrate the use of replication in LQN models. Replicas are copies of LQN elements where instead of manually adding each element copy, the replicas can be used to indicate such characteristics in the model. The paper provides few quite useful examples of LQN 9

models such as: a web server system, a search engine, Management Information System (MIS) and an Air Traffic Control System. The results from some of models are also presented. Xu et al. [50] model a J2EE bank application using LQN templates and perform model validation. The test system for measurements comprises of client load generator, EJB Application server and Database server machines. Tools like JProbe and sar were used for profiling and obtaining usage information of resources such as CPU, network and disk. For tests, the beans were either accessed sequentially or in random order. The validation results for higher client numbers reported errors ranging from 6.2% to 23.9% (sequential) and from 2.1% to 24.5% (random). It is interesting to realize that many works have been done with regards to modeling but very few compare the model results with actual measurements. Alongside LQN modeling, only Tiwari and Mynampati [9] and Xu et al. [50] from above, use measurements to perform model validation, and these are considered as similar pieces of works to ours. Also, similar to our work, Dilley et al. [34] and Pastsyak et al. [4] have modeled a CGI Web server and an Apache-PHP-MySQL system, respectively, using LQN. The CGI Web server model [34] incorporates serving of both dynamic and static contents by the server, however no database tier is considered in the study. The response time from the model evaluation is found to match closely with response time measurements over a period of two months. Measurement data collected through custom instrumentation serve as input parameters to the LQN model and are also used for model-validation. The web system LQN model by Pastsyak et al. [4] is evaluated for 40 users and compared with the load test results, which show the successful prediction of performance by the model. The system consists of two Web servers, a Load balancer and a Database server. Other performance modeling formalisms such as Stochastic Process Algebra (SPA) and Stochastic Petri Nets (SPN) have also been evaluated in this paper.

10

However, in contrast to these two aforementioned papers ([34] and [4]), our work uses LQN "activities" to define in detail the web software entities and the precedence of interactions between them. The model is validated through measurements done on the application. A detailed methodology is provided for load testing and to obtain service demands with mention of the utilities used to make the work of performance modeling easier for systems similar to ours.

1.6 Thesis Contributions
Following are the contributions of this thesis: 1. Performance evaluation of a Linux-Apache-PHP-PostgreSQL (LAPP) web application: a. Performance Measurements has been done on the web application. b. LQN performance models using activities have been developed and analyzed for the web application to identify the bottleneck resource. c. Validation of the performance model results through the measurements done. d. Model Analysis has been performed to ease the bottleneck and to meet the performance objective. 2. Utilities and methodologies that have been adopted for measurement and determination of service demands for the model are described in detail, which will help for the purposes of performance evaluation of similar web systems.

1.7 Thesis Outline
This thesis is organized as follows: Chapter 2 is a background on measurements and analytic performance modeling approaches. The chapter includes an introduction to LQN modeling formalism. Chapter 3 describes the web application whose performance analysis has been conducted. Performance aspects relating to LAPP server are also discussed. The methods adopted to perform the measurements and obtain the service demand parameters are described in Chapter

11

4. The base LQN model of the application is discussed and the service demand parameters are presented. Chapter 5 presents the performance metrics obtained from the model evaluation. Furthermore, the analysis of results and model validation is performed. Chapter 6 presents the conclusions of the research done in this thesis.

12

Chapter 2 Background
The study of a system's performance relies on performance evaluation methods and this chapter is a short background on this topic. Software Performance Engineering (SPE) technique, which helps in development of software that meet performance objectives is briefly discussed in Section 2.1. In section 2.2, the significance of performance evaluation and the comparison of the different methods of evaluation are made. Queueuing Networks (QN) and Layered Queueing Networks (LQN) modeling are also described in detail.

2.1 Software Performance Engineering (SPE)
Just as software development, web development also follows a Software Development Lifecycle (SDLC). As performance is important, it should be considered during the application development from the get-go. Software Performance Engineering (SPE), which is a well-defined systematic and quantitative process integrates performance considerations in the SDLC, relying on a proactive rather than a reactive approach, where the former keeps track of system performance and mitigates problems before they happen, whereas latter is a restricted response once problems are encountered [25]. Following of the SPE process allows the development of software that is able to meet its performance objectives, achieved by means of a performance modeling. Before a system is available, performance models can be developed and evaluated to determine if performance objectives ­ which form part of the Service Level Agreements (SLA) ­ are being satisfied. Once the SLA requirements are met, then the software development can commence. The modeling process should continue alongside with the development until deployment. The effect on performance by a proposed design change should be evaluated by the model and once the design is finalized, the model should be updated to reflect the actual modifications made. 13

SPE also uses strategies to guide the stakeholders in the modeling process. One of SPE strategies is Simple-Model strategy, where a simple model is constructed in the early stages of software development and then evaluated to obtain early performance indicators. Another strategy is Adapt-to-Precision which is useful when adequate information of system is available and a more detailed model can be made, possibly by extending model already developed using the SimpleModel strategy. In this work, we apply SPE and use its strategies. In the following section, the performance evaluation approaches, which are an important part of SPE technique, are discussed.

2.2 Performance Evaluation
To comprehend and locate performance problems, an assessment of the system's performance ­ also known as Performance Evaluation ­ is necessary. A performance evaluation provides metrics such as throughput, resource-utilization and response time to discover the system's bottlenecks, which altogether aid in finding performance-related problems in the design [1], [2]. Measurement, Simulation modeling and Analytical modeling are the three approaches used in studying of a system's performance [11], [12], [13]. Measurement is a direct assessment of the actual or representative system under varying workloads, providing the most accurate results. Simulation and Analytical modeling are performance modeling approaches. Performance modeling encapsulates the performance characteristics of a system that are available from its design in a performance model. In Simulation Modeling, a software representation of the system and its interactions are developed, and statistics collected from the model software's execution provide the performance metrics [13], [14]. Analytical modeling requires the use of mathematical equations for their solution. Availability of a system is pre-requisite for measurement; however, a system is not available during early software design stages. System unavailability, high cost and long duration of 14

measurement, or resource-constraints hinder performing measurement and are the reasons behind the choosing of performance modeling. During design stages, estimates of software demands on hardware need to be made which then serve as inputs for performance models. However, once the system is available, measurements are necessary and should be performed, especially to judge whether the performance objectives are being satisfied and also to validate the correctness of performance models [11]. Simulation models provide high accuracy, however, the time required to develop and simulate the models can be considerable for large systems with need of highly accurate predictions [11], [14]. Including this, the manageability of these models is challenging, as modification made to the system design need to be incorporated to the model, which involve software code modifications and this is non-trivial. In this thesis, the focus is on measurement and analytical modeling approaches therefore simulation modeling techniques are not discussed further. Analytical models exhibit lower accuracy in comparison to other evaluation methods. However, these are quicker to create and solve, and alongside are easier to manage, to find their usefulness throughout the development life of software. They are especially useful when a quick performance feasibility evaluation of a conceptual system is desired. Queueing Networks and Petri Nets (PN) are known analytic modeling methods [3], [4]. 2.2.1 Measurements Measurements can serve many purposes in performance studies, few of which include: obtaining performance metrics for software assessment, identifying system bottlenecks, performance model parameter estimation, and model validation [25]. For performance assessment, Load Testing may be adopted, where a load generator creates virtual users to mimic the behaviour of real system users. While under load test, system-based or application-based monitoring/event recording can be used to get data, which when analyzed provides performance measurements [25]. 15

These measurements can be checked with the performance objectives to determine whether a system design needs further modifications for performance improvements. In the process, Profilers can be helpful, which provide statistics regarding software execution and may have capabilities to display call graphs ­ that pictorially depict the relationship between the caller functions and the called functions ­ of the application. This helps in both model construction and parameterization. Furthermore, Instrumentation, which allows the insertion of data collection code within the software code, can be used for obtaining software execution statistics. The comparison of the measurement results with the model evaluation is used in the model validation process. There are many aspects that need to be considered before the measurement of a system can commence. Questions which need answering are: what is being measured, which components need to be measured, and how a system can be measured? Performance walkthrough as described by Smith in [25] for SPE can be very helpful in such situations. The walkthrough is a meeting of software developers, performance engineers and system architects with the purpose to know more about the system and the customers that interact with it, focusing towards performance. 2.2.2 Analytical Performance Modeling In this section we discuss Markov Chains, Queueing Networks, Petri-Nets, and Layered Queueing Networks analytical modeling techniques. 2.2.2.1 Markov Chains Markov Chains are state-based models that adhere to the Markov property, which declares that each future state depends only on the current state of the system and not on the prior states. Transition from one state to another has an associated probability and the state-to-state probabilities are assembled in a transition matrix. These models can be solved to obtain transient or steady-state behaviour of the system. Solution for Transient behaviour provides the system's

16

state at a point in time, whereas steady-state behaviour represents the long-term behaviour of a system in equilibrium. Figure 1, is a Markov Chain representation of a simple system with a maximum of two jobs. Each state represents the number of jobs currently in the system, which is measured at discrete time intervals, i.e. Discrete Time Markov Chains (DTMC). The arcs show the transitions between the state and their respective transition probabilities, {P11, P12, P21, P22}. Markov chains may also be continuous time, known as Continuous Time Markov Chains (CTMC).

P12
P11
State1 State2

P22

P21
Figure 1: An Example Markov Chain The main shortcoming of Markov Chains is the state space explosion problem [12] where the state space of a system becomes overwhelmingly large, introducing the need to search for automated state-space generation and better solution techniques [11]. QN-based and PN-based modeling formalisms have been developed that can help overcome some of the problems along with helpful tools that can automatically create the state-space of the model [11]. 2.2.2.2 Queueing Networks Behavior of Client-Server system, Web Applications, Operating System (OS) Kernels, etc. can be represented through Queueing Networks. Bolch et al. [11] presents case-studies that show the use of Queueing Systems in modeling of the aforementioned systems. Queueing networks are an interconnection of service stations or queues that serve customers or jobs. Depending on the number of customers in the system, QN are classified as open, closed or mixed. When new

17

customers enter the system at a defined arrival rate (), the system is Open. Figure 2 shows an open QN with two service stations with service rates 1 and 2. Systems with fixed number of customers are Closed Queueing Networks. Figure 3 depicts a closed QN with 30 customers. Mixed systems contain a group of fixed customers that circulate between the stations and also have new customers enter and leave the system.

Customers Arriving 

Customers Departing
1 2

Figure 2: A Simple Open Queueing Network with Arriving and Departing Customers

1
2

30 Customers
1 2

30

Figure 3: A Simple Closed Queueing Network with 30 customers Customers are served based on the Scheduling Discipline of the service station. If the station is busy with processing a customer, the newly arriving customers form a queue waiting to be processed. The queue could infinite or of finite capacity. Few of the regularly used scheduling disciplines are [11]:   FCFS (First-Come-First-Served): Customers that arrive first are served before others. LCFS (Last-Come-First-Served): Last arriving customers are served first.

18



PS (Processor Sharing): Server processing is shared amongst customers who are given very small execution time slices, as jobs appear to be processed simultaneously.



Infinite Server (IS): Also known as Delay Servers. There is no queueing at these servers.

Kendall's notation can be used to define a queueing station as follows [11]:  Inter-arrival time distribution/Service time Distribution/Server Count/Capacity ­Scheduling Discipline An example service station is M/M/1/10-FCFS, which specifies a Markovian inter-arrival time ­ i.e. exponential inter-arrival time (Poisson distribution) ­ and exponential service time uni-processor with FCFS scheduling discipline that has a total capacity of 10 customers. Inputs to QNs include description of the queueing stations, customer workload intensity and the service demands of the customers [39]. The number of the stations and scheduling discipline of each station form one set of the inputs. Customer workload intensity can either be expressed as the number of concurrent customers in the system (closed-system) or the arrival rate (open-system). Along with concurrent customers, Think time of the customers may also be specified. Think time (Z) defines the duration of time that the customer waits and thinks before again requesting service from the system. The service demands specify the service time of the customer at each station and the number of visits made to the station. More specifically, Service demand (D) is the product of number of visits (V) to the station and the average service time per visit (S), i.e. D = V * S. Here, V represents the number of visits made to a station for each request that is made to the system [39], [40]. For further details about inputs of QNs, readers may refer to [39]. The outputs of the QNs are the system and station throughputs, average response times (system) or average residence time (station), average queue length (station) or number of customers in the system, and utilization [39]. Throughput (X) is the rate of customer request 19

completions. Response time (R) is the total time for a request to complete processing through the system or the round trip time of the request. Residence time (Ri for station i) is the total time spent at the station by a system request. Sum of residence times of all stations is the response time. Utilization (U) is the ratio of time a station is busy over a given time period [39]. QNs have single or multiple classes of customers. Single-class customers have the same customer information details: workload intensity and service demands. In multi-class, multiple classes of customers exist where each class is different from the other in customer information detail. Few important Operational Laws which are used in Queueing theory are briefly mentioned as follows [39], [40]: Assuming Job Flow Balance, i.e. System Arrival Rate = System Throughput 1. Little's Law: N = X * R The number of customers in the system is equal to the product of the throughput and the response time. 2. Response Time Law: N = X * (R + Z) An extension of Little's Law, considering think time, where the number of customers in the system is the product of throughput and the sum of system response time and think time. 3. Forced Flow Law: Vi = Xi/X The number of visits made to a station i is the quotient of the throughput at the station and the system throughput. 4. Utilization Law: Ui = Xi * Si Utilization of station i is the product of throughput at the station and the service time. If Forced Flow Law is applied then the Utilization Law can also be expressed as follows: 20

Utilization Law: Ui = X * Di Utilization of station i is the product of system throughput and the service demand. In terms of representing systems, aspects such as simultaneous resource possession and parallel execution (fork and synchronization) cannot be modelled directly through QNs [39] . Readers may also refer to section 1.3.2 for further details about the limitations. QN models have been solved by using Markov Chains; however, in this case, they are constrained by the limitations of the latter. Product Form Queueing Networks (PFQN) are a set of QNs which can be solved without having to rely on their state-space. The use of algorithms has helped in solving of PFQN rather quickly. The notable algorithms for this task are Mean Value Analysis (MVA) and Convolution algorithm [11]. However, not every system can be represented as PFQN because the assumptions of the latter do not hold. In this scenario, approximate algorithms, instead of the exact algorithms have been developed. 2.2.2.3 Petri Nets Petri-nets are useful in modeling of synchronization and parallel execution behaviour of systems. The graphical representation of PNs can clearly describe a system and thus appeal for their use in system modeling [27]. PNs have been extended to include timing information, known as Timed Petri-nets, which find their use in performance and reliability analysis [27]. PNs consist of places and transitions that are connected by arcs. Places are represented by circles and transitions are characterized by bars. Transitions fire when each of the input places has a token. If the corresponding arc has a multiplicity m assigned then the tokens in each input place should be equal or greater than m [27]. Tokens are synonymous to customers which navigate between places when transitions fire. Figure 4 below shows a PN model of a queueing system with six jobs (based on [45] and [51]). The jobs circulate in the system, where they first wait in the Queue before receiving service at the station one at a time. The station is either in Busy or Idle state represented by a token 21

residing in the respective place. Figure 5 shows the state when Service transition fires and the station is Busy. At this point, any newly arriving jobs will continue to be placed in the Queue and the Service transition will not fire until the processing at the station is complete and the Idle place has a token.

Figure 4: PN Example1 - Representation of a Queueing System (Based on [45] and [51])

Figure 5: PN Example1 - After transition "Service" fires (Based on [45] and [51]) PNs are able to portray any QN but QNs cannot represent every PN [26]. The PNs however suffer from similar issues regarding model evaluation, i.e. a computationally intensive process for

22

obtaining performance metrics, where similar approaches akin to use of PFQN in QNs are available to decrease the computation requirements [26]. 2.2.2.4 Layered Queueing Networks Our work concerns with LQN analytical models, which are based on extended QNs and are well-suited to depict both complex software applications and the hardware resources that these software entities run on [5]. The formalism is versatile enough to allow representing multi-tier systems. With the ability to incorporate varying degrees of details in the model, LQN performance modeling can easily be integrated with the SDLC. LQN Solver (LQNS) [5] tool is used for evaluating the models. LQNS actually derives from the functionalities of two solvers: Stochastic Rendezvous Network (SRVN) and Method of Layers (MOL) [12]. For solution purposes, an LQN model is subdivided into "submodels" which are subsequently solved through approximate MVA [12]. This thesis work relates to application of LQN models. Interested readers may refer to [12] for further details regarding LQN model solution.

Interact [1.0, 0] Z =7

{100}

Client
3

pClient
{infinite}

parseR
readDB reply Read

parseW
writeDB Write

{2}

Server
getData setData

pServer

DB

pDB

Figure 6: Example LQN Model

23

An example LQN model is shown in Figure 6. The outermost parallelograms represent the Tasks, which are software entities. The parallelogram within the Tasks correspond to the operations that they perform referred as Entries [1]. Entries are akin to customer classes of QNs [12]. Furthermore, each Entry can be sub-divided into smaller units of work known as Activities [12], represented by rectangles. Activities are important for portraying precedence relations that exist in software with the ability to even represent model down to the code level. Entries by default have one Activity [12]. There is always an entry activity, which begins execution when control passes to the Entry. Each Task may execute on single or multiple Processors shown as oval shapes with connection to the Task. Multiplicity of Tasks signifies multiple threads of a software process shown in the figure within braces. Communication between the Entries of Tasks can be of three types: Synchronous, Asynchronous and Forwarding. In synchronous communication the requesting task (client) blocks until a response is received from a server task. Phase one of the server begins when it receives a request. After sending a response back to the client, the phase two execution of the server begins. In asynchronous interaction the client task does not block after sending a request. In forwarding communication, the server (task A) that received the request forwards the request to another task (task B). Task A at this point starts execution in phase two and when task B is done processing its request, it sends a response back to the client after which task B will begin its execution in phase two. Nesting of different communication patterns can also happen. In Figure 6, synchronous calls are shown with `normal' arrows and asynchronous calls are shown as `vee' arrows. Figure 6 represents a system where there are 100 Clients interacting with a singleprocessor, two-threaded Server process. The data is stored in the DB database and data manipulation from the Client happens through Read and Write operations on the Server. The Client has a think time of 7 seconds (Z = 7). Each entry has an associated service time per phase. Due to lack of space, only the Client task's service times are presented ­ shown within square brackets. The 24

arrows show the direction in which the requests are made. Unless specified otherwise, the frequency of interactions is one. The LQN model structure comprises of tasks, entries, activities, host hardware resource of the former components and their interconnections. Inputs to the model are scheduling discipline of the hardware resources, customer workload intensity and the service demands of the customers for the model components at each phase. The main performance metrics available from model evaluation are steady-state throughput, response times, and utilizations of the modeled components. In the example, the Client initiates the requests performing the Interact operation, requesting three synchronous Reads and an asynchronous Write operation on the Server. To fulfill the Read request, the Server first parses the request and then makes requests to the DB. After receiving the data response from the DB database the Server sends the response back to the Client through the reply activity, completing the Read interaction initiated by the Client. The request from Interact to Write operation is asynchronous and the Client does not wait for a response from the Server. Similarly, the writeDB activity does not wait for a response from the DB. Once the response is received from the Server, the Client thinks for 7 seconds and then initiates another set of requests for Read, repeating this process infinitely.

2.3 Conclusions
This chapter introduces SPE process as briefly discussed in Section 2.1. Section 2.2 describes the types of performance evaluations such as measurement, simulation modeling and analytical modeling. Different analytical modeling formalisms such as Markov Chains, QN, PN and LQN are also discussed. The following chapter describes the Web Application whose performance has been studied in this thesis.

25

Chapter 3 Web Application
This chapter introduces MyBikeRoutes-OSM, a web application whose performance has been analyzed in this work using LQN models. Furthermore, the aspects of LAPP server configuration that are important for performance modeling are discussed.

3.1 MyBikeRoutes-OSM Web Application
The web application under study, MyBikeRoutes-OSM, is a repository of bicycle routes that allows the users to create and share bicycle routes from anywhere in the world. Users can search for the best bike route between given source and destination locations, i.e. best path search. The application uses OpenLayers JavaScript API to display OpenStreetMap4 (OSM) maps [15]. In the particular case of this application, the maps are generated on a Browser through an ordering of prerendered map tiles/images residing on the server. PostgreSQL database functions as the storage facility of the bike routes data and provides best path routing features, where the routing functionality is made available by the pgRouting5 project [16]. For displaying bike routes data or for route-search, OpenLayers at client-side basically communicates with the PostgreSQL backenddatabase through an Apache-PHP server. MyBikeRoutes-OSM derives from the MyBikeRoutes6 web application [17], which is functionality-wise similar but an online web replica of its derivative project. The apparent difference is the use of mapping services from Google Maps API7 [18] to display Google Maps and route information by MyBikeRoutes website, where the bicycle routes data is housed in a MySQL backend-database. Furthermore, best path search on MyBikeRoutes project is processed by client-

http://www.openstreetmap.org/ http://www.pgrouting.org/ 6 http://www.mybikeroutes.com/ 7 http://code.google.com/apis/maps/index.html
4 5

26

side JavaScript interacting with Google Maps API, whereas the MyBikeRoutes-OSM project uses pgRouting at the Database server layer to provide the functionality. MyBikeRoutes and MyBikeRoutes-OSM projects are both recent development endeavors and there are subsequent enhancements of these applications that have been envisioned for future. However, before proceeding with involved development efforts, it is best to know about system bottlenecks and find the performance indicators following the SPE process. Thus, we have analyzed the performance of MyBikeRoutes-OSM application in this thesis.

3.2 Process Flow Diagram

Visit Website

Routes Displayed

Draw Draw Route

Search or Draw?
No Store? Yes

Search

Perform Best Path Search

Best Path Displayed

Routes Stored

Figure 7: MyBikeRoutes-OSM Process Flow Diagram Figure 7 displays the process flow of the MyBikeRoutes-OSM web application. As a user visits the website from their browser, the bicycle routes are displayed on the OSM map. Next, the user may draw bicycle routes or perform a best path search between their chosen source and destination locations. If a search is performed then the best path algorithm is run and the result is 27

displayed on the map. An option to save the route found is subsequently made available. On the other hand, the user may draw and store bicycle routes to share them with other users.

3.3 LAPP Performance Aspects
The following discussion is based on Apache and PostgreSQL configurations that have influence on their performance. These factors have been considered during performance modeling. 3.3.1 Apache Configuration The configuration of a web software server plays a crucial role in its performance. Two of such configurations are the "processing model" and the "pool-size behavior" [35]. The following few paragraphs of discussions on processing models and pool-size behavior have been based on [35]. Processing model determines if Web server would run based on processes or threads. Pool-size behavior determines whether the number of processes/threads of the server remains constant (static) or dynamically changes based on the workload that is being handled (dynamic) [35]. A process-based server has multiple processes and each process is designated to handle a newly arrived client request. In thread-based servers, threads within a process instead of multiple processes manage requests. A hybrid approach may also be adopted where multiple threads execute within multiple processes as part of the Web server [35]. As [35] describes, each processing-model approach has advantages and disadvantages, which is as follows. Process-based configuration is more stable because if one process fails, others are available to continue handling requests while the failed process restarts. However, this model could lead to high memory usage when a web application faces high workloads. The thread-based approach has benefit of using shared data while being light-weight and using less memory. However, single thread failure can cause other threads to fail due to sharing of memory. The hybrid-approach as [35] justifies is a good tradeoff between the two extremes.

28

Apache Multi-Processing Modules (MPM) provide

prefork8 as a process-based

implementation of Apache and worker9 as a hybrid-based implementation. Apache prefork has been used in this work. Prefork relies on configuration parameters which could be adjusted based on the requirements. The server has some processes running to begin with and if more requests arrive then new processes are created to handle them, demonstrating a dynamic pool-size behavior. If workload decreases then the number of processes is also lowered [35]. However, there is a limit to the number of processes that are available to handle client requests concurrently, and this limit is specified by the MaxClients10 [43] directive. This directive should be set adequately based on workload the application experiences, as lower values would lead to queueing and eventual rejection of client requests. For the web application studied in this work the MaxClients was set to 150. Note that if there is a constant number of concurrent client requests that are sent to the Web server, such as in a load test scenario, the server would initially create more processes but then for a longer run in steady-state the number of processes would stabilize and remain constant, assuming that the MaxClients directive is set high enough for server to handle the load. Such steadystate behavior is the focus of this work. 3.3.2 PostgreSQL configuration Similar to the MaxClients directive of Apache, PostgreSQL has the max_connections parameter [20], which specifies maximum concurrent connections to PostgreSQL database. These parameters that relate to Apache and PostgreSQL servers are part of the web application performance model that is discussed in the next chapter.

http://httpd.apache.org/docs/2.0/mod/prefork.html http://httpd.apache.org/docs/2.0/mod/worker.html 10 http://httpd.apache.org/docs/2.0/mod/mpm_common.html#maxclients
8 9

29

3.4 Conclusions
In this chapter, MyBikeRoutes-OSM web application has been introduced and the interactions of the application have been described through the process flow diagram. The significant parameters that affect the performance of Apache and PostgreSQL are presented. The subsequent chapter explains in detail the performance measurement and modeling methodology that has been followed to study the web application.

30

Chapter 4 Performance Evaluation
An essential matter of this work pertains to the measurement and modeling of the web application performance. This chapter explains the process and the methodology employed to perform system load tests, obtain service demand parameters and construct the performance model. Figure 8 gives a general overview of this chapter in context of the performance modeling process which has been followed for LQN modeling of the web application. Interested readers may further refer to the SPE Process Flow Diagram [54], from which any performance study derives and which also serves as the basis for Figure 8. In this thesis, for performance modeling (Figure 8), the performance intensive scenarios are found and used for creating a base scenario, which is used for development of a load test plan. Section 4.1 provides details of the load testing approach, the test setup and the test plan creation which are used for the measurements. Using the base scenario an LQN model structure is created and the service demands are found next using operational laws of QN. Section 4.2 describes the model that has been constructed and the methodology employed to find the service demands. Chapter 5 provides the LQN modeling results and compares them with load testing results for model validation.

4.1 Load Testing
To perform measurements, the web system under test (SUT) was subjected to workloads from the JMeter11 [19] load generator. JMeter is a free open-source multi-platform tool which supports load testing of web applications and includes functionality to test the applications based on following protocols: HTTP, JDBC (for database), FTP, LDAP (directory authentication), etc.

11

http://jakarta.apache.org/jmeter/

31

Performance Modeling
Find Performance Intensive Scenarios (Section 4.1.2)

Create Load Test Plan (Section 4.1.2)

Load Test Plan

Create/Modify LQN Model Structure (Section 4.2.1) Discover Service Demands (Section 4.2.2)

Load Test (Section 4.1)

LQN Model

Model Evaluation (LQNS) Modeling Results (Section 5.1)
Measurement Results (Section 5.1)

Model Validation (Section 5.1)

No
Model Valid

Yes End

Figure 8: Performance Modeling Flow Chart Refer SPE Process Flow Chart [54] 32

Being open-source, enhancements can be incorporated in JMeter based on the specific requirements of the test, which makes it quite a useful and flexible tool. In JMeter, user workloads are defined by Thread Groups which specify the concurrent users and the requests that would be sent by each group. Furthermore, Timers specify the user think time. The test could run for a restricted number of thread group loops or for specified time duration. Listeners are available to record and display results such as number of samples of a request sent, throughput and average response time. The software's interface can either be GUI or command-line based. For the purpose of this work, the GUI was used to create complete test plans and the command-line was utilized in running of the tests. Along with load generation from a single machine, JMeter tests can be deployed to perform Remote Testing in distributed environments where user load is generated from various client-machines. A Master-Slave setup is used when remote testing, which is described in the subsequent section. 4.1.1 Remote-Test Setup and Machine Configuration

Server
(Apache + DB)

Jmeter-Master

Client
(Jmeter-Slave)

LAN
Figure 9: Remote-Test Setup Figure 9 outlines the Remote Testing setup (Master-Slave configuration) that was used in this work for the measurements. In the figure, Jmeter-Master machine initiates the test while the Jmeter-Slave is the client machine that simulates the virtual users (VU) for sending requests to the SUT. The Master-Slave configuration is advantageous when one client-machine is unable to generate the required load on the SUT before they themselves become saturated, and therefore multiple client-machines are essentially needed, all of which are to be controlled by the Master. 33

Each of the client-machines and the SUT machines used had identical physical configuration. The Server machine runs all three software server processes: Apache Web Server, PHP runtime and PostgreSQL Database. Table 1 provides specifications of hardware and software components used for the Load Generator and the Test system. Table 1: Load Test: Hardware and Software Specification Component Type CPU RAM OS File system Network Web Server Scripting Database Load Generator Specification Pentium 4 3.4 GHz (32-bit) 993 MB Ubuntu 10.04 ext3 1000 Mb/s Apache Prefork 2.2.14 PHP 5.32 PostgreSQL 8.4 JMeter 2.3.4 12

4.1.2

Base-Scenario and Test Implementation As a part of the SPE process, it is essential to determine the performance-intensive

scenarios of the application early [25]. To achieve this, first, all the HTTP requests generated by Mozilla Firefox browser while navigating the MyBikeRoutes-OSM application were recorded in JMeter. A load test with a single user was run to determine the performance intensive requests, i.e. requests which had high response time. From the previous step the base scenario (or base test plan

12

JMeter 2.4 jar libraries were also used for SQL query testing.

34

in JMeter) was created by removing non-critical requests. Figure 10 shows the scenario's sequence diagram for one user session. The figure represents the actions of a User who initiates the web communication. The user actions are shown for clarity and for depicting the three web page loads, however, the actual load test plan consists of the HTTP requests of the Client/Browser only. The AppServer or Application Server represents the Apache-PHP server and the DB corresponds to the backend-database.

Figure 10: Base Scenario Sequence Diagram Based on Figure 10, in all, there are nine HTTP request classes sent to the Application Server in one complete user session. The first web page request from user on visiting the site is a set of five HTTP requests consisting of an HTML file, three JavaScript files, and bike routes data. In this first group, except for the last request of bike routes data which interacts also with the 35

Database, all the other requests just interact with the Application Server. The second web page request consists of one HTTP request for best path search and another to save the best path results is run. Very similar requests are again made for the third web page request performing the second bike routes search, however with different start and end destinations. This final set of HTTP requests, communicate with both the Application Server and the Database. From this point forward, a request refers to an HTTP request and web page requests will be explicitly mentioned. Table 2 provides the order in which requests have been sent for reference in the future sections. Table 2: Request classes Request # 1 2 3 4 5 6 7 8 9 Request class name reqHTML reqJS1 reqJS2 reqJS3 reqViewRoutes reqROUTING1 reqADD1 reqROUTING2 reqADD2

For the load tests, a think time of 7 seconds was added before calling reqHTML, i.e. a wait of 7 seconds after one session is complete before sending requests for the new session. This scenario which includes think time of 7 seconds will be referred as Base-Scenario from this point onwards. The load tests were run for a duration of 1800 sec for each N Virtual Users, where N = {1, 2, 4, 6, 10, 20, 30, 40, 50, 80, 120}. To make sure that the Client machine could simulate the users and was not the bottleneck, JMeter tests were run in command-line mode with summary reporting. Furthermore, applying Little's Law to the JMeter results it was easily verified that the number of users active in the SUT were close to the number of virtual users initiated by JMeter [37]. The results obtained from the load tests were the session throughput and average session response time. These results are presented in Chapter 5. 36

4.2 LQN Performance Modeling
The Base-Scenario described earlier is used for LQN model creation. Along with the model structure, for a complete model the service demands of the tasks on their processors are also necessary. The base performance model, its service demands and the technique used to determine the service demands are described in the following sections. 4.2.1 Base-Scenario Model
load req ROUTING1

req HTML

req JS1

req JS2

req JS3

req Routes

req ADD1

req ROUTING2

req ADD2

1 to 120 Browsers

Browser
{infinite}

n1c

n2c

n3c

n4c

n5c

n6c

n7c

n8c

n9c

pClient
{infinite}

NetworkClient
{150}
phpViewRoutes send HTML send JS1 send JS2 send JS3
phpNS5

phpRouting1
phpNS6

phpAdd1
phpNS7

phpRouting2
phpNS8

phpAdd2
phpNS9

ViewRoutes

Routing1

Add1

Routing2

Add2 AppServer

db ViewRoutes

db Routing1

db Add1

db db Routing2 Add2 DB

{100}

pApache

disk1 disk2 disk3 disk4 disk5 disk6 disk7 disk8 disk9

Disk
pDisk {infinite}

n1s

n2s

n3s

n4s

n5s

n6s

n7s

n8s

n9s

Network Server
Figure 11: Base-Scenario LQN Model Figure 11, shows the LQN model of Base-Scenario in a load test. Reply activities are not shown to ease in understanding of the figure. There are N Browsers running on Infinite Servers (pClient). The model is evaluated separately for each value of N. This is a closed model where once a

37

user session is completely processed, the customer is sent back to begin a new set of requests after waiting for a given think time. Since the processing at the client machine did not include browser rendering or page generation in the load test, the service demands for the Browser entries in the model are set to 0. Each Browser sends requests to the AppServer task through the entries of the NetworkClient, which represents the delay incurred when sending messages through the LAN from Browser to the AppServer. Similarly, NetworkServer represents delay due to sending reply from the AppServer. The two Network tasks are modeled as infinite threads running on Infinite Servers. Based on the MaxClients directive of Apache server (refer 3.3), the AppServer multiplicity is set to 150. Similarly, based on max_connections parameter of PostgreSQL database (refer 3.3), the DB task multiplicity is set to 100. Both AppServer and DB tasks execute on the same uni-processor (pApache), which has PS scheduling discipline. The disk is modelled by the Disk task, which has nine entries. Each entry pertains to the nine requests issued by the Browser, i.e the first request (reqHTML) has its disk service demand provided by disk1 entry, then reqJS1 has the disk service demand provided by disk2, and following the same pattern for other requests. An assumption regarding Disk entries have been made in the model. For a request, only one interaction with the Disk entries is assumed, i.e. if both AppServer and DB task perform certain number of disk I/Os for a particular request, then only one call to the Disk at the end of the nested interaction is depicted in the model. In this case, the service time of the respective Disk entry is for one visit only found by multiplying the number of disk I/Os and the average service time at the disk. For the finding the number of disk I/Os, the Forced Flow Law has been used, where both the system throughput and disk I/O throughputs have been found through measurements. Similar ideas have been used previously by [38] and [14] for the database-tier in their works. Like-wise this work also makes the same assumption as the Disk entries for the DB entries. Following paragraph discusses details about the DB layer.

38

There are five request classes that require database access, i.e. reqViewRoutes, reqROUTING1, reqROUTING2, reqADD1 and reqADD2. There is one SQL query executed for reqViewRoutes, which just involves retrieving the routes data. Both the routing requests (reqROUTING1 and reqROUTING2) require running three queries ­ for start and end points and finally the routing search. There is one query executed to insert a route for reqADD1 and reqADD2 requests. However, as mentioned in the earlier paragraph, each entry of DB task has only one visit made to it in the model, i.e. dbViewRoutes, dbRouting1, dbAdd1, dbRouting2 and dbAdd2 have only one visit from the upper AppServer task layer. Based on this, considering dbViewRoutes, the service times of each SQL query executed for dbViewRoutes was summed and presented finally as the service time of the dbViewRoutes entry. Similarly, service times for other entries of the DB task have been found. The reader may refer to section 4.2.2 for further details regarding the calculations of service demands for the entries. Following, the sequence of execution that the model represents is explained in detail. In the model, the reqHTML activity initiates the Browser requests to which the AppServer responds back after execution of sendHTML. Any disk operations by sendHTML happen using the disk1 entry at the pDisk. The network delay of NetworkClient is also incorporated as the request goes from reqHTML through the n1c entry to the sendHTML entry. Following reqHTML, there are three JavaScript (reqJS1, reqJS2, reqJS3) and reqRoutes requests that are sent sequentially, completing the actions of a web site visit. In a similar pattern, the other four remaining requests are also sent in order and processed. Some AppServer activities need to retrieve or amend data on the DB, forming a nested interaction, where the AppServer only sends reply back to the Browser when the AppServer's request has been responded back by the DB. Each AppServer reply also guarantees incorporation of the Network delay in the response time by interacting either directly or indirectly ­ through a Disk ­ with the NetworkServer before sending any reply. This models a complete session

39

of the Base-Scenario of the application. The service demands for the Base-Scenario model is discovered by applying the Utilization Law, details of which are presented in the following sections. 4.2.2 Discovering Service Demand Parameters Discovery of service demand parameters relies on measurements, where metrics obtained from monitoring the system under given conditions help in deriving the necessary parameters. For this task, CPU utilization from the pApache machine has been used. The following sections describe the approach adopted and the values of service demands. 4.2.2.1 Service Demands from Utilization Law If the utilizations and throughputs of system resources are available ­ from load testing or monitoring ­ the Utilization Law can be applied to derive service demands. Previously, Kounev & Buchmann in [14] have used the Utilization Law to find service demands and this work uses the same approach. Many operating systems have tools that make available details about the CPU and disk utilization. For Linux, utilities such as iostat and sar are useful monitoring tools to accomplish this job. The following paragraph gives a brief overview of the calculations related to queueing theory that involve finding service demands using the aforementioned ideas. Consider a station i in a queueing system and a request class c, with average utilization of the station due to requests of c as Uc,i , average system throughput as Xc , and throughput at station i as Xc,i , then the service demand (Dc,i) at the station due to the request class can be calculated by applying Utilization Law as follows, Dc,i = Uc,i/Xc [39]. Service demand is also the product of number of visits (Vc,i) and the average service time per visit (Sc,i), i.e. Dc,i = Vc,i * Sc,i . Here, Vc,i represents the number of visits made to a station for each request of a class-request [39], [40]. If Xc,i and Xc are known then Vc,i = Xc,i/Xc (Forced Flow Law) [39]. Thus, given Xc and Uc,i then Dc,i can be derived. And then if Vc,i is found from Forced Flow Law, then Sc,i can be calculated where both Vc,i and Sc,i serve as

40

inputs to the LQN model. For this work, Xc was found using JMeter, and Uc,i was found from sar utility. Xc,i is required for disk I/O service demands and also found from running sar.

Discover Service Demands Load Test for each Request Class (Measurements) Response Time, Throughput, Utilization

Utilization Law and Forced Flow Law

Service Time, Visits (LQN Inputs)

Back to Performance Modeling
Figure 12: Discovering Service Demands - Flow Chart Figure 12 describes how service demands have been derived for the Base-Scenario model. To determine service demands, the first step was to create separate JMeter tests with one user load having no think time for each request class of Base-Scenario, thereby creating nine tests. Since there is only one user, the contention due to queueing is at the lowest. Each test was run for duration of 900 seconds while the SUT was monitored using sar. Utilization output of sar includes iowait% and idle%, which specify the % of time the CPU is waiting for IO processing and the % of time CPU is not processing, respectively. The CPU utilization can then be obtained by subtracting the idle% from 100. The throughput obtained from JMeter for each test and the CPU utilization obtained from sar was used to find the total service time of each request at the SUT processor using Utilization Law. 41

Furthermore, sar was used to find the average disk service time, and the disk tps (throughput per second). The number of disk I/Os were found by dividing the disk tps by the JMeter throughput, thereby using the Forced Flow Law. As mentioned earlier in section 4.2.1, only one visit from the upper layers to the disk entries is assumed in the model for each request, therefore, the average disk service time was multiplied by the number of disk I/Os to obtain the normalized service time for one disk I/O in the model, which is the input for the Disk entries at the pDisk. The service time found for a request's disk I/O was subtracted from the total service time at the SUT processor, thereby giving the service time for the request class at the AppServer task at the pServer processor. Note that if the CPU utilization was obtained earlier by subtracting both idle% and iowait% from 100 then there would be no need to subtract the request's disk I/O from the total service time at the SUT. The total Network delay was found by subtracting the total service time at the SUT by the JMeter response time for the request. Note for obtaining the NetworkClient and NetworkServer service times, the Network delay is divided by 2. An example of the explanation above follows. Example1: For the first request class (reqHTML) load test, the average CPU Utilization was 59.18%, i.e. the % of time the CPU was not idle. The JMeter throughput was 61.44 requests/s and the average response time was 11 ms. Applying Utilization Law, the total service demand at the SUT processor and disk would be 0.5918/61.44 = 9.63 ms. The disk I/O service time was found to be 0.36 ms and the disk tps was 1.69. Using Forced Flow Law, the average number of disk visits were 1.69/61.44 = 0.02751. Therefore, the normalized service time for disk1 entry is 0.02751 * 0.36 = 0.01 ms. The service time at the sendHTML entry is 9.63 ­ 0.01 = 9.62 ms. Based on this the total Network delay is 11 ­ 9.63 = 1.37 ms, and therefore the service times for n1c and n1s are 1.37/2 = 0.685 ms. For requests that involve database calls, the following approach was adopted. For request classes that have very small service times of about 1 ms at the DB task, such as start and stop point

42

queries of the routing search (4 of such queries in total), the EXPLAIN ANALYZE [20] query command of PostgreSQL was used. The latter command provides the runtime details of a query that is to be evaluated. The EXPLAIN ANALYZE command was executed five times for each start and stop queries and the average runtime was used as the service time. For the other longer queries, i.e. the two routing searches, the viewing routes query, and the two insertion of new routes queries, JMeter tests with one user load and no think time were run for a duration of 900 sec. Applying Utilization Law, and following similar calculations as for Example1 the service demands were found. Note that only one visit is made from the AppServer layer entries/activities to a corresponding database query based on the web application, therefore for a "database query" the visit count is one and the service demand is equal to the service time. However, it is key to realize that based on the web application, the Application Server may call multiple database queries for a particular request class, i.e. for reqROUTING1 request, one visit is made for start point, one for end point and finally one for routing, thereby a total of three queries are called. In this case, the assumptions regarding the model have been made as clearly outlined in section 4.2.1. For the reqROUTING1 request example just considered, the service times of the three queries is summed to form one entry, dbRouting1. Based on above methodology and calculations presented, the service times for each entry of the Base-Scenario model are shown in Table 3. Note that the service times for Network entries with (*) were found to have negative service times. The reqROUTING1, reqADD1, reqROUTING2 and reqADD2 classes had network service times of -0.00067, -0.00117, -0.00048 and -0.00117 (ms) respectively. This is considered as an anomaly and therefore, the network service times for each of these have been changed to 0.0002 ms, i.e. n1c and n1s each have service times of 0.0001 ms. Table

43

4 shows the LQN entries/activities that represent the demands of each request class on AppServer, DB, Disk and Network tasks. Table 3: Service Demand Parameters for Base-Scenario Model Request class reqHTML reqJS1 reqJS2 reqJS3 reqViewRoutes reqROUTING1 reqADD1 reqROUTING2 reqADD2 AppServer (ms) 9.62 0.85 4.8 0.64 95.55 211.67 53.42 186.16 53.43 DB (ms) 29.38 252.72 0.43 52.09 0.41 Disk (ms) 0.01 0.01 0.01 0.02 0.06 0.28 0.32 0.23 0.33 Network (ms) 1.37 0.14 1.19 0.34 2.01 0.0002* 0.0002* 0.0002* 0.0002*

Table 4: LQN entries/activities and request class demands Request class reqHTML reqJS1 reqJS2 reqJS3 reqViewRoutes reqROUTING1 reqADD1 reqROUTING2 reqADD2 AppServer (ms) sendHTML sendJS1 sendJS2 sendJS3 phpViewRoutes phpRouting1 phpAdd1 phpRouting2 phpAdd2 DB (ms) dbViewRoutes dbRouting1 dbAdd1 phpRouting2 phpAdd2 Disk (ms) disk1 disk2 disk3 disk4 disk5 disk6 disk7 disk8 disk9 Network (ms) n1c, n1s n2c, n2s n3c,n3s n4c,n4s n5c,n5s n6c,n6s n7c,n7s n8c,n8s n9c,n9s

The pervious discussions have provided a very detailed methodology employed in this work to find service demands of various entries. The determination of service demands formally completes the LQN model, preparing the latter as an input to the LQN Solver. Most of the model solutions took less than few seconds. These results and the model analysis are presented in the next chapter.

4.3 Conclusions
In this chapter the methodology adopted to study web applications performance has been presented. The systematic processes followed to measure performance, find service demands and 44

to create the performance model are explained. For load testing, the details of remote test setup, the machine specifications and test plan creation steps have been presented. For LQN modeling, the Base-Scenario model is clearly explained and the details of how Utilization Law has been used for finding the service demands are presented. Finally, in the chapter the service demands obtained for the Base-Scenario model has been provided. The following chapter provides results from load testing the web application and from model-based evaluation of the Base-Scenario model. The model validation and the afterward analysis performed to ease the system bottleneck is presented. The chapter also explains an alternative methodology using profiling to derive service demand parameters.

45

Chapter 5 Results and Analysis
In previous chapter an in depth discussion of load testing and performance modeling methodology was presented. In this chapter we present the results, first of which are the load tests measurements. Based on the results the performance objective for the web application is defined. Following this, performance metrics obtained from the model evaluation are presented along with the measurements in both tabular and graphical formats for model validation. Section 5.2, explains the performance analysis done to achieve performance objectives. In contrast to previous method of using Utilization Law for determining model parameters, section 5.3 presents an experimental approach to aid in quick derivation of the parameters, which as proposed can also aid in model creation. The advantages of and efforts to further improve such a process are outlined.

5.1 Results and Model Validation
5.1.1 Load Test Results: Base-Scenario Table 5: Measurement Results - Base-Scenario Users 1 2 4 6 10 20 30 40 50 80 120
Throughput (sessions/s) Response Time (s)

0.12558 0.24844 0.49897 0.73930 0.92838 1.03278 1.05149 1.02687 1.04190 1.01935 1.01470

0.951 0.951 0.987 1.077 3.719 12.156 21.182 31.205 39.629 68.317 105.873

Table 5 shows the results from load testing of Base-Scenario for up to 120 users. Figure 13 and Figure 14 show the same results graphically. With low user counts the steady-state throughput

46

is seen to increase initially and then stabilize at around 1 sessions/s. The response time remains constant for up to 6 users and then continuously rises.

1.2
Throughput (sessions/s)

Throughput vs. Users

1 0.8 0.6 0.4 0.2 0
Measure-Base

0

20

40

60

Users

80

100

120

140

Figure 13: Measurements Base-Scenario (Throughput vs. Users)

120
Session Response Time (s)

Response Time vs. Users

100

80 60 40 20
0 0 20 40 60

Measure-Base

Users

80

100

120

140

Figure 14: Measurements Base-Scenario (Response Time vs. Users) The above results are helpful but they show that web application will not be able to support large number of users at satisfactory response times, showing poor performance. Based on the functionality provided by the web application, a reasonable performance objective is to sustain 40 to 50 users with a session response time of 5 seconds without think time. Considering such as 47

objective, the web application performance has to be improved. The plan is to identify bottlenecks and remove it. The bottleneck could be a software resource such as the Application Server, the database or it could a hardware bottleneck such as pApache CPU. To do so, we will employ performance modeling. Before doing so, the results from the evaluation of the Base-Scenario performance model, which was described in the previous chapter, will be presented and validated. The evaluation will help in pinpointing the bottleneck resource. Intuitive modifications to the model will be analyzed such that performance objectives are met. 5.1.2 LQN Results: Base-Scenario Following presents the results of LQN model evaluation. The model evaluation has been carried out for up to 150 users. Table 6 shows the throughputs and response times from BaseScenario model evaluation and compares them with the measurements. The relative error% for the throughput from the model matches very closely with the actual results with an average error of 3.77%. The response time results show that for 6 users the error% is higher than other user counts, however as also seen the error% is very close to accurate for low numbers of 1 and 2 users and also for higher numbers of 10- 120 users. For response times the average error is 12.15%. Table 6: LQN Model Results - Base-Scenario
Users 1 2 4 6 10 20 30 40 50 80 120 150 AVG ERROR Throughput (sessions/s) Load Test LQN Error% 0.12558 0.12567 0.07% 0.24844 0.24864 0.08% 0.49897 0.48027 3.75% 0.73930 0.68434 7.43% 0.92838 0.96871 4.34% 1.03278 1.09454 5.98% 1.05149 1.08406 3.10% 1.02687 1.07721 4.90% 1.04190 1.07176 2.87% 1.01935 1.06507 4.49% 1.01470 1.06017 4.48% 1.05922 3.77% Response Time (s) Load Test LQN Error% 0.951 0.957 0.63% 0.951 1.044 9.78% 0.987 1.329 34.65% 1.077 1.768 64.16% 3.719 3.323 10.65% 12.156 11.273 7.26% 21.182 20.672 2.41% 31.205 30.135 3.43% 39.629 39.649 0.05% 68.317 68.118 0.29% 105.873 106.182 0.29% 134.623 12.15%

48

1.2

Throughput vs. Users

Throughput (sessions/s)

1 0.8
LQN-Base Measure-Base

0.6 0.4 0.2 0
0 20 40 60

Users

80

100

120

140

160

Figure 15: LQN Base-Scenario (Throughput vs. Users)

Response Time vs. Users
160
Session Response Time (s)
140

120
100 80
LQN-Base
Measure-Base

60
40

20
0

0

20

40

60

Users

80

100

120

140

160

Figure 16: LQN Base-Scenario (Response Time vs. Users) Figure 15 and Figure 16 show the graphs for Throughput vs. Users, and Response Time vs. Users, respectively. As seen from both these graphs, the model closely follows the behavior of the system's performance. Figure 17 shows the pApache CPU utilization from LQN evaluation that approaches about 100% utilization for more than 20 users. This is the hardware bottleneck and saturates before other resources.

49

pApache Util. vs. Users
100%

pApache Utilization

80% 60%

LQN-Base

40% 20% 0%
0 20 40 60 Users 80 100 120 140 160

Figure 17: LQN Base-Scenario (pApache Utilization) 5.1.3 Base-Scenario model validation Lazowska et al. [39] provide rough error percentages for model validation. For multiple classes throughput error between 5% and 10% and for response time error between 10% and 30% are considered acceptable. For our model results, all the throughputs are very accurate with average error lower than 5%. Considering response times, the exceptions are 4 and 6 users which have high error however in real situations the system will probably witness higher number of users. One possible reason for this shortcoming is that CPU cache hit ratio is higher with lower user counts and CPU caching has not been considered in the model. Based on average response time error of about 12%, the model represents the actual results. After the model validation, the next steps will be to determine the improvements to the system.

5.2 Attaining Performance Objectives
To achieve performance objectives, designs that ease the bottleneck have to be determined. From previous analysis it is found that the pApache server is the bottleneck for the MyBikeRoutesOSM web application. To scale the system, options include adding threads, using a multi-processor system or using copies (replicas) of the server [46]. Since the bottleneck is hardware, addition of 50

threads of AppServer task or the DB task will not be helpful. We evaluate the performance model for two possible solutions: 1. Multi-processor machine and, 2. Separating AppServer and DB task into separate identical machines. These solutions are analyzed in the subsequent discussions. 5.2.1 Overview The following two types of modifications have been analyzed through the model: 1. Multiprocessor pApache: The models have been evaluated for processors with multiplicity of two and four, which are referred as Base-Scenario-m2 and BaseScenario-m4 models respectively. (Note that LQNS did not support PS scheduling for pApache multiprocessor. For two and four multiprocessors, FCFS discipline was used). 2. Separate machine for Database: Instead of having both Application and Database software servers run on the pApache machine, the database-tier can be deployed on a separate and identical machine. The changes to the model include creating a new pDB processor which will be hosting the DB task and its entries while the Disk2 task on pDisk2 will now handle the disk I/O for the DB task. The previous pDisk is renamed as pDisk1, and the Disk task is renamed as Disk1 which will handle disk I/O for AppServer. The model will be referred as SeparateDB-Scenario. There are two assumptions made for this model. One is that service times for the disk I/O is divided by two to derive the service times of each entries of Disk1 and Disk2 tasks. Second, no network delay between the pApache and the pDB machines are considered. In a LAN environment the delay would not be very large, however, for very detailed study significant delays should be considered. The modifications described above have been evaluated and the results are presented in the following sections. 51

5.2.2

Performance Analysis

4.5
Throughput (sessions/s)

Throughput vs. Users
LQN-Base LQN-Base-m2 LQN-Base-m4 LQN-SeparateDB

4.0 3.5 3.0 2.5

2.0 1.5 1.0 0.5 0.0
0 20 40 60 80 100 120 140 160

Users

Figure 18: Performance Analysis (Throughput vs. Users)

Response Time vs. Users
20 18 16 14 12 10 8 6 4 2 0 0

Session Response Time (s)

LQN-Base LQN-Base-m2 LQN-Base-m4 LQN-SeparateDB responsetime=5

10

20

Users

30

40

50

60

Figure 19: Performance Analysis (Response Time vs. Users) ­ 60 users

52

pApache Util. vs. Users
100%
pApache Utilization
LQN-Base LQN-Base-m2 LQN-Base-m4 LQN-SeparateDB

80% 60%

40% 20% 0%
0 20 40 60 Users 80 100 120 140 160

Figure 20: Performance Analysis (pServer Utilization) Figure 18, Figure 19, and Figure 20 shows the throughput, response time and the utilization graphs comparing the Base-Scenario, Base-Scenario-m2, Base-Scenario-m4 and SeparateDB models. Note that Figure 19 displays results until 60 users only. As depicted, Base-Scenario can sustain about 15 users within a response time of 5 seconds, whereas the separate database can sustain 19 users, the dual processor 25 users, and quad-processor 50 users within this response time with respective throughputs of about 1 sessions/s, 1.5 sessions/s, 2sessions/s and 4 sessions/s. Furthermore, considering the quad-processor case with loads at and below 50 users, the user perceived web page load is under the 3 seconds duration (refer Section 1.1), thereby providing some assurance that as per the scenario chosen the website will be able to meet performance expectations of most customers. Based on the model evaluations, for this work, we choose the quad-processor considering that it meets the performance objective set earlier. Although having a dedicated separate machine for the database was an attractive choice, the results suggest towards choosing from multiprocessor system to scale the system.

53

5.3 Performance Modeling and XDebug
Earlier we have used Utilization Law to derive service demands; however, an experimental approach adopted here is to find the demands through XDebug profiling. The aim here is to portray XDebug as an easy to use tool to aid creation of models and also for determination of service demands. The following paragraphs first provide a brief introduction of XDebug and describe how it was used in this work for Performance Modeling. 5.3.1 Overview XDebug13 [21] is a PHP extension developed by Derick Rethans to aid in debugging and profiling of PHP applications. Tracing provides the necessary information required for debugging and profiling gives details about the how many times a function was called and how much time was spent executing the functions ­ the very information that is needed for service demands. Both tracing and profiling are achievable through instrumentation of a PHP application with XDebug function calls; however, a global profiler also can be enabled to profile the applications. To comprehend and analyze the generated profiling data, other tools are required, which display the information in a GUI showing call-graphs. KCacheGrind14 tool have been used in this work to analyze the global profiling data generated. 5.3.2 XDeb-Scenario LQN Model Base-Scenario model describes the complex interactions of the MyBikeRoutes-OSM web application. This model was developed through knowledge of the system architecture and design. However, in retrospect before such a model can be developed, a simple model can be created at first adhering to the SPE's Simple-Model strategy. If a prototype of an application is present and not much information is available then it is wise to start with a simple model based on SPE. In our case,

13 14

http://www.xdebug.org/ http://kcachegrind.sourceforge.net/html/Home.html

54

we can use profiling, obtain some details about the application and create a model from that information. Figure 21 shows the XDeb-Scenario model that can be one of the initial models. This model does not contain the disk I/O interactions found in Base-Scenario. Following the Adapt-toPrecision strategy of SPE, the XDeb-Scenario model can be extended to form Base-Scenario, i.e. when further details are found about the system. In addition to previous difference, how the service demands are obtained also differentiates the XDeb-Scenario from the Base-Scenario. In the following section the results from evaluating the XDeb-Scenario model based on the XDebug parameters are provided.

load
req HTML req JS1 req JS2 req JS3 req Routes req ROUTING1 req ADD1 req ROUTING2 req ADD2
1 to 120 Browsers

Browser
{infinite}

pClient n1c n2c n3c n4c n5c n6c n7c n8c n9c
{infinite}

NetworkClient
{150} phpViewRoutes send HTML send JS1 send JS2 send JS3 phpNS5
phpRouting1

phpAdd1 phpNS7

phpRouting2 phpNS8

phpAdd2 phpNS9

phpNS6

ViewRoutes

Routing1

Add1

Routing2

Add2

AppServer
pApache {infinite}

n1s n2s n3s n4s n5s n6s n7s n8s n9s

db db db ViewRoutes Routing1 Add1

db db Routing2 Add2

Network Server

DB

{100}

Figure 21: XDeb-Scenario LQN Model 5.3.3 Service Demand Parameters using XDebug To determine service demands, the first step was to run JMeter test for one loop of the BaseScenario with a single user load, providing separate response time information for each of nine request classes that make a session. While the load test was run, XDebug was set to profile the PHP

55

interactions of the Apache-PHP Application Server. The profiling provided detailed information about service times of different PHP function calls with call graph helping determine the number of calls and time spent making calls to the backend-database. The difference between each request's response time provided by JMeter and the service time reported by XDebug was considered as the network delay for the request. For modeling, the total network delay was divided by two to have the service times of NetworkClient and NetworkServer entries. Following is a brief description of how service demands originating due to reqRouting1 request was determined. Figure 22 and Figure 23 show the Call Graph and Flat Profile output from KCacheGrind based on the XDebug profiling of reqRouting1 functionality in PHP at the Application Server. The Call Graph as shown can be the basis to create models of PHP applications without resorting to view source code. The figures shows that the total time to run reqRouting1 (referred by {main} here) of the AppServer task was 460 ms and that the executing the respective database query for routing search ­ which also includes processing of PHP API calls to interact with the database ­ took a significant portion of time, i.e. 322 ms. For service demands, we assume that 322ms was spent in DB task and the remaining time, i.e. about 138 ms was spent at the AppServer.

Figure 22: Call Graph from XDebug (Routing1)

56

Figure 23: Flat Profile from XDebug (reqRouting1) Table 7 shows the service demands obtained from XDebug. These parameters would be used as inputs to the model. Table 7: Service Demand Parameters for Model1 (XDebug) Request class reqHTML reqJS1 reqJS2 reqJS3 reqViewRoutes reqROUTING1 reqADD1 reqROUTING2 reqADD2 AppServer (ms) 13.0 1.0 7.0 1.0 107.181 137.809 43.590 146.591 42.121 DB (ms) 47.882 322.489 9.164 136.38 9.222 Network (ms) 2.0 1.0 1.0 1.0 10.0 4.0 2.0 4.0 4.0

5.3.4

XDeb-Scenario Results Table 8 shows the throughputs and response times from XDeb-Scenario model evaluation

comparing them with the load test results. The average throughput error is 5.76% and the response time error is 18.69%. For users 4 and 6, the response time error is higher in comparison to other users.

57

Table 8: LQN Model Results ­ XDeb-Scenario
Users 1 2 4 6 10 20 30 40 50 80 120 150 Avg. Error Throughput (sessions/s) Load Test LQN Error% 0.12558 0.12418 1.11% 0.24844 0.24522 1.30% 0.49897 0.47101 5.60% 0.73930 0.66538 10.00% 0.92838 0.92033 0.87% 1.03278 1.03869 0.57% 1.05149 1.0844 3.13% 1.02687 1.10804 7.90% 1.04190 1.12155 7.64% 1.01935 1.14025 11.86% 1.01470 1.14988 13.32% 1.1537 5.76% Response Time (s) Load Test LQN Error% 0.951 1.053 10.73% 0.951 1.156 21.56% 0.987 1.492 51.17% 1.077 2.018 87.37% 3.719 3.866 3.95% 12.156 12.256 0.82% 21.182 20.665 2.44% 31.205 29.099 6.75% 39.629 37.581 5.17% 68.317 63.161 7.55% 105.873 97.350 8.05% 123.018 18.69%

Figure 24, Figure 25 and Figure 26 presents the throughput, response time and pApache utilization graphs from evaluating the XDeb-Scenario model.

1.4
Throughput (sessions/s)

Throughput vs. Users

1.2 1

0.8 0.6
0.4

LQN-XDeb Measure-Base

0.2 0
0 20 40 60 Users 80 100 120 140 160

Figure 24: LQN XDeb-Scenario (Throughput vs. Users)

58

Response Time vs. Users
140
Session Response Time (s)

120 100
80 60

LQN-XDeb Measure-Base

40 20
0

0

20

40

60

Users

80

100

120

140

160

Figure 25: LQN XDeb-Scenario (Response Time vs. Users)

pApache Util. vs. Users
120%
pApache Utilization

100%
80%

LQN-XDeb

60% 40%
20% 0% 0 20 40 60 Users 80 100 120 140 160

Figure 26: LQN XDeb-Scenario (pApache Utilization vs. Users) As seen, XDeb-Scenario model results are not as accurate as Base-Scenario; however, the former does capture the behaviour of the system to a degree. The use of XDebug was experimental and the process described here can be improved such that it is found reliable in obtaining service demands. One such improvement is to run a load test while having XDebug profile the Application Server, instead of running just one session of the test plan. Another intuitive approach is to use

59

some disk profiling along with XDebug to provide disk I/O information. However, some disk I/O will be part of XDebug profiling itself which is difficult to avoid. One of the main advantages of XDebug is the time spend in obtaining the service demands information. For this work, the methodology to find service demands through CPU Utilization required running load tests for each class for 1800 sec. Furthermore, tests were run for database queries as well. The time and resources spent in such a process is considerably more than performing load tests just once for 1800 sec when using XDebug for the task. The second advantage is the Call graph obtained from profiling, which is very useful in the performance model creation itself. In many cases the performance engineer does not initially have detailed information about the SUT, and such profiling can be a major help in describing the interactions of the components to develop the model. Applying the suggested improvements (as above) for determining service demands from XDebug is a prospect for future research.

5.4 Conclusions
In this chapter the results from Base-Scenario load tests have been used for the model validation. The model results show an average throughput error of 3.77% and average response time error of 12.15%. Once the model is validated, performance analysis is done to achieve performance objectives. It is found that replacing the existing pApache uni-processor with a multiprocessor machine is a better choice for scalability in comparison to the other option of separating the Application Server and database into two separate systems. Finally, the chapter presents an experimental approach for deriving service demand parameters using XDebug. The possible improvements to the methodology are outlined and advantages of using the approach are mentioned. The next chapter provides conclusions from the work done in this thesis.

60

Chapter 6 Conclusions
6.1 Summary
In this thesis, the need to carry out capacity planning of web applications to meet their performance objectives has been addressed through use of measurements and performance modeling. A LQN performance model of MyBikeRoutes-OSM web application running on LAPP server has been introduced in this work. For the base scenario described, performance results have been obtained from both measurement-based and model-based evaluations, the methodologies of which have clearly been explained and the results analyzed. JMeter was used for load testing and Utilization Law was applied to obtain service demand parameters. The model is validated by comparison with the load test results. With average error of 3.77% for throughput and 12.15% for response times the model is shown to capture the web application's performance. The analysis of the base model shows that the processor running the Application Server is the hardware bottleneck. To ease the bottleneck such that desired performance objectives are satisfied, modeling is used to represent the configuration options. The best configuration is found to use a quad-processor machine for Application Server instead of having separate machines for Application and Database servers. Lastly, a new experimental methodology is described for deriving service demand parameters for LAPP server and variants by using XDebug. The results from including the service demands in a model is presented and analyzed. Furthermore, ways to further improve the process is outlined, followed by mention of advantages of the approach.

61

6.2 Limitations
The following are few limitations of the modeling approach that has been presented: 1. Caching: The models did not incorporate the effects of caching. The possible effect of this was seen with model results of lower user counts (4 and 6 users), where CPU caching was a probable case. 2. Network: The network was modeled as an Infinite Server and the service demands were obtained through estimation rather than through network utilization data. Using the latter would have provided more accurate and reliable results from the model. The limitations of the model are candidates for future work.

6.3 Future Work
The following are proposed future works: 1. Caching: Caching can occur at various layers such as at CPU, Application Server and Database. The future work involves adding dedicated caching at the Apache server using available modules to improve the performance and representing the different caching behaviors in the web application's model. 2. Network: Methods to obtain service demands through network utilization data from the OS need to be determined. Work done by researchers on modeling of network behavior also can be a strong candidate for future work. 3. Thread pool Limits of Apache Server: The work presented dealt with observing the behavior of the system below the set maximum concurrent connections parameter (maxclients). Future work would be to observe the behavior of the system when the limits have been reached and to incorporate this in the performance model.

62

4. XDebug service demands: As mentioned earlier that using XDebug to find service demands is a time saving process which can further be perfected. More research into improving this approach ­ first by following the earlier suggested improvements ­ will be quite beneficial.

6.4 Conclusions
One of the strengths of adopting LQN model based approach as seen from this thesis is the short time required for model creation, manipulation and evaluation with good accuracy. Before modification of any system component, the effect of the change can be predicted and a decision can be reached just through quick model evaluation.

63

References
[1]

T. A. Israr, D. H. Lau, G. Franks, and M. Woodside, "Automatic generation of layered queuing software performance models from commonly available traces," In Proceedings of the 5th international Workshop on Software and Performance, 2005, pp. 147-158. J. E. Neilson, C. M. Woodside, D. C. Petriu and S. Majumdar, "Software bottlenecking in clientserver systems and rendezvous networks," IEEE Transactions on Software Engineering, vol. 21, no. 9, pp. 776-782, 1995. T. Verdickt, B. Dhoedt, F. Gielen and P. Demeester, "Modelling the performance of CORBA using layered queueing networks," In Proceedings of 29th Euromicro Conference, 2003, pp. 117-123. A. Pastsyak, Y. Rebrova, and V. Okulevich, "Performance Prediction of Client-Server Systems By High-Level Abstraction Models," In 4th Software Engineering Conference (Russia) 2008 (SEC(R) 2008) (Oct 23-24 2008), 2008. [Online]. Available: http://2008.ceesecr.org/en/etc/secr2008_alexander_pastsyak_performance_prediction_of_clientserver_systems.pdf. [Accessed Apr 12, 2010]. G. Franks, P. Maly, M. Woodside, D. C. Petriu and A. Hubbard, Layered Queueing Network Solver and Simulator User Manual, Real-time and Distributed Systems Lab, Carleton University, Ottawa, 2005. [Online]. Available: http://www.sce.carleton.ca/rads/lqns/LQNSUserMan.pdf. [Accessed Dec 28, 2010]. T. Liu, S. Kumaran, and J. Chung, "Performance Engineering of a Java-Based eCommerce System," In Proceedings of the 2004 IEEE international Conference on E-Technology, E-Commerce and E-Service (Eee'04), 2004, pp. 33-37. J. Xu and M. Woodside, "Template-Driven Performance Modeling of Enterprise Java Beans," In Proc. Workshop on Middleware for Web Services, Enschede, Netherlands, 2005, pp. 57-64. A. Ufimtsev, and L. Murphy, "Performance modeling of a JavaEE component application using layered queuing networks: revised approach and a case study," In Proceedings of the 2006 Conference on Specification and Verification of Component-Based Systems, 2006, pp. 11-18. N. Tiwari, and P. Mynampati, "Experiences of using LQN and QPN tools for performance modeling of a J2EE Application," In Proc. of Computer Measurement Group (CMG) Conference, 2006, pp. 537­548. B. Urgaonkar, G. Pacifici, P. Shenoy, M. Spreitzer, and A. Tantawi, "Analytic modeling of multitier Internet applications," ACM Trans. Web, vol. 1, no. 1, pp. 1-35, May 2007. G. Bolch, S. Greiner, H. de Meer and K. S. Trivedi, Queueing networks and Markov chains: Modeling and Performance Evaluation with Computer Science Applications, Wiley-Interscience, 2005. 64

[2]

[3]

[4]

[5]

[6]

[7]

[8]

[9]

[10]

[11]

[12]

G. Franks, Performance Analysis of Distributed Server Systems. PhD Thesis, Report OCIEE-00-01, Carleton University, Ottawa, Ontario, Canada, December 1999. A. Gambi, G. Toffetti and S. Comai, "Model-driven web engineering performance prediction with layered queue networks," in Current Trends in Web Engineering, F. Daniel and F. Facca, Eds. Springer Berlin / Heidelberg, 2010, pp. 25-36. S. Kounev, and A. Buchmann, "Performance modeling and evaluation of large-scale J2EE applications," In Proceedings of the Computer Measurement Group's International Conference (CMG'03), 2003, pp. 273-283. M. Haklay and P. Weber, "OpenStreetMap: User-Generated Street Maps," IEEE Pervasive Computing, vol. 7, no. 4, pp. 12-18, Oct 2008. M. F. M. Firdhous, D. L. Basnayake, K. H. L. Kodithuwakku, N. K. Hatthalla, N. W. Charlin and P. M. R. I. K. Bandara, "Route Advising in a Dynamic Environment ­ A High-Tech Approach," in Innovations in Computing Sciences and Software Engineering, T. Sobh, and K. Elleithy, Eds. 2010, pp. 249-254. Y. Shoaib, N. Vasandani, A. Sinha, and A. Goel, MyBikeRoutes.com, 2008. [Online]. Available: http://www.mybikeroutes.com. [Accessed Dec 28, 2010]. G. Svennerberg, Beginning Google Maps API 3, 2nd edition, Apress, 2010. E. Halili, Apache Jmeter, Packt Publishing, 2008. N. Matthew, and R. Stones, "PostgreSQL Administration," in Beginning Databases with PostgreSQL, Apress, 2005, p. 314. K. McArthur, "Testing, Deployment, and Continuous Integration," in Pro Php: Patterns, Frameworks, Testing and More (Pro), Apress, 2008, pp. 123-124. T. Verdickt, B. Dhoedt, F. De Turck, and P. Demeester, "Hybrid performance modeling approach for network intensive distributed software," In Proceedings of the 6th international Workshop on Software and Performance, 2007, pp. 189-200. S. Elbaum, S. Karre, G, Rothermel, "Improving web application testing with user session data," in Proceedings of 25th International Conference on Software Engineering (ICSE'03), 3-10 May 2003, pp. 49- 59. Forrester Consulting, "eCommerce Web Site Performance Today: An Updated Look At Consumer Reaction To A Poor Online Shopping Experience," White paper, Aug. 17, 2009, pp. 121. C.U. Smith and L.G. Williams, Performance Solutions: A Practical Guide to Creating Responsive, Scalable Software, Addison-Wesley, 2002.

[13]

[14]

[15]

[16]

[17]

[18] [19] [20]

[21]

[22]

[23]

[24]

[25]

65

[26]

J. A. Rolia, Predicting the Performance of Software Systems, PhD thesis, University of Toronto, Toronto, Ontario, Canada, January 1992. G. Balbo, "Introduction to generalized stochastic petri nets," in Proceedings of the 7th international conference on Formal methods for performance evaluation (SFM'07), M. Bernardo and J. Hillston, Eds. Springer Berlin / Heidelberg, 2007, pp. 83-131. X. P. Wu, An Approach to Predicting Performance for Component Based Systems, MASc Thesis, Carleton University, Ottawa, Ontario, Canada, July 2003. N. Singh, H. S. Alhorr and B. P. Bartikowski, " Global e-commerce: A Portal Bridging the world markets," Journal of Electronic Commerce Research: Special Issue: Global B-Commerce, vol. 11, no. 1, pp. 1-5, 2010. A. Totok and V. Karamcheti, "RDRP: Reward-Driven Request Prioritization for e-Commerce web sites", Electronic Commerce Research and Applications, vol. 9, no. 6, pp. 549-561, November 2010. D. A. Menasce, "Load Testing of Web Sites," IEEE Internet Computing, vol. 6, no. 4, pp. 70-74, July 2002. Y. Shoaib, and O. Das, "Software Performance Modeling Using Layered Queueing Networks," In Proceedings of Graduate Research Symposium (INNOVATIONS 2010), Poster Abstract Presentation, Electrical and Computer Engineering, Ryerson University, Canada, 29th April 2010, p. 30. L. P. Slothouber, "A model of web server performance," In Proceedings of the 5th International World Wide Web Conference, 1996. J. Dilley, R. Friedrich, T. Jin, and J. Rolia, "Web server performance measurement and modeling techniques," Performance Evaluation - Special issue on tools for performance evaluation, vol. 33, no. 1, pp. 5-26, June 1998. D. A. Menasce, "Web Server Software Architectures," IEEE Internet Computing, vol. 7, no. 6, pp. 78-81, November 2003. T. Suzumura, M. Tatsubori, S. Trent, A. Tozawa, and T. Onodera, "Highly scalable web applications with zero-copy data transfer," In Proceedings of the 18th International Conference on World Wide Web (WWW '09), ACM: New York, NY, USA, 2009, pp. 921­930. N. J. Gunther, "What is guerrilla capacity planning?" in Guerrilla Capacity Planning: A Tactical Approach to Planning for Highly Scalable Applications and Services, Springer: Berlin Heidelberg, 2007, pp. 1-16. X. Liu , J. Heo , L. Sha, "Modeling 3-Tiered Web Applications," in Proceedings of the 13th IEEE International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS '05) , September 27-29, 2005, pp. 307-310. 66

[27]

[28]

[29]

[30]

[31]

[32]

[33]

[34]

[35]

[36]

[37]

[38]

[39]

E. D. Lazowska, J. Zahorjan, G. S. Graham, and K. C. Sevcik, Quantitative System Performance: Computer System Analysis Using Queueing Network Models, Prentice-Hall, 1984. P. J. Denning and J. P. Buzen, "The Operational Analysis of Queueing Network Models," ACM Computing Surveys (CSUR), vol. 10, no. 3, pp. 225-261, September 1978. M. A. Poulter, "Open source in libraries: an introduction and overview," Library Review, vol. 59, no. 9, pp. 655-661, January 2010. G. Lawton, "LAMP Lights Enterprise Development Efforts", Computer, vol. 38, no. 9, September, pp. 18-20, 2005. S. Vugt, "Setting Up Web Services," Beginning Ubuntu Server Administration, Apress, 2008, pp. 313-328. S. Kounev and A. Buchmann, "Performance modelling of distributed e-business applications using Queuing Petri Nets," In Proceedings of the 2003 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS '03). IEEE Computer Society, Washington, DC, USA, 2003, pp. 143-155. O. Das, "Petri Nets," Class lecture for Computer Systems Modeling (EE8214), Ryerson University, Toronto, March 17, 2010. T. Omari, G. Franks, M. Woodside, and A. Pan, "Solving layered queueing networks of large client-server systems with symmetric replication," In Proceedings of the 5th international workshop on Software and performance (WOSP '05), 2005, pp. 159-166. D. Peng, Y. Yuan, K. Yue, X. Wang and A. Zhou, "Capacity planning for composite web services using queueing network-based models," in Advances in Web-Age Information Management, Q. Li, G. Wang and L. Feng, Eds. Springer Berlin / Heidelberg, 2004, pp. 439-448. M. Tribastone, P. Mayer and M. Wirsing, "Performance prediction of service-oriented systems with layered queueing networks," in Leveraging Applications of Formal Methods, Verification, and Validation, T. Margaria and B. Steffen, Eds. Springer Berlin / Heidelberg, 2010, pp. 51-65. X. Wu and M. Woodside, "Performance Modeling from Software Components", in Proceedings of the 4th international workshop on Software and performance (WOSP '04), 2004, pp. 290-301. J. Xu, A. Oufimtsev, M. Woodside, and L. Murphy, "Performance modeling and prediction of enterprise JavaBeans with layered queuing network templates," In Proceedings of the 2005 conference on Specification and verification of component-based systems (SAVCBS '05). Article 5, 2005. C. G. Cassandras, S. Lafortune, "Petri nets," in Introduction to Discrete Event Systems. 2nd ed. Springer: US, 2008, pp. 223-267.

[40]

[41]

[42]

[43]

[44]

[45]

[46]

[47]

[48]

[49]

[50]

[51]

67

[52]

E. Nygren, R. K. Sitaraman, and J. Sun, "The Akamai network: a platform for high-performance internet applications," ACM SIGOPS Operating Systems Review., vol. 44, no. 3, pp. 2-19, August 2010. Y. Shoaib, and O. Das, "Web Application Performance Modeling Using Layered Queueing Networks," Fifth International Workshop on Practical Applications of Stochastic Modelling (PASM 2011), Karlsruhe, Germany, March 2011, accepted for publication, 20 pages. ** C.U. Smith and L.G. Williams, "The SPE Process," in Performance Solutions: A Practical Guide to Creating Responsive, Scalable Software, Addison-Wesley, 2002, p. 409

[53]

[54]

68

Appendices
Appendix 1: Base-Scenario LQN model
G "OSM BikeRoutes, Base-Scenario (Total Client Think Time=7)" 0.001 5000 1 0.9 -1 P0 p pClient f i p pNetwork f i p pApache s p pDisk f -1 T0 t Browser r load -1 pClient z 7 m 150 t NetworkClient i n1c n2c n3c n4c n5c n6c n7c n8c n9c -1 t NetworkServer i n1s n2s n3s n4s n5s n6s n7s n8s n9s -1 t Server f sendHTML sendJS1 sendJS2 sendJS3 processViewRoutes processRouting1 processAdd1 processRouting2 processAdd2 -1 pApache m 150 t DB f dbViewRoutes dbRouting1 dbAdd1 dbRouting2 dbAdd2 -1 pApache m 100 t Disk f disk1 disk2 disk3 disk4 disk5 disk6 disk7 disk8 disk9 -1 pDisk -1 E0 A load reqHTML s n1c 0.000685 0.0 0.0 -1 s n2c 0.00007 0.0 0.0 -1 s n3c 0.000595 0.0 0.0 -1 s n4c 0.00017 0.0 0.0 -1 s n5c 0.001005 0.0 0.0 -1 s n6c 0.0000001 0.0 0.0 -1 s n7c 0.0000001 0.0 0.0 -1 s n8c 0.0000001 0.0 0.0 -1 s n9c 0.0000001 0.0 0.0 -1 s n1s 0.000685 0.0 0.0 -1 s n2s 0.00007 0.0 0.0 -1 s n3s 0.000595 0.0 0.0 -1 s n4s 0.00017 0.0 0.0 -1 s n5s 0.001005 0.0 0.0 -1 69

s n6s 0.0000001 0.0 0.0 -1 s n7s 0.0000001 0.0 0.0 -1 s n8s 0.0000001 0.0 0.0 -1 s n9s 0.0000001 0.0 0.0 -1 y n1c sendHTML 1.0 0.0 0.0 -1 y n2c sendJS1 1.0 0.0 0.0 -1 y n3c sendJS2 1.0 0.0 0.0 -1 y n4c sendJS3 1.0 0.0 0.0 -1 y n5c processViewRoutes 1.0 0.0 0.0 -1 y n6c processRouting1 1.0 0.0 0.0 -1 y n7c processAdd1 1.0 0.0 0.0 -1 y n8c processRouting2 1.0 0.0 0.0 -1 y n9c processAdd2 1.0 0.0 0.0 -1 s sendHTML 0.00962 0.0 0.0 -1 y sendHTML disk1 1.0 0.0 0.0 -1 y disk1 n1s 1.0 0.0 0.0 -1 s sendJS1 0.00085 0.0 0.0 -1 y sendJS1 disk2 1.0 0.0 0.0 -1 y disk2 n2s 1.0 0.0 0.0 -1 s sendJS2 0.0048 0.0 0.0 -1 y sendJS2 disk3 1.0 0.0 0.0 -1 y disk3 n3s 1.0 0.0 0.0 -1 s sendJS3 0.00064 0.0 0.0 -1 y sendJS3 disk4 1.0 0.0 0.0 -1 y disk4 n4s 1.0 0.0 0.0 -1 A processViewRoutes phpViewRoutes A processRouting1 phpRouting1 A processAdd1 phpAdd1 A processRouting2 phpRouting2 A processAdd2 phpAdd2 #edit s dbViewRoutes 0.02938 0 0 -1 s dbRouting1 0.25272 0 0 -1 s dbAdd1 0.00043 0.0 0.0 -1 s dbRouting2 0.05209 0.0 0.0 -1 s dbAdd2 0.00041 0.0 0.0 -1 #added disk s disk1 0.00001 0 0 -1 s disk2 0.00001 0 0 -1 s disk3 0.00001 0 0 -1 70

s disk4 0.00002 0 0 -1 s disk5 0.00006 0 0 -1 s disk6 0.00028 0 0 -1 s disk7 0.00032 0 0 -1 s disk8 0.00023 0 0 -1 s disk9 0.00033 0 0 -1 y dbViewRoutes disk5 1 0 0 -1 y dbRouting1 disk6 1 0 0 -1 y dbAdd1 disk7 1 0 0 -1 y dbRouting2 disk8 1 0 0 -1 y dbAdd2 disk9 1 0 0 -1 -1 A Browser s reqHTML 0.0 y reqHTML n1c 1.0 s reqJS1 0.0 y reqJS1 n2c 1.0 s reqJS2 0.0 y reqJS2 n3c 1.0 s reqJS3 0.0 y reqJS3 n4c 1.0 s reqROUTES 0.0 y reqROUTES n5c 1.0 s reqROUTING1 0.0 y reqROUTING1 n6c 1.0 s reqADD1 0.0 y reqADD1 n7c 1.0 s reqROUTING2 0.0 y reqROUTING2 n8c 1.0 s reqADD2 0.0 y reqADD2 n9c 1.0 s sendReply 0.0 : reqHTML -> reqJS1; reqJS1 -> reqJS2; reqJS2 -> reqJS3; reqJS3 -> reqROUTES; reqROUTES -> reqROUTING1; reqROUTING1 -> reqADD1; 71

reqADD1 -> reqROUTING2; reqROUTING2 -> reqADD2; reqADD2 -> sendReply; sendReply[load] -1 A Server #edit s phpViewRoutes 0.09555 y phpViewRoutes dbViewRoutes 1.0 #edit s phpRouting1 0.21167 y phpRouting1 dbRouting1 1.0 #edit s phpAdd1 0.05342 y phpAdd1 dbAdd1 1.0 #edit s phpRouting2 0.18616 y phpRouting2 dbRouting2 1.0 #edit s phpAdd2 0.05343 y phpAdd2 dbAdd2 1.0 s phpN5s 0.0 y phpN5s n5s 1.0 s phpN6s 0.0 y phpN6s n6s 1.0 s phpN7s 0.0 y phpN7s n7s 1.0 s phpN8s 0.0 y phpN8s n8s 1.0 s phpN9s 0.0 y phpN9s n9s 1.0 s phpSendReply5 s phpSendReply6 s phpSendReply7 s phpSendReply8 s phpSendReply9 0.0 0.0 0.0 0.0 0.0

: phpViewRoutes -> phpN5s; 72

phpRouting1 -> phpN6s; phpAdd1 -> phpN7s; phpRouting2 -> phpN8s; phpAdd2 -> phpN9s; phpN5s -> phpN6s -> phpN7s -> phpN8s -> phpN9s -> phpSendReply5; phpSendReply6; phpSendReply7; phpSendReply8; phpSendReply9;

phpSendReply5[processViewRoutes]; phpSendReply6[processRouting1]; phpSendReply7[processAdd1]; phpSendReply8[processRouting2]; phpSendReply9[processAdd2] -1

73

Appendix 2: XDeb-Scenario (XDebug parameters)
G "OSM BikeRoutes, XDeb-Scenario (Total Client Think Time=7)" 0.001 5000 1 0.9 -1 P0 p pClient f i p pNetwork f i p pApache s -1 T0 t Browser r load -1 pClient z 7 m 150 t NetworkClient i n1c n2c n3c n4c n5c n6c n7c n8c n9c -1 t NetworkServer i n1s n2s n3s n4s n5s n6s n7s n8s n9s -1 t Server f sendHTML sendJS1 sendJS2 sendJS3 processViewRoutes processRouting1 processAdd1 processRouting2 processAdd2 -1 pApache m 150 t DB f dbViewRoutes dbRouting1 dbAdd1 dbRouting2 dbAdd2 -1 pApache m 100 -1 E0 A load reqHTML s n1c 0.0010 0.0 0.0 -1 y n1c sendHTML 1.0 0.0 0.0 -1 s n2c 5.0E-4 0.0 0.0 -1 y n2c sendJS1 1.0 0.0 0.0 -1 s n3c 5.0E-4 0.0 0.0 -1 y n3c sendJS2 1.0 0.0 0.0 -1 s n4c 5.0E-4 0.0 0.0 -1 y n4c sendJS3 1.0 0.0 0.0 -1 s n5c 0.0050 0.0 0.0 -1 y n5c processViewRoutes 1.0 0.0 0.0 -1 s n6c 0.0020 0.0 0.0 -1 y n6c processRouting1 1.0 0.0 0.0 -1 s n7c 0.0010 0.0 0.0 -1 y n7c processAdd1 1.0 0.0 0.0 -1 s n8c 0.0020 0.0 0.0 -1 y n8c processRouting2 1.0 0.0 0.0 -1 s n9c 0.0020 0.0 0.0 -1 y n9c processAdd2 1.0 0.0 0.0 -1 s n1s 0.0010 0.0 0.0 -1 s n2s 5.0E-4 0.0 0.0 -1 s n3s 5.0E-4 0.0 0.0 -1 s n4s 5.0E-4 0.0 0.0 -1 s n5s 0.0050 0.0 0.0 -1 74

s n6s 0.0020 0.0 0.0 -1 s n7s 0.0010 0.0 0.0 -1 s n8s 0.0020 0.0 0.0 -1 s n9s 0.0020 0.0 0.0 -1 s sendHTML 0.013 0.0 0.0 -1 y sendHTML n1s 1.0 0.0 0.0 -1 s sendJS1 0.0010 0.0 0.0 -1 y sendJS1 n2s 1.0 0.0 0.0 -1 s sendJS2 0.0070 0.0 0.0 -1 y sendJS2 n3s 1.0 0.0 0.0 -1 s sendJS3 0.0010 0.0 0.0 -1 y sendJS3 n4s 1.0 0.0 0.0 -1 A processViewRoutes phpViewRoutes A processRouting1 phpRouting1 A processAdd1 phpAdd1 A processRouting2 phpRouting2 A processAdd2 phpAdd2 #edit s dbViewRoutes 0.047882 0 0 -1 s dbRouting1 0.322489 0 0 -1 s dbAdd1 0.009164 0.0 0.0 -1 s dbRouting2 0.13638 0.0 0.0 -1 s dbAdd2 0.009222 0.0 0.0 -1 -1 A Browser s reqHTML 0.0 y reqHTML n1c 1.0 s reqJS1 0.0 y reqJS1 n2c 1.0 s reqJS2 0.0 y reqJS2 n3c 1.0 s reqJS3 0.0 y reqJS3 n4c 1.0 s reqROUTES 0.0 y reqROUTES n5c 1.0 s reqROUTING1 0.0 y reqROUTING1 n6c 1.0 s reqADD1 0.0 y reqADD1 n7c 1.0 s reqROUTING2 0.0 y reqROUTING2 n8c 1.0 75

s reqADD2 0.0 y reqADD2 n9c 1.0 s sendReply 0.0 : reqHTML -> reqJS1; reqJS1 -> reqJS2; reqJS2 -> reqJS3; reqJS3 -> reqROUTES; reqROUTES -> reqROUTING1; reqROUTING1 -> reqADD1; reqADD1 -> reqROUTING2; reqROUTING2 -> reqADD2; reqADD2 -> sendReply; sendReply[load] -1 A Server #edit s phpViewRoutes 0.107181 y phpViewRoutes dbViewRoutes 1.0 #edit s phpRouting1 0.137809 y phpRouting1 dbRouting1 1.0 #edit s phpAdd1 0.043590 y phpAdd1 dbAdd1 1.0 #edit s phpRouting2 0.146591 y phpRouting2 dbRouting2 1.0 #edit s phpAdd2 0.042121 y phpAdd2 dbAdd2 1.0 s phpN5s 0.0 y phpN5s n5s 1.0 s phpN6s 0.0 y phpN6s n6s 1.0 s phpN7s 0.0 y phpN7s n7s 1.0 s phpN8s 0.0 y phpN8s n8s 1.0 76

s phpN9s 0.0 y phpN9s n9s 1.0 s phpSendReply5 s phpSendReply6 s phpSendReply7 s phpSendReply8 s phpSendReply9 0.0 0.0 0.0 0.0 0.0

: phpViewRoutes -> phpN5s; phpRouting1 -> phpN6s; phpAdd1 -> phpN7s; phpRouting2 -> phpN8s; phpAdd2 -> phpN9s; phpN5s -> phpN6s -> phpN7s -> phpN8s -> phpN9s -> phpSendReply5; phpSendReply6; phpSendReply7; phpSendReply8; phpSendReply9;

phpSendReply5[processViewRoutes]; phpSendReply6[processRouting1]; phpSendReply7[processAdd1]; phpSendReply8[processRouting2]; phpSendReply9[processAdd2] -1

77

Appendix 3: SeparateDB-Scenario
G "OSM BikeRoutes, SeparateDB (Total Client Think Time=7)" 0.001 5000 1 0.9 -1 P0 p pClient f i p pNetwork f i p pApache s p pDB s p pDisk1 f p pDisk2 f -1 T0 t Browser r load -1 pClient z 7 m 80 t NetworkClient i n1c n2c n3c n4c n5c n6c n7c n8c n9c -1 t NetworkServer i n1s n2s n3s n4s n5s n6s n7s n8s n9s -1 t Server f sendHTML sendJS1 sendJS2 sendJS3 processViewRoutes processRouting1 processAdd1 processRouting2 processAdd2 -1 pApache m 150 t DB f dbViewRoutes dbRouting1 dbAdd1 dbRouting2 dbAdd2 -1 pDB m 100 t Disk1 f disk11 disk12 disk13 disk14 disk15 disk16 disk17 disk18 disk19 -1 pDisk1 t Disk2 f disk25 disk26 disk27 disk28 disk29 -1 pDisk2 -1 E0 A load reqHTML s n1c 0.000685 0.0 0.0 -1 s n2c 0.00007 0.0 0.0 -1 s n3c 0.000595 0.0 0.0 -1 s n4c 0.00017 0.0 0.0 -1 s n5c 0.001005 0.0 0.0 -1 s n6c 0.0000001 0.0 0.0 -1 s n7c 0.0000001 0.0 0.0 -1 s n8c 0.0000001 0.0 0.0 -1 s n9c 0.0000001 0.0 0.0 -1 s n1s 0.000685 0.0 0.0 -1 s n2s 0.00007 0.0 0.0 -1 s n3s 0.000595 0.0 0.0 -1 s n4s 0.00017 0.0 0.0 -1 s n5s 0.001005 0.0 0.0 -1 s n6s 0.0000001 0.0 0.0 -1 s n7s 0.0000001 0.0 0.0 -1 78

s n8s 0.0000001 0.0 0.0 -1 s n9s 0.0000001 0.0 0.0 -1 y n1c sendHTML 1.0 0.0 0.0 -1 y n2c sendJS1 1.0 0.0 0.0 -1 y n3c sendJS2 1.0 0.0 0.0 -1 y n4c sendJS3 1.0 0.0 0.0 -1 y n5c processViewRoutes 1.0 0.0 0.0 -1 y n6c processRouting1 1.0 0.0 0.0 -1 y n7c processAdd1 1.0 0.0 0.0 -1 y n8c processRouting2 1.0 0.0 0.0 -1 y n9c processAdd2 1.0 0.0 0.0 -1 s sendHTML 0.00962 0.0 0.0 -1 y sendHTML disk11 1.0 0.0 0.0 -1 y disk11 n1s 1.0 0.0 0.0 -1 s sendJS1 0.00085 0.0 0.0 -1 y sendJS1 disk12 1.0 0.0 0.0 -1 y disk12 n2s 1.0 0.0 0.0 -1 s sendJS2 0.0048 0.0 0.0 -1 y sendJS2 disk13 1.0 0.0 0.0 -1 y disk13 n3s 1.0 0.0 0.0 -1 s sendJS3 0.00064 0.0 0.0 -1 y sendJS3 disk14 1.0 0.0 0.0 -1 y disk14 n4s 1.0 0.0 0.0 -1 A processViewRoutes phpViewRoutes A processRouting1 phpRouting1 A processAdd1 phpAdd1 A processRouting2 phpRouting2 A processAdd2 phpAdd2 #edit s dbViewRoutes 0.02938 0 0 -1 s dbRouting1 0.25272 0 0 -1 s dbAdd1 0.00043 0.0 0.0 -1 s dbRouting2 0.05209 0.0 0.0 -1 s dbAdd2 0.00041 0.0 0.0 -1 #added disk s disk11 0.000005 0 0 -1 s disk12 0.000005 0 0 -1 s disk13 0.000005 0 0 -1 s disk14 0.00001 0 0 -1 s disk15 0.00003 0 0 -1 79

s disk16 0.00014 0 0 -1 s disk17 0.00016 0 0 -1 s disk18 0.000115 0 0 -1 s disk19 0.000165 0 0 -1 s disk25 0.00003 0 0 -1 s disk26 0.00014 0 0 -1 s disk27 0.00016 0 0 -1 s disk28 0.000115 0 0 -1 s disk29 0.000165 0 0 -1 y dbViewRoutes disk25 1 0 0 -1 y dbRouting1 disk26 1 0 0 -1 y dbAdd1 disk27 1 0 0 -1 y dbRouting2 disk28 1 0 0 -1 y dbAdd2 disk29 1 0 0 -1 -1 A Browser s reqHTML 0.0 y reqHTML n1c 1.0 s reqJS1 0.0 y reqJS1 n2c 1.0 s reqJS2 0.0 y reqJS2 n3c 1.0 s reqJS3 0.0 y reqJS3 n4c 1.0 s reqROUTES 0.0 y reqROUTES n5c 1.0 s reqROUTING1 0.0 y reqROUTING1 n6c 1.0 s reqADD1 0.0 y reqADD1 n7c 1.0 s reqROUTING2 0.0 y reqROUTING2 n8c 1.0 s reqADD2 0.0 y reqADD2 n9c 1.0 s sendReply 0.0 : reqHTML -> reqJS1; 80

reqJS1 -> reqJS2; reqJS2 -> reqJS3; reqJS3 -> reqROUTES; reqROUTES -> reqROUTING1; reqROUTING1 -> reqADD1; reqADD1 -> reqROUTING2; reqROUTING2 -> reqADD2; reqADD2 -> sendReply; sendReply[load] -1 A Server #edit s phpViewRoutes 0.09555 y phpViewRoutes dbViewRoutes 1.0 y phpViewRoutes disk15 1.0 #edit s phpRouting1 0.21167 y phpRouting1 dbRouting1 1.0 y phpRouting1 disk16 1.0 #edit s phpAdd1 0.05342 y phpAdd1 dbAdd1 1.0 y phpAdd1 disk17 1.0 #edit s phpRouting2 0.18616 y phpRouting2 dbRouting2 1.0 y phpRouting2 disk18 1.0 #edit s phpAdd2 0.05343 y phpAdd2 dbAdd2 1.0 y phpAdd2 disk19 1.0 s phpN5s 0.0 y phpN5s n5s 1.0 s phpN6s 0.0 y phpN6s n6s 1.0 s phpN7s 0.0 y phpN7s n7s 1.0 81

s phpN8s 0.0 y phpN8s n8s 1.0 s phpN9s 0.0 y phpN9s n9s 1.0 s phpSendReply5 s phpSendReply6 s phpSendReply7 s phpSendReply8 s phpSendReply9 0.0 0.0 0.0 0.0 0.0

: phpViewRoutes -> phpN5s; phpRouting1 -> phpN6s; phpAdd1 -> phpN7s; phpRouting2 -> phpN8s; phpAdd2 -> phpN9s; phpN5s -> phpN6s -> phpN7s -> phpN8s -> phpN9s -> phpSendReply5; phpSendReply6; phpSendReply7; phpSendReply8; phpSendReply9;

phpSendReply5[processViewRoutes]; phpSendReply6[processRouting1]; phpSendReply7[processAdd1]; phpSendReply8[processRouting2]; phpSendReply9[processAdd2] -1

82

