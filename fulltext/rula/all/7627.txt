An Efficient Tau-Leaping Simulation Method for Stochastic Biochemical Kinetics by Serguei Rousskikh Bachelor of Science, Ryerson University, 2015 A thesis presented to Ryerson University in partial fulfilment of the requirements for the degree of Master of Science in the program of Applied Mathematics

Toronto, Ontario, Canada, 2018 c Serguei Rousskikh, 2018

Declaration
AUTHOR'S DECLARATION FOR ELECTRONIC SUBMISSION OF A THESIS

I hereby declare that I am the sole author of this thesis. This is a true copy of the thesis, including any required final revisions, as accepted by my examiners.

I authorize Ryerson University to lend this thesis to other institutions or individuals for the purpose of scholarly research.

I further authorize Ryerson University to reproduce this thesis by photocopying or by other means, in total or in part, at the request of other institutions or individuals for the purpose of scholarly research.

I understand that my thesis may be made electronically available to the public.

ii

Abstract
An Efficient Tau-Leaping Simulation Method for Stochastic Biochemical Kinetics Serguei Rousskikh Master of Science in Applied Mathematics Ryerson University 2018

Stochastic modeling and simulation of biochemical systems are topics of high interest in Computational Biology. Stochastic mathematical models are critical in accurately capturing the variability observed experimentally in cellular processes, in particular when some species have low molecular numbers. Many, realistic biochemical networks exhibit stiffness, due to the presence of multiple time-scales. For such networks explicit simulation methods are computationally quite intensive. In this thesis, we introduce an improved implicit tau-leaping strategy for the simulation of stochastic biochemical kinetic models. Numerical tests on various biochemical systems of interest in applications show the efficiency of our method.

iii

Acknowledgements
I would like first of all to express my appreciation to everyone who has supported me in my studies and beyond, and without whom this thesis would not have been possible. I would also like to take the time to posthumously acknowledge the late Dr. Daniel Gillespie, the pioneer and architect of this research topic.

I would like to begin by expressing my outmost gratitude to my supervisor Dr. Silvana Ilie who has spent much time, energy and resources in order to give me the best possible opportunity to succeed in this project. I thank her for the support given to me throughout this endeavor as well as the remarkable poise and patience she demonstrated during very trying times and during periods when I clearly was not at my best.

Subsequently, I would like to thank all of my classmates, friends and all with whom I have had the pleasure to work closely with over the past two years. I have had a wonderful time, met some extraordinary people and have had the chance to learn a lot from them in this time.

iv

Finally, I would like to thank all of my family, here with me in Canada and back home in Russia. In particular I would like to thank my mother and father who have sacrificed so much to give me the best opportunities in life. The patience, guidance and support that they give me on a daily basis plays an immeasurable role in all aspects of my life. A very special mention goes to my little sister, who has witnessed the ups and downs of this process firsthand, and has always been there to provide a shoulder especially during the downs. I cannot end this without expressing my gratitude to my grandparents, who despite being thousands of miles away are aware of every little intricate detail surrounding my master's degree.

v

Dedication
To all that I have mentioned I dedicate this thesis.

vi

Table of Contents

Abstract

ii

List of Tables

x

List of Figures

xi

1 Introduction to Biochemical Systems 1.1 1.2 1.3 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Review of Stochastic Simulation Techniques . . . . . . . . . . . . . 1.3.1 1.3.2 1.4 Exact Methods . . . . . . . . . . . . . . . . . . . . . . . . . Approximate Methods . . . . . . . . . . . . . . . . . . . . .

1 1 4 9 9 10 11

Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 Mathematical Background 2.1 2.2 2.3 Probability Models . . . . . . . . . . . . . . . . . . . . . . . . . . . Monte Carlo Method . . . . . . . . . . . . . . . . . . . . . . . . . . Introduction to Stochastic Processes 2.3.1 Markov Process Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

12 12 18 20 20

vii

2.3.2 2.3.3

Markov Process Notation . . . . . . . . . . . . . . . . . . . . Markov Processes: Continuous Time, Finite State-Space . .

22 26

3 Biochemical Systems Background 3.1 3.2 3.3 3.4 3.5 3.6 Chemical Master Equation . . . . . . . . . . . . . . . . . . . . . . . Stochastic Simulation Algorithm . . . . . . . . . . . . . . . . . . . . Tau-Leaping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Chemical Langevin Equation . . . . . . . . . . . . . . . . . . . . . . Reaction Rate Equation . . . . . . . . . . . . . . . . . . . . . . . . Potential Applications . . . . . . . . . . . . . . . . . . . . . . . . .

32 33 40 49 51 53 58

4 Algorithms and Models 4.1 4.2 Explicit Tau-Leaping . . . . . . . . . . . . . . . . . . . . . . . . . . Implicit Tau-Leaping . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2.1 4.2.2 4.3 4.4 Stiffness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Newton's Method . . . . . . . . . . . . . . . . . . . . . . . .

61 62 67 67 68 74 79

Adaptive Explicit-Implicit Tau-Leaping Method . . . . . . . . . . . Modified Adaptive Tau-Leaping Method . . . . . . . . . . . . . . .

5 Numerical Results 5.1 5.2 5.3 5.4 Stiff Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Decay-Dimerization Model . . . . . . . . . . . . . . . . . . . . . . . Modified Cycle Model . . . . . . . . . . . . . . . . . . . . . . . . . Table of Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

87 87 92 95 99

6 Conclusion and Further Research Topics viii

100

Bibliography

103

ix

List of Tables

5.1

Computational times of the SSA, Adaptive and Modified Tau-Leaping Methods. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99

5.2

Improvement in computational speed of the adaptive tau-leaping method vs. SSA. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99

x

List of Figures

5.1

Stiff Model: Histogram of the X1 species (SSA vs. Adaptive TauLeaping vs. Modified Adaptive Tau-Leaping) at t=0.01 . . . . . . . 90

5.2

Stiff Model: Histogram of the X2 species (SSA vs. Adaptive TauLeaping vs. Modified Adaptive Tau-Leaping) at t = 0.01 . . . . . . 91

5.3

Stiff Model: Histogram of the X3 species (SSA vs. Adaptive TauLeaping vs. Modified Adaptive Tau-Leaping) at t = 0.01 . . . . . . 91

5.4

Decay-Dimerization model: Histograms of X1 species (SSA vs. Adaptive Tau-Leaping vs. Modified Adaptive Tau-Leaping) at t = 3 . . . 94

5.5

Decay-Dimerization model: Histograms of X3 species (SSA vs. Adaptive Tau-Leaping vs. Modified Adaptive Tau-Leaping) at t = 3 . . . 94

5.6

Modified Cycle model: Histograms of X1 species (SSA vs. Adaptive Tau-Leaping vs. Modified Adaptive Tau-Leaping) at t = 0.05. . . . 97

5.7

Modified Cycle model: Histograms of X2 species (SSA vs. Adaptive Tau-Leaping vs. Modified Adaptive Tau-Leaping) at t = 0.05. . . . 98

5.8

Modified Cycle model: Histograms of X3 species (SSA vs. Adaptive Tau-Leaping vs. Modified Adaptive Tau-Leaping) at t = 0.05. . . . 98

xi

Chapter 1 Introduction to Biochemical Systems

1.1

Motivation

Why is this topic so important? First and foremost, biochemical systems are at the heart of modern medicine and biomedical research. The medical and biomedical industry is already one of the highest grossing and socially vital economic sectors in the world. As the global population continues to age, particularly in North America and Europe, the reliance upon this industry will only continue to grow. Biochemical research already provides us with the basis for revolutionary medical procedures, treatments and medication. Yet, with all of the progress that has been made in recent history, the field still has boundless potential. In this thesis we strive to build on the massive amount of work already done with respect to biochemical simulation, and develop a more efficient way of achieving results.

1

Aside from the broad economic and medical implications of computational biology, the motivational factor at the heart of this research is our interest in improving stochastic simulation of mathematical models of biochemical systems. In particular, our goal is to identify and correct inefficiencies in stochastic methods. The focus of our study will be the tau-leaping method developed by Gillespie [13] for simulating stochastic discrete models of biochemical kinetics.

In the study of biochemical networks we encounter a wide spectrum of systems and a range of numerical strategies to approximate the solution of their mathematical models. Biochemical systems can be categorized into spectrum ranging from small systems with few species and small population sizes to large systems with many species and large populations sizes. On the small system side of the scale, an appropriate solution algorithm would be the stochastic simulation algorithm (SSA) [9], while on the opposite end one would often utilize ordinary differential equation (ODE) solvers for the reaction rate equations (RRE). These methods have been extensively studied and are currently heavily relied upon in the industry. This however, does not mean that these techniques and others do not come without their drawbacks. Thus, we turn our attention to the tau-leaping scheme. At its core, the tau-leaping strategy is an improvement on the SSA. The SSA is built upon the premise that we carry out each reaction consecutively, whereas the tauleaping mechanism allows us to jump several reactions ahead under certain conditions. The tau-leaping method addresses two shortcomings of the SSA in particular,

2

1) the SSA is extremely slow for a large number of reactant species and/or large molecular amounts of certain species [13]; 2) the tau-leaping method, especially the implicit variation, is far better suited to approximate stiff problems. Referencing what was stated a few sentences before, no method comes without its faults, and the tau-leaping methods are no different. Within the realm of this thesis we strive to identify the inefficiencies associated with existing tau-leaping methods, on our way to establishing a modified tau-leaping algorithm capable of being an accurate and efficient alternative for a wider range of biochemical systems.

Finally, as with any other research undertaking we are motivated by a desire to advance the field of computational biology and build on the work done by scientists before us (McAdams & Arkin [21]; McAdams & Arkin [22]; Arkin et al. [23]; Elowitz et al. [6]; Fedoroff & Fontana [7]). In the next few paragraphs, we will briefly explore the work of other computational biologists and the influence that their work has had on this particular research area as well as the field in general.

Our first motivational piece comes from Harley H. McAdams and Adam Arkin and their work concerning modeling of genetic activity. In 1997, they published a paper titled "Stochastic mechanisms in gene expression" where they proposed that the pattern of protein concentration, essential in controlling the promoter, which in turn is responsible for gene expression, can be modeled using stochastic processes at varying time intervals [21]. In 1998, McAdams and Arkin with the help of John Ross explored the effect of fluctuations (noise) in rates of gene expression and con-

3

sidered molecular level stochastic modeling as a noise-modeling mechanism [22]. McAdams and Arkin followed this up with a paper in 1999, "It's a noisy business". They again explored the effects of noise on reaction rates and subsequently gene expression, but this time on a nanomolar level [23].

In 2002, Michael B. Elowitz along with his team expanded on the research done by McAdams and Arkin in a paper titled "Stochastic gene expression in a single cell". The endeavour focused on noise that arises from stochasticity. The team analysed both intrinsic (stochasticity inherent in the biochemical process of gene expression) and extrinsic (fluctuations in other cellular components) noise [6]. Their results established a quantitative base for modeling noise in gene expression and revealed how low intracellular copy numbers of molecules can fundamentally limit the precision of gene regulation. A similar study was conducted by Fedoroff and Fontana titled "Small Numbers of Big Molecules" in 2002 [7].

1.2

Introduction

In the previous section we shed light upon the influence of biomedicine and biochemical systems on healthcare, the economy and laid out our motivations for choosing stochastic modeling of biochemical kinetics as a research topic. In this section we will present the key concepts at high level, in order to facilitate a big picture understanding of the subject for the reader.

Consider a biochemical system, which contains a system of N different types of 4

molecules, known as chemical species, which are involved in M types of chemical reactions. We can think of this as a simple mathematical equation where different numbers through various mathematical operations give some result, for instance x + y = z + u. If we have several equations, then we have a system of equations, which we can then solve using a variety of methods available at our disposal. Similarly, we have chemical equations, where reactants (left side of the equation) go through a chemical reaction to generate some products (right side of the equation). For instance, chemical x reacts with chemical y to produce chemicals z and u (this is expressed as x + y  z + u). If we have several reactions, then we have a chemical system. Our goal is to model the evolution of the population of each of the chemical species. The most accurate approach to modeling the effects of such systems on molecular populations is to track the position and velocity of each individual molecule while allowing it to evolve under the appropriate laws of physics [14]. Subsequently, we monitor the reactions as they take place and make the necessary observations. This approach is known as the "molecular dynamics" approach. Naturally, we can deduce that even with a moderate species population this approach becomes far too time-consuming when simulated on larger time intervals, relevant in applications. However, we can often simplify the problem by ignoring the spatial information and tracking the population size of each type of molecule as a function of time. We can achieve this by making three critical assumptions. The first being, the well-stirred assumption, which states that molecules are uniformly spread throughout the spatial domain [14]. In order to disregard the spatial aspect, we must also assume thermal equilibrium and constant volume throughout the reac-

5

tion system [16]. With these assumptions, the behaviour of the biochemical system may be described using the Chemical Master equation (CME)[10].

As was aforementioned rather than taking a brute force approach known as the molecular dynamics approach, we are interested in calculating the number of molecules for each molecular species at a given time; this is known as the system state. The Chemical Master equation (CME) is a large system of ordinary differential equations (ODE s), with one equation for every state [10]. The CME, as we stated before, is a valid model under three key assumptions: the system is well-stirred, it is in thermal equilibrium and the volume remains constant throughout the reaction. The issue with the CME is that once the system or the population size becomes large, the system of ODE s cannot be solved analytically and are computationally very challenging to simulate directly.

This problem remained unsolved, until the breakthrough work of Dr. Daniel Gillespie. Dr. Gillespie is a renowned American physicist, with a Ph.D. from Johns Hopkins University. His research took him to some of the leading technical universities in the United States, such as the California Institute of Technology (Caltech) and the University of California Berkeley. In 1976, his research culminated in the establishment of the stochastic simulation algorithm [9]. The SSA computes only a single realization of the state vector rather than the entire probability distribution [16]. This method allowed scientists the opportunity to simulate chemical reactions stochastically. However, the SSA is not without faults of its own. While an exact

6

solution of the CME, it comes at the price of high computational cost, particularly when dealing with large species populations. It is this high cost, which leads us to the tau-leaping methods.

Theoretically, one way that we can accelerate the simulation, in contrast to the SSA, is by having more than one reaction fire during a time-step [14]. Mathematically, we can achieve this by making the time-step ( ) larger. This strategy is known as the tau-leaping method. It was also developed by Dr. Gillespie as an answer to the computational issues associated with the SSA. To avoid compromising the accuracy of the method, there is a restriction that we must placed upon the length of the step  . Known as the leap condition, this restriction states that one must ensure  is small enough such that the propensity functions will not change significantly. Naturally, there are other safeguards within the algorithm that guarantee the accuracy of the method, such as error percentages. In this thesis, we will reference different types of tau-leaping algorithms. In particular, the explicit [14], implicit [27] and adaptive tau-leaping methods,[2, 3]. Each of these strategies is especially effective under specific conditions and, as was stated previously, our main objective is to develop a modified algorithm. The modified algorithm will incorporate elements of the former and correct inefficiencies associated with each technique. Given that tau-leaping will serve as the keystone element of this research undertaking, this scheme will be discussed in much greater depth in the subsequent chapters.

Tau-leaping has been referred to in the past as a "bridge process" [14]. This is

7

indeed correct. Tau-leaping serves as the bridge between the CME and the Chemical Langevin equation (CLE) [13] a stochastic continuous model. In the previous section we have established that over the new time-step [t, t +  ] we have more than one firing of a reaction. Additionally, we also make the assumption that the mean of the Poisson random variables in the tau-leaping method is large. Once again we will delve deeper into this in consequent chapters; for now, however, probability theory dictates that a Poisson random variable can be accurately approximated by a normal random variable with the same mean and variance. This assumption serves as the backbone for the CLE. While our previous processes were discrete and stochastic in nature, the normal random variable approximation has turned the system state into a continuous and stochastic process.

While tau-leaping is the bridge to the CLE, it ultimately acts as the bridge between the CME and the reaction rate equations (RRE). We have seen that under certain assumptions, through probabilistic approximation, a discrete and stochastic process was reduced to a stochastic and continuous one. Now, if we were to disregard the stochastic term of the CLE we reduce the model to the RRE, a continuous and deterministic model. However, it is imperative to note that this reduction is valid under a key assumption, the thermodynamic limit [14]. In Chapter 3 we will describe the thermodynamic limit mathematically and how it facilitates the reduction to the reaction rate equations.

8

1.3

Review of Stochastic Simulation Techniques

This final section of the introductory Chapter will serve as a review or a quick tutorial on the different types of stochastic simulation techniques at our disposal. Some will be familiar, for instance the stochastic simulation algorithm (SSA), while some may be new to the reader. We will break this section down into two categories, exact methods and approximate methods.

1.3.1

Exact Methods

Before we begin, it is important to emphasize that when it comes to simulations there is no such thing as an "exact" algorithm. The first exact method that we mention is Gillespie's stochastic simulation algorithm (SSA). We briefly discussed this method in the previous section and will discuss it at much greater length in Chapter 3. For now, all we will add, is that this method epitomizes accuracy and if we were given unlimited computational capability this would be the method of choice.

Next we move to a method that can be thought of as the predecessor to the SSA, the first reaction method, also developed by Gillespie [14]. The algorithm computes the time i at which a reaction could be occurring, barring any reaction firing. Subsequently, the index j of the first reaction is directly correlated to the index of the reaction with the shortest time to reaction. Finally, we examine the exact method developed by Michael A. Gibson and Jehoshua Bruck, called the next reaction method [8]. The next reaction method is a modification of the aforementioned 9

first reaction method. This method has the computational time proportional to the logarithm of the number of reactions log(M ). This is accomplished by constructing a dependency graph from the set of reactions and incorporating an appropriate data structure capable of amassing the propensities ai and possible times i . This method is unique for two particular reasons, i) it uses only a single random number per simulation event, and ii) its computational time is proportional to the logarithm of the number of reactions rather than the number of reactions itself.

1.3.2

Approximate Methods

Methods of this category are designed to approximate the exact solution as well as possible, while being far more efficient than their exact counterparts. The first type of approximate schemes that we mention are the various tau-leaping methods that are at the heart of this work. Other methods are presented for the CME, CLE and RRE. We briefly delved into these topics earlier, and we dissect these methods much more meticulously later on.

In this thesis we propose a modified adaptive explicit-implicit tau-leaping strategy to simulate a large class of well-stirred biochemical systems. Our work improves the state-of-the-art adaptive tau-leaping strategy [3], by eliminating the need for symbolic computation the Jacobian required by Newton's method for the implicit tau-leaping technique. For this we use the finite-difference approximations. Our results show that the new method maintains a similar accuracy and computational cost, with minimal intervention from the user.

10

1.4

Outline

This thesis will be constructed in the following way. The subsequent chapter will entail a comprehensive review of the mathematical background, which will form the basis for the rest of the thesis. The mathematical review will contain a discussion on probability theory, Monte Carlo simulation techniques and stochastic processes. In Chapter 3, we will delve into the background of biochemical systems including in-depth analysis and rigorous derivations of the CME, SSA, tau-leaping, CLE and the RRE. Chapter 4, will form the core of the of this thesis, with the introduction of several tau-leaping strategies notably featuring the state-of-the art adaptive explicit-implicit tau-leaping method followed by the new user-friendly modified adaptive explicit-implicit tau-leaping scheme. In the following Chapter, we present the numerical results which will justify the theoretical component of this thesis. The final chapter, will entail rationalization the preceding work and contemplation of topics for future study.

11

Chapter 2 Mathematical Background
In this Chapter we cover the necessary mathematical theory for studying stochastic modelling and simulation of biochemical systems. To begin this section we review the necessary probability distributions, upon which our theories are based. Consequently, we consider the Monte Carlo method, which is essential to the numerical simulation of the stochastic models of biochemical kinetics. Finally, we discuss several properties of stochastic processes that are key to stochatic modelling of biochemical processes.

2.1

Probability Models

In this Section we explore three probability distributions, which will prove to be critical in the derivations and proofs of future concepts. In particular, we consider exponential, normal and Poisson distributions. Nonetheless, we begin with several crucial definitions, including, probability density and mass functions, cumulative distribution function and expectation (we refer the reader to [30] for more details). 12

Definition 2.1.1. Probability mass function [30]. For any discrete random variable X, we define the probability mass function (PMF) to be the function which gives the probability of each x  SX (where SX is the set of possible observed values for X ). We denote this as,

P ( X = x) =
{sS |X (s)=x}

P ({s}),

where S represents any arbitrary sample set. This function has a continuous counterpart, known as the probability density function (PDF), which serves the same purpose, only in continuous space. Definition 2.1.2. Probability density function [30]. If X is a continuous random variable, then there exists a function fX (x), called the probability density function, which satisfies the following conditions,

1. fX (x)  0,  x for any real number;

2.

 -

fX (x)dx = 1;

3. P (a  X  b) =

b a

fX (x)dx for any a  b.

A related concept, is the cumulative distribution function (CDF). The CDF applies to both discrete and continuous distributions. Definition 2.1.3. Cumulative distribution function [30]. The cumulative

13

distribution function for a discrete random variable is signified by,

FX (x) = P (X  x) =
{y S |y x}

P (X = y ).

The continuous analogue has the following structure,

FX (x) = P (X  x) = P (-  X  x)
x

=
-

fX (z )dz.

The expectation is the average of the random variable we are interested in, it should not be confused with the sample mean. Again, we combine the discrete and continuous interpretations of expectation into one definition. Definition 2.1.4. Expectation [30]. The expectation of a discrete random variable X, designated by E(X) is denoted by,

E (X ) =
{xSX }

xP (X = x).

The expectation of the continuous analogue is as follows,



E (X ) =
-

xfX (x)dx.

Now, we discuss some important probability distributions. We begin with the exponential distribution. In general a random variable, X, that is exponentially

14

distributed is represented as [30],

X  Exp(),

where  is the rate parameter. The exponential distribution has the probability density function, e-x , x  0; fX (x) = 0, otherwise. The exponential has the cumulative distribution function,

0, FX (x) =

x < 0,

1 - e-x , x  0,

and expected value, E (X ) = 1 . 

The exponential distribution will prove to be essential in the derivation of the stochastic simulation algorithm.

The Poisson distribution is equally important, for, it will serve as the backbone for our tau-leaping methods. This discrete probability distribution, has a parameter , and a Poisson random variable is expressed as,

X  Po ().

15

Since it is discrete, it has an associated PMF given by,

P (X = k ) =

k - e , k!

for k = 0, 1, 2, 3, . . . .

The expectation value is , that is,

E (X ) = .

The final distribution to be discussed is the normal (or Gaussian) distribution. This probability distribution will serve as the bridge from tau-leaping to the Chemical Langevin Equation. A normal random variable X with mean х and variance  2 , is denoted by, X  N х,  2 .

The associated PDF is written as,

fX (x) =

1 2 exp 

-

1 x-х 2 

2

Through integration of


xfX (x)dx,
-

we obtain the expected value of the normal distribution X,

E (X ) = х.

As it was already mentioned, the normal distribution is essential in making the 16

transition from tau-leaping to the CLE. This is achieved through the following proposition. Proposition 2.1.1. Normal approximation of Poisson distribution [30]. The Poisson distribution can be approximated by the normal distribution in circumstances where the mean of the Poisson distribution is large; in general the mean has to be greater than a threshold, more precisely,

X  Po ()

N (, ),

f or  > 20.

In the context of this thesis, a formal proof is unnecessary, nonetheless we will present the reasoning behind this property. The Poisson distribution is derived from the binomial distribution. Using the Central Limit Theorem it can be proven that the Binomial Theorem is well approximated by the normal distribution if the number of successes (n) is large. Given, that the Poisson distribution is derived from the binomial distribution, it also possesses this property. The only discrepancy is the fact that the Poisson is approximated with the parameter  as opposed to the number of successes.

With this we conclude our review of probabilistic methods and distributions. The next section will delve into the Monte Carlo method and its application to the simulation of stochastic models of biochemical systems.

17

2.2

Monte Carlo Method

The Monte Carlo scheme is a broad class of computational algorithms that use random distribution over a large number of iterations, taking the average of said iterations and providing the desired results. An early variant of the method was first used in Buffons needle experiment, and subsequently again in the 1930 s by Enrico Fermi when studying neutron diffusion [1]; however, its use was not acknowledged. Physicists Stanislaw Ulam and John von Neumann working at the Los Alamos Scientific Laboratory in Los Alamos, New Mexico on a project regarding radiation shielding, first coined the term Monte Carlo, which was a code name for their work [25].

As was mentioned in the previous paragraph, the Monte Carlo strategy was first applied to physics. Nevertheless, over time it has been increasingly applied to fields other than physics such as computational biology and financial analytics. In regards to systems biology, the Monte Carlo method is vital to the implementation, among others, of the stochastic simulation algorithm and various tau-leaping methods.

Since the Monte Carlo technique proves to be a useful computational tool, let us present it briefly below. Monte Carlo simulation is a strategy by which we simulate many different realizations of the stochastic process of interest. We accomplish this through random number generation from the appropriate probability distribution and subsequent application within the parameters of the problem we wish to solve. Our desired outcome is a probability distribution of the results.

18

The Monte Carlo method is a crucial step in the implementation of both the SSA and subsequent tau-leaping methods. In each case we wish to construct a distribution of the number of species remaining following their evolution through time in a biochemical system. Each method will adhere to a similar structure. In Sections 3.2 and 3.3 we will provide a detailed description of the algorithms, but for now we focus strictly on the Monte Carlo component. Our initial step is to define all the variables and set the parameter values for the simulation. In this case, the parameters will include the number of species and reaction channels, the initial conditions, the stoichiometric matrix and the reaction rate constants. The variables are the molecular amounts of the biochemical species, depending on the time t. After initialization, the algorithm simulates, the various trajectories species populations can take. In biochemical systems the accepted practice is to simulate ten thousand trajectories. While this number may not seem huge, especially, when by comparison, Monte Carlo simulations in financial mathematics require upwards of a million trajectories, it will more than suffice. We have to be mindful of the fact that we are dealing with tiny molecules, where for instance a 5% error is insignificant, while in financial terms a 5% error on a billion dollar deal could potentially have a devastating impact. Using the ten thousand trajectories simulated, we can construct a probability distribution based on the results.

In conclusion, the Monte Carlo method is essentially a practical manifestation of the Law of Large Numbers fused with relatively simple statistics tools. Nevertheless, it

19

is an indispensable part of our computational capabilities.

2.3

Introduction to Stochastic Processes

This section gives a brief introduction to stochastic processes, which are used in stochastic modeling approaches of biochemical systems. Particular attention is placed upon comprehension, derivation and application of Markov processes and Kolmogorov's equations. The interested reader is referred to [30] for more details.

2.3.1

Markov Process Introduction

Introductory sections dedicated to probability theory and computational aspects of stochastic modeling, allows us to delve deeper into stochastic theory before transitioning to biochemical systems theory. A stochastic process is a random variable, in this case, the state change vector X(t),   x1 (t)    x (t)  2 X(t) =   .  .  .   xN (t)            

which evolves in time [16]. The random variable, or hereafter, the system state vector, can either be continuous or discrete. Definition 2.3.1. Markov Process [30]. A Markov process is a stochastic process that possesses the property that future states do not depend on the past states,

20

given the present state. In other words, Markov processes can be thought of as history-less. It is known that Markov processes model the behaviour of biochemical kinetics remarkably well and will serve as the underlying theoretical foundation to our research.

Armed with a general understanding of what stochastic and Markov processes are, the next task is to construct a mathematical framework based on what was stated earlier. Assume that the set

{(n) |n = 0, 1, 2, 3, . . . }

is a discrete time stochastic process. It is worth repeating that the state space S , is such that (n)  S , for all n can be continuous or discrete. A first order Markov chain, is a stochastic process where future states are only dependent on the present state,

P ((n+1) A|(n) = x, (n-1) = xn-1 , . . ., (0) = x0 ) = P ((n+1) A|(n) = x), (2.1)

where A  S . The first order Markov chain depends on A, x and n. However, if the process is independent of n, then,

P ((n+1) A|(n) = x) = P (x, A).

(2.2)

21

In this case, the Markov Chain is said to be (time) homogeneous, and the transition kernel P (x, A) determines the behaviour of the chain [30].

2.3.2

Markov Process Notation

If S is discrete, the following notation is used,

P (x, y ) = P ((n+1) = y |(n) = x).

(2.3)

Furthermore, assuming the presence of a discrete and finite state space, S = {x1 , . . . , xm }, probability P can be rewritten in matrix form   P (x1 , x1 ) P (x1 , x2 )    P (x , x ) P (x , x )  1 1 2 2 P =  . .  . . . .    P (xm , x1 ) P (xm , x2 )  . . . P (x1 , xm )    . . . P (x2 , xm )   .  . ..  . . .    . . . P (xm , xm )

Matrix P is known as a stochastic matrix. Definition 2.3.2. Stochastic Matrix [30]. An m О m matrix P is a stochastic matrix if its entries are non-negative and the sum of all of its elements on each row equal to 1. Proposition 2.3.1. 1) If P1 , P2 are m О m stochastic matrices then, the product of P1 and P2 is also a stochastic matrix. 2) For all eigenvalues () of a stochastic matrix P satisfy ||  1. 3) For a stochastic matrix P, there exists at least one eigenvalue  = 1. 22

Proof. Proof of Proposition 2.3.1.

Suppose we take an eigenvalue, , of a stochastic matrix P, there exists a vector x = 0 such that P x = x. Let us denote by || и || the matrix -norm, and by || и ||V  the -norm for m-dimensional column vectors. It then follows that

V V V ||P x||V  = ||x||  || и ||x|| = ||P x|| .

(2.4)

||P || = max
x=0 V ||P x||V   ||P || и ||x||

||P x||V ||P x||V    V ||x||V || x ||   for any x = (0, 0, . . . , 0)T (2.5)

Substituting (2.4) into (2.5), we get.

V V || и ||x||V  = ||P x||  ||P || и ||x||

T where ||x||V  = (0, 0, . . . , 0)

||  ||P || = max(P11 + P12 + и и и + P1m , P21 + P22 + и и и + P2m , ..., Pm1 + Pm2 + и и и + Pmm ) = max(1, 1, . . . , 1) =1 Thus, ||  1.

23

Now we set up the basis for the Chapman - Kolmogorov equations. Let us define, for time tn [30], P ((n) = x1 ) =  (n) (x1 ) P ((n) = x2 ) =  (n) (x2 ) . . . P ((n) = xm ) =  (n) (xm )  (n) = ( (n) (x1 ),  (n) (x2 ), . . . ,  (n) (xm )) at time tn .

Then,

P ((n+1) = x1 ) =P ((n) = x1 )P (x1 , x1 )+ +P ((n) = x2 )P (x2 , x1 )+ +иии+ +P ((n) = xm )P (xm , x1 )

  P (x1 , x1 )    P (x , x )  2 1 (n) (n) (n) =( (x1 ),  (x2 ), . . . ,  (xm )) и   .  . .    P (xm , x1 ))

           

24

  P (x1 , x1 )    P (x , x )  2 1 (n+1) (n+1) (n)   (x1 ) = P ( )= и .  . .    P (xm , x1 ))   P (x1 , x2 )    P (x , x )  2 2 (n+1) (n)   (x2 ) =  и  .  . .    P (xm , x2 )) . . .   P (x1 , xm )    P (x , x )  2 m (n+1) (n)   (xm ) =  и  .  . .    P (xm , xm )                        

           

 (n+1) =  (n+1) (x1 ),  (n+1) (x2 ), . . . ,  (n+1) (xm ) =    P (x1 , x1 ) P (x1 , x2 )    P (x , x ) P (x , x )  1 1 2 2 (n)  = и . .  . . . .    P (xm , x1 ) P (xm , x2 ) . . . P (x1 , xm )    . . . P (x2 , xm )     . ..  . . .    . . . P (xm , xm )

  (n+1) =  (n) и P

25

Therefore, we obtain

 (n+1) =  (n) и P =  (n-1) и P и P =  (n-1) и P 2 =  (n-2) и P 3 = и и и =  (0) и P (n+1)

 (n) =  (0) P (n+1)

(2.6)

Why does this result bear significance? Equation (2.6) above, states that the initial state and the stochastic matrix P determine future probability distributions. Equipped with this knowledge, it can be deduced that if one step is dependent on P, then two steps are dependent on P 2 and the nth -step is determined by P n , accordingly. Furthermore, suppose we have two different step sizes, for instance n and p, then P n и P p = P (n+p) [30]. And this statement is of the utmost importance to us because it forms the basis for the Chapman - Kolmogorov equations, which are vital in deriving the stochastic discrete model of well-stirred biochemical kinetics, namely the Chemical Master equation.

2.3.3

Markov Processes: Continuous Time, Finite StateSpace

This thesis deals with stochastic processes that are continuous in time and have finite state-spaces. Therefore, it is necessary to get acquainted with this concept. More details on these processes may be found in [30]. Definition 2.3.3. A stochastic process X (t) is a Markov process continuous in time

26

if,

P (X (t + dt) = x|X (s) = x(s)|s  [0, t]) = P (X (t + dt) = x|X (t) = x(t)), t  [0, ), x  S (2.7)

where S is the state space, S = {1, 2, . . . , m}. Identically to the discrete cases covered earlier, future behaviour of the process does not depend on the past states, if the current state is known. Consider a process which is characterized by one of the m states denoted earlier, if at time t it is in the state x  S , then future behaviour will be contingent upon the transition kernel,

p(x, t, x , t )  P (X (t + t ) = x |X (t) = x),

the notation P (X (t + t ) = x |X (t) = x) means the conditional probability X (t + t ) = x given that X (t) = x. If the transition kernel is independent of t, then it is considered to be homogeneous, and can be written as p(x, x , t ). For each t the transition is denoted by P(t'), a m О m matrix. There are a few properties which can be attributed to the transition kernel. First of all,  1   0  P (0) =  . . .   0  0 . . . 0   1 . . . 0  =I  . . . . . . .. .    0 ... 1

where I is an m О m identity matrix. This is intuitive as no transition can take 27

place in the absence of time. Similarly to what was stated in the previous section, we can carry out regular multiplication operations with P because it is a transition matrix for each value of t. The latter sentence gives the following identity,

P (t + t ) = P (t) и P (t ) = P (t ) и P (t).

Let us denote by, Q, the transition rate matrix or just the rate matrix. The rate matrix is defined to be [30] Q := d P (t ) dt .
t =0

(2.8)

Thus,

Q = lim

P (t) - P (0) t0 t P (t) - I = lim t0 t

and therefore, P (dt) = I + Qdt. (2.9)

It should be stated that P (dt) is a stochastic matrix (a matrix where its entries are non-negative and the sum of all of its elements on each row equal to 1). From this we can make a few deductions. First, given that the identity matrix I consists of zeros, aside from the diagonal, leads us to the realization that off-diagonal elements of Q are also non-negative. Secondly, since diagonal elements of P (dt) are bounded above by 1, then Q's diagonal elements must be non-positive. Lastly, knowing that all rows of P (dt) and I sum to 1, logic dictates that the rows of Q must sum to 0.

28

These properties must be satisfied by a rate matrix Q. Indeed,

1 = sum of elements in one row of P = sum of all elements in same row of I + (sum of all elements in same row of Q)dt = 1 + (sum of all elements in a row of Q)dt = (sum of all elements in a row of Q) = 0.

Using equation (2.9) we can calculate the stationary distribution of the Markov chain. If  is the stationary distribution of P (dt) it the follows that [30],

P (dt) = .

Since, P (dt) =  + Qdt = 

= Qdt = 0,

where we know dt = 0 = Q = 0.

It should be noted here that  is a vector.

29

We can write,

d P (t + dt) - P (t) P (t) = dt dt P (t + dt) - P (t) = lim dt0 dt P (dt) и P (t) - P (t) = dt (P (dt) - I ) и P (t) = dt Qdt и P (t) = . (2.9) dt

From the above we obtain, d P (t) = Q и P (t). dt Then, (2.10)

d P (t) и P (dt) - P (t) P (t) = dt dt P (t)[P (dt) - I ] = dt P (t) и Qdt = . (2.9) dt

Thus, we derived d P (t) = P (t) и Q. dt (2.11)

Equation (2.10) can be written out using the components i and j , which leads to equations (2.12) [30],

d p(i, j, t) = dt

m

qik и p(k, j, t) for i, j = 1, 2, . . . , m.
k=1

(2.12)

30

Equations (2.12) are known as Kolmogorov s backward equations. Although the set does look different then its predecessor, equation (2.10), upon a closer look it is easy to identify that [p(i, j, t)](i,j ) = P (t), (q )ik = Q and [p(k, j, t)](k,j ) = P (t). Carrying out a similar rearrangement of equation (2.11) we arrive at the following equation [30],

d p(i, j, t) = dt

m

p(i, k, t) и qik
k=1

for i, j = 1, 2, . . . , m.

(2.13)

The equation (2.13) is the set of Kolmogorov s f orward equations. Kolmogorov's forward equation can now be used to derive the Chemical Master equation in the next chapter.

31

Chapter 3 Biochemical Systems Background
The previous chapter provided the mathematical framework essential to studying stochastic models of well-stirred biochemical systems upon which this thesis is based. This Chapter commences with a thorough examination of the discrete stochastic model of biochemical kinetics, the Chemical Master equation (CME) including definitions of all assumptions made, foundational theory and the derivation. The ensuing section will explore the motivation and underlying concepts behind the stochastic simulation algorithm (SSA). This will be followed by a detailed analysis of the tau-leaping method, which will be central in our use of the adaptive tauleaping method and the modification of the latter. Next, we will demonstrate that the tau-leaping method can be reduced to the Chemical Langevin equation (CLE) and subsequently to the reaction rate equations (RRE) under certain assumptions. The culmination of this Chapter will be an outline of potential future work and practical applications of this research.

32

3.1

Chemical Master Equation

Definition 3.1.1. Chemical Master Equation [14]. The CME is the system of equations that determines the probability of the system state to be in each possible state of the well-stirred biochemical network, at the current time, provided that the initial state is known. The most refined model of biochemical systems is that of molecular dynamics. That is, the position and velocity of each molecule are obtained at each time t. However, this molecular dynamics approach bears enormous computational costs and is highly impractical. The foundations upon which the CME is built is probabilistic. Under certain simplifying assumptions, rather than keeping track of the positions and velocities for every single molecule, the objective is to find the molecular population number of each species depending on time. Consider a system where there are N different types of molecules, or chemical species, denoted by {S1 , . . . , SN }. These molecules are involved in M types of chemical reactions denoted by {R1 , . . . , RM }. Implementation is contingent upon ignoring positions and velocities of individual molecules, however, this simplification can only occur if the system is assumed to be "well-stirred". Definition 3.1.2. Well-stirred. A well-stirred system is one where molecules of each type are uniformly spread throughout the spatial domain. The "well - stirred" assumption is fundamental in deriving the CME because, most molecular collisions are non-reactive (elastic) [14]. Two consequences arise. First, molecules, as stated in the definition, are spread uniformly throughout the

33

spatial domain; secondly, velocities of molecules become thermally randomized to the Maxwell-Boltzman distribution. Ergo, non-reactive collisions are negated and the focus shifts to completed reactions, which significantly reduces computational time. In addition to the well-stirred assumption, two more assumptions have to be made. One, the system has to be in thermal equilibrium and two, the volume of the spatial domain is constant.

At this point the introduction of a biochemical system would be beneficial to the reader, as it can be used to demonstrate concepts currently being discussed. Consider the following biochemical system [20], known as the decay-dimerization model. We let Xi (t) represent the number of molecules of species Si at some time t. Definition 3.1.3. State-change vector. The change in the vector of the species molecular populations induced by a single occurrence of a particular reaction is known as the state-change vector of that reaction. The system state vector at time t is denoted by,   X1 (t)    X (t)  2 X(t) =   .  . .    XN (t)       .     

At time t = 0 the initial state vector is given, X(t0 ) = x0 . Change in the state vector synonymously, the population of the species, is the result of a chemical reaction. Thus, if the system is in state X(t) at time t and one reaction Rj happens, then 34

the system state becomes X(t) + j , where j is the state change vector of reaction Rj . For example, for the decay-dimerization model [20] we have,

1 S1 -  0

C

(3.1) (3.2) (3.3) (3.4)

2 S1 + S1 -  S2

C

3 S2 -  S1 + S1

C

4 S2 -  S3 .

C

Prior to advancing, we must make note of a few things. First of all, only the molecules that are reactants, that is molecules that appear on the left side of the reactions will be considered. For instance, in decay-dimerization it is apparent that S3 does not react, as such, S3 can be disregarded. Next, it should be said that all state-change vectors for each reaction channel will combine to form the "stoichiometric matrix". Drawing from our model, the state change vectors associated with the first reaction channel being,  

 -1  . 1 =    0

For complete clarity, let us consider Reaction (3.1). In Reaction (3.1) one molecule of species S1 is lost and nothing is gained in return, thus 1 is written as above.

Similarly, in Reaction (3.2) one molecule of species S2 is exhausted and two molecules

35

of species S2 are formed. The union of all the associated state change vectors will form the stoichiometric matrix,  

0 -1 -2 2 . =   0 1 -1 -1

Let us regress. The CME as has been noted at various times, is a stochastic model. It would not be egregious to purport that a reaction can only occur if certain molecules are to collide. Using probability theory, we know that P (A  B ) for two independent variables is P(A) и P(B). Similar logic applies here. Therefore, the probability that the next reaction takes place in the interval [t, t + dt), where dt is an infinitesimal time step, is proportional to some combination of Si 's. The constant of proportionality is called a reaction rate parameter. Naturally arises a question, why is a constant necessary? As was discussed earlier, not all collisions lead to reactions, therefore this constant is designed to account for unreactive collisions. This very general equation reduces to three cases, first and second order, and dimerization. This probability is also known as, and from henceforth will be referred to as, the propensity f unction. Definition 3.1.4. Propensity Function. The function aj (x) whose product with dt gives the probability that a reaction Rj will occur in [t, t + dt) for an infinitesimal time dt, given that X(t) = x. First and second order propensities are fairly intuitive, however the dimerization propensity leaves room for questions. The answer is also in fact straightforward; it 36

represents the number of ways we can choose an unordered pair of objects from a total of Xm molecules using combinatorics.

The expression of these propensities are justified by kinetic theory principles [16].

First order. Sm -  products of reaction, = aj (X (t)) = cj Xm (t). Second order. Sm + Sn -  products of reaction, where m = n, = aj (X (t)) = cj Xm (t)Xn (t).
1 cj Xm (t) Dimerization. Sm + Sm -  products of reaction,= aj (X (t)) = 2 Cj Cj

Cj

(Xm (t) - 1).

Utilizing the results from the preceding section we can now derive the Chemical Master equation from the Kolmogorov forward equations [30]. Theorem 3.1.1. Kolmogorov forward equations for a biochemical system may be written as ,

d p(x0 , t0 , x, t) = dt

M

ai (x - i , Ci )p(x0 , t0 , x - i , t) - ai (x, Ci )p(x0 , t0 , x, t) , (3.5)
i=1

for any t0  IR, x0 , x  S, where S is the state - space. Equations (3.5) are known as the Chemical Master equation, a discrete stochastic model of well-stirred biochemical kinetics.

37

Proof. Using Kolmogorov's forward equations (2.13), we obtain,

d p(x0 , t0 , x, t) = qx ,x p(x0 , t0 , x , t) dt x S =
x S,x =x

(3.6)

qx ,x p(x0 , t0 , x , t) + qx,x p(x0 , t0 , x, t)

(3.7)

Since P = I + Qdt,

we derive the following property of the entries of the matrix Q:

qx,x +
x S,x =x

qx,x = 0

and thus qx,x = -
x S,x =x

qx,x

(3.8)

Substituting (3.8) into (3.7), we get

d p(x0 , t0 , x, t) = qx ,x p(x0 , t0 , x , t) - qx,x p(x0 , t0 , x, t) dt x S,x =x x S,x =x =
x S,x =x

qx ,x p(x0 , t0 , x , t) - qx,x p(x0 , t0 , x, t) .

Given that x = x and qx ,x = 0 it then follows that it is only possible that x = x-j . This in turn means that qx-j ,x = 0. Following a similar train of thought, we can state that given x = x and qx,x = 0 then x = x + j , and thus qx+j ,x = 0.

38

Using what we just stated, it follows,

d p(x0 , t0 , x, t) = dt

M

qx-j ,x p(x0 , t0 , x - j , t) - qx,x+j p(x0 , t0 , x, t) .
j =1

(3.9)

Also, we note from the definition of a propensity function that,

qx-j ,x = aj x - j , Cj

qx,x+j = aj (x, Cj ) Then, after substituting in (3.9), we derive:

d p(x0 , t0 , x, t) = dt

M

aj (x - j , Cj )p(x0 , t0 , x - j , t)
j =1

- aj (x, Cj )p(x0 , t0 , x, t)

(3.10)

Thus we have arrived at the Chemical Master equation. Parts of this section will be revisited in the rest of the chapter, in particular when we derive the Reaction Rate Equation (RRE). In what follows we use the notation P (x, t|x0 , t0 ) to represent the probability that X(t) = x if X(t0 ) = x0 . With this notation the CME becomes,

39

d P (x, t|x0 , t0 ) = dt
M

(P (x - j , t|x0 , t0 )aj (x - j ) - P (x, t|x0 , t0 )aj (x)) . (3.11)
j =1

The CME (3.10) is clearly faster to solve numerically than the molecular dynamics approach and is accurate. However, the solution of the CME is computationally very challenging to approximate directly, hence it is still very slow. The CME is a system of ordinary differential equations with one ordinary differential equation (ODE) for every single state thus it has a large dimension in general. Solving the CME directly came to be replaced by the SSA, which will be discussed at length in the next section.

3.2

Stochastic Simulation Algorithm

As we discussed in the introduction and the preceding section, the set of CME equations becomes impossible to solve analytically when the system is sufficiently large. The stochastic simulation algorithm (SSA) also known as Gillespie's algorithm was first introduced in 1945 by Joseph L. Doob [4]. It however was not until 1976, and Daniel Gillespie presented it, that the method became a biokinetic mainstay [9].

The SSA uses the Monte Carlo method to generate trajectories with a distribution in exact agreement with the solution of the CME. Enhanced computational capability of the SSA is a direct result of the explicit simulation mechanism. Additionally, 40

we have numerously mentioned that the SSA is an upgrade on the direct solution of the CME, but never examined why. In order to facilitate our explanation we refer to definition 2.1.1 in Chapter 2, which declares, "for a discrete random variable X, the probability mass function (PMF) is the function that outputs the probability of each event x  Sx (where Sx is the sample space)" [30]. Indeed, the SSA takes a sample from the probability mass function of the solution set of CME rather than the whole PMF.

Below, we provide the theoretical justification of the SSA. Let us first introduce the quantity P0 ( |x, t) [16] which denotes the probability that no reaction occurs in the next time interval [t, t +  ], given the state vector X(t) = x. Next, consider the time interval [t, t +  + d ), where d is an infinitesimal time step, where at most one reaction can occur. The probability that no reaction occurs over [t, t +  + d ) is denoted by "event C". Similarly, "event A" will signify the probability that no reaction happens during [t, t +  ) and "event B " as the probability that no reaction happens over [t + , t +  + d ). Then the probability that event C occurs is equivalent to the probability of events A and B taking place.

P (C ) = P (A) и P (B ).

(3.12)

Since the events in the interval [t, t +  ) and those in [t + , t +  + d ) are independent the "and" can be expressed using multiplication, and then the key to advancing the derivation is, restating P(B) differently. P(B) can be thought of as 1 - the probability of each reaction occurring in the interval, [t + , t +  + d ). Using this 41

reconfiguration, equation (3.12) leads to,

P (C ) = P (A) и (1 - P (B c )).

(3.13)

Then using the definition of a propensity (recall that the propensity function coupled with an infinitesimal time step is equivalent to the probability of a reaction occuring in the next time step), equation (3.13), can be expressed as,

M

P (C ) = P (A) и

1-
k=1

ak (x)d .

(3.14)

Then, using our initial statements and notation, this can be rewritten as,

M

P0 ( + d |x, t) = P0 ( |x, t) 1 -
k=1

ak (x)d .

(3.15)

Rearranging the above we arrive at,

P0 ( + d |x, t) - P0 ( |x, t) = -asum (x)P0 ( |x, t), d

where asum (x) :=

M k=1

ak (x). Equivalently, this can be restated as,

d P ( |x, t) = -asum (x)P0 ( |x, t). d

(3.16)

Approaching the limit as d  0, leads to a linear scalar ODE, which by definition has the initial condition P0 (0|x, t) = 1 [16]. We remark that, P0 (0|x, t) = 1, signifies that the probability that no reactions take place in no time is always 1. Solving the

42

ODE (3.16) with this initial condition, the following result is obtained,

P0 ( |x, t) = e-asum (x) .

(3.17)

Now we introduce the quantity p(, j |x, t) which denotes the probability that the next reaction will be i) the j th reaction and ii) will occur in the time interval [t + , t +  + d ). As before event i) is signified by D and ii) by E. Events D and E are again independent of each other by virtue of similar logic expressed a few paragraphs earlier, thus,

P (D  E ) = P (D) и P (E ).

(3.18)

As earlier we assume d to be an infinitesimal time step where no more than one reaction can take place. Using established definitions of P0 and propensity, we arrive at equation, p(, j |x, t)d = P0 ( |x, t)aj (x)d. (3.19)

Recall the earlier result in the form of equation (3.17). Substituting equation (3.17) into (3.19) and cancelling the d terms, we obtain the following,

p(, j |x, t) = e-asum (x) aj (x).

(3.20)

43

Finally, equation (3.20) can be rearranged ensuring that the a's are gathered and outside the exponential function, simplifying our algorithm,

p(, j |x, t) =

aj ( x ) asum (x)e-asum (x) . asum (x)

(3.21)

Before analysing the significance of what was just derived, let us examine in greater detail the mathematics behind this equation. This will require two propositions. We begin with the time to next reaction. Proposition 3.2.1. Simulation of time to next reaction [30]. Recall the definitions presented in Section 2.1, it is apparent that  is exponentially distributed with parameter asum (x). Then,

=

1 asum (x)

ln

1 1

where 1 is a uniformly distributed random variable (1  U (0, 1)) necessary for simulation. Proof. We have to solve e-asum (x) = 1 with 1 is uniformly distributed in [0,1]. Consequently, e-asum (x) = 1 -asum (x) = ln (1 )  =- 1 asum (x) 1 asum (x) 44 ln

ln (1 ) 1 1

=

(3.22)

Equation (3.22) is the time to next reaction simulated by the SSA, with asum (x)e-asum (x) serving as the PDF.

Proof that the index of the next reaction is indeed 3.2.2.

aj (x) , asum (x)

requires Proposition

Proposition 3.2.2. Index of time to next reaction [30]. The reaction channels Rj are exponentially distributed, with the parameters aj (x), that is,

j  Exp(aj (x)),

where j = 1, 2, . . . , n, are independent random variables. Then,

0 

i=1,2,...,M

min

j  Exp(asum (x)).

Given that the SSA is a critical component of this thesis, consequently, this is an equally important proposition; thus, we will take the time to provide the proofs. Proof. For a exponential random variable X  Exp(), P (X > x) = e-x .In our

45

case, X = j ,  = aj (x)

P (X0 >  ) = P min{Xj } > 
j

= P (|X1 >  |  |X2 >  |  и и и  |XM >  |)
M

=
j =1 M

P ( Xj >  ) e-aj (x)
j =1
M j =1

= = e-x

aj (x)

= e-asum (x) .

Thus, 0  Exp(asum (x)). Lemma 3.2.3. [30] If we suppose X  Exp() and Y  Exp(х) are independent random variables, then, P (X < Y ) =  . +х

Proof. We can derive that,



P (X < Y ) =
0 

P (X < Y )|Y = y )f (y )dy P (X < Y )f (y )dy
0 

= =
0

(1 - e-y )хe-хy dy  , +х

=

46

since f (y ) = хe-хy . Proposition 3.2.4. [30] If j  Exp(aj (x)), i = 1, 2, . . . , M are independent random variables, let k be the index of the smallest of the j . Then k is a discrete random variable with the PMF,

j =

aj ( x ) , j = 1, 2, . . . , M. asum (x)

Proof. Let us consider,

j = P (k < min{j })
j =K

= P (k < Y ).

Then, according to Lemma 3.2.3,

j = =

ak (x) ak (x) + a-k (x) ak (x) . asum (x)

Note: Y = minj =k {j }, such that, Y  Exp(a-k (x)), where a-k (x) =

j =k

aj (x). Why is this result meaningful? Primarily, it is because our two variables are gathered in one equation (3.21), a joint density function. This in turn, maintains independence between the two variables, while optimizing the computational cost. Recall that, j represents the next reaction index and  defines the time to next reaction. Each of the independent variables can be simulated using a uniform sample 47

on the (0,1) interval, with time to next reaction being simulated using the exponential distribution. With all of that being said, we present Gillespie's algorithm also known as the SSA,

1. Initialize the simulation X(t0 ) = x0 at t = 0 and set the parameters M , N , cj 's and  . 2. Evaluate {ak (X (t))}M k=1 and asum ((X (t))) :=
M j =1

ak (X (t)).

3. Select two independent uniform (0,1) random numbers 1 and 2 . 4. Evaluate j , the smallest integer satisfying 5. Compute  = ln(1/2 )/asum ((X (t))). 6. Update X (t +  ) = X (t) + j and t to t +  . 7. Go to step 1.
j k=1

ak (X (t)) > 1 asum ((X (t))).

Nearing the end of the topic, conclusions can be drawn. The SSA is a vast improvement over solving the CME directly, in terms of computational cost. Also, the SSA is an exact Monte Carlo method for the CME, generating a possible sequence of reaction events. In the following section, our attention is turned to tau-leaping, a topic at the heart of this thesis. An obvious shortcoming of the SSA is that the algorithm advances through time one reaction at a time, therefore, it can be very slow when some very fast reactions happen in the system. In contrast, tau-leaping provides a platform where the system can fire several reactions during one time-step.

48

3.3

Tau-Leaping

Gillespie [13] proposed a strategy to accelerate the SSA in which each time step  advances the system through possibly many reaction events.

A common theme within this thesis will be the inefficiencies of methods associated with high computational costs. The SSA, while an exact method, is nonetheless computationally expensive when some reactions are fast. We mentioned in the previous Section that the SSA progresses one reaction at a time. This in turn implies that at each iteration random number generation has to be utilized, the state vector updated and so on. The idea behind tau-leaping is allowing many reactions of each type to fire over one time step and then to update the state vector. However, the key to the tau-leaping method is maintaining accuracy comparable to the SSA, while improving execution speed. This is accomplished by mandating the leap condition, which states that the propensity cannot change its value significantly as a result of the larger time step. Definition 3.3.1. Leap Condition [14]. A time step  satisfies the leap condition if  is sufficiently small such that the propensity aj (x(s))does not undergo any observable change for any 1  j  M and any t  s  t +  . Mathematically speaking, this can be written as,

aj (x(t +  ))

aj (x(t)),

for any 1  j  M.

The leap condition requires propensities aj (x(t)) to remain almost constant during 49

the step, while the number of reactions that will fire is calculated using a counting process. The probability of the j -th reaction firing over the time step  is aj (x(t)) , by the multiplication rule for independent variables. Subsequently, we need to determine how many of these events occur over [t, t +  ). This can be well approximated using a Poisson distribution, with mean and variance, aj (x(t)) in Pj (aj (x),  ). Putting everything together garners the general tau-leaping equation [13],
M

X(t +  ) = x +
j =1

j Pj (aj (x),  ).

(3.23)

where the random variables {Pj (aj (x),  )}M j =1 are independent Poisson random variables and X(t) = x.

An exact representation of the stochastic process X(t) was given by Kurtz [18]. If X(t) = x, then

M

t+

X(t +  ) = X(t) +
j =1

j Pj
t

aj (X(s))ds) .

(3.24)

Using the leap condition we can make the following assumption,

aj (X(s))

aj (X(t)),

for all t  s  t + .

Then,

t+

t+

aj (X(s))ds
t t

aj (X(t))dt = aj (X(t))

(3.25)

50

Using (3.24) and (3.25) we get,

M

X(t +  )

x+
j =1

j Pj (aj (x),  ).

Thus the tau-leaping method (3.23) is an approximate Monte Carlo strategy for solving the CME. In the introduction tau-leaping was characterized as the "bridge equation" to the Chemical Langevin Equation (CLE) from the CME. Let us investigate this claim further. We begin by making the assumption that  is small enough to satisfy the leap condition, but also large enough to ensure that the number of firings for each reaction channel Rj is much larger than 1 (i.e. aj (x(t)) 1, for 1  j  M ).

Now we invoke Proposition 2.1.1 from Chapter 2, which states that a Poisson random variable with a large mean and variance, can be well approximated by a normal random variable with the same mean and variance [30].

3.4

Chemical Langevin Equation

Section 3.3 featured comprehensive coverage of the tau-leaping method; tau-leaping is considered to be a bridge to the Chemical Langevin equation (CLE). Recall from the previous section that we made the assumption that  is chosen such that (i) the leap condition is satisfied and (ii) the average number of firings for each reaction channel Rj is aj (x(t)) и  1 for any 1  j  M .

If Pj (aj (x),  ) in equation (3.23) is replaced by aj (X (t)) +

aj (Y (t))Zj , where Zj

51

are independent normal variables with mean 0 and variance 1, then we get,

M

X(t +  ) = X(t) + 
j =1

j aj (X(t)) +



M


j =1

j

aj (X(t))Zj .

(3.26)

The algorithm for simulating the above is,

1. Select independent samples {Zj }M j =1 from the normal distribution with mean 0 and variance 1.

2. Substitute samples from first step into equation (3.26), to obtain X(t +  ) and update time t to t +  .

3. Return to step 1.

This algorithm is to be repeated for as many simulations as needed (the standard number of Monte Carlo trajectories used for stochastic simulation of biochemical systems is 10,000). Also noteworthy is the fact that equation (3.26) is the EulerMaruyama solution to equation (3.27).

M

M

dX(t) =
j =1

j aj X(t)dt +
j =1

j

aj X(t)dWj (t),

(3.27)

where, Wj (t) are independent scalar Brownian motions. Equation (3.27), is, in fact a system of stochastic differential equations which is called the Chemical Langevin equation.

52

In the paragraph above we briefly mentioned a key concept, Brownian motion, let us explore it further. Brownian motion is a physical phenomenon pioneered by Robert Brown [26] and later developed by Albert Einstein and Jean Perrin, for which Perrin would eventually be awarded a Nobel Prize in Physics in 1926 [19]. From the physics perspective, Brownian motion is the random motion of a particle surrounded by fluid. The motion is the result of continuous and random pounding of the particle by the surrounding atoms. Einstein was able to derive an equation for the average displacement of the particle, however this is not necessary in the context of this thesis. Eventually Brownian motion was adopted into the world of biochemical simulation, since the simulations tend to move in random trajectories, mimicking the movement of the particle in fluid described above.

The conclusory paragraph is a good time to make two remarks. First of all, the CLE is dependent on two assumptions, one, the time step has to be small enough not to cause a significant variation in the propensities, yet large enough to satisfy the approximation of the Poisson distribution by the normal distribution. Secondly, the CLE is a "bridge process" itself, as we shall see in the next section.

3.5

Reaction Rate Equation

The reaction rate equations constitutes a model of well-stirred biochemical systems. We have traced simulation of biochemical systems from the molecular dynamics approach, to the CME, then via the tau-leaping method we arrived at the CLE and 53

now it is time to dissect the RRE. As was aforementioned, the RRE is simply the deterministic part of the CLE [16]. We said that this simplification can be achieved through the thermodynamic limit. Well in that case, the question begs itself, what is the thermodynamic limit? Definition 3.5.1. Thermodynamic Limit [14]. The thermodynamic limit is defined as the limit in which the species populations Xi , and the system volume  all approach infinity, but in such a way that the species concentrations Xi / stay constant. As this limit approaches infinity, the propensities grow proportionally to the size of the system. This occurs for both types of propensities, unimolecular and bimolecular. The latter is a result of the inversely proportional relationship between the reaction constants and the system volume. Therefore, as the propensities grow, so do both sides of equation (3.27). However, the term on the right
M j =1 M j =1

j aj Y(t)dt j

will grow much faster than the square root term on the left

aj Y(t)dWj (t) . Naturally, as the limit approaches infinity, the term

on the left becomes negligible, thus reducing (3.27) to the reaction rate equations (RRE).

Similarly to what we presented in Section 3.1, the rate constants, cj , can be catogorized into the three identical scenarios we described for the CME; for first and second order reactions and dimerization.

Propensity functions for the first and second order reactions and the dimeriza54

tion are:

First order. Sm -  products of reaction, then aj (X (t)) = cj Xm (t). Second order. Sm + Sn -  products of reaction, where m = n, then aj (X (t)) = cj Xm (t)Xn (t).
1 cj Xm (t)2 . Dimerization. Sm + Sm -  products of reaction, then aj (X (t)) = 2 Cj Cj

Cj

Therefore, to achieve the transformation from the CLE to the RRE, we induce the necessary assumptions mentioned in the previous paragraph followed by application of Definition 3.5.1.

All that remains is derivation of the reaction rate equations. We start by recalling the general model of the Chemical Master equation,

d P (x, t|x0 , t0 ) = dt

M

(P (x - j , t|x0 , t0 )aj (x - j ) - P (x, t|x0 , t0 )aj (x)) .
j =1

Proof. In order to derive the RRE the expectation of both sides of the CME has to

55

be taken [30].

  E ( Xt ) = t t =

x и p(x, t)
x S

xи
xS

 p(x, t) t
M

=
xS M

xи
j =1

aj (x - j , cj ) и p(x - j , t) - aj (x, cj )p(x, t) x и aj (x - j , cj )p(x - j , t) - x и aj (x, cj )p(x, t)

=
j =1 xS

where x = (x - j ) + j
M

=
j =1 x S

(x - j )aj (x - j , cj )p(x - j , t) j aj (x - j , cj )p(x - j , cj ) -
xS x S

+

x и aj (x, cj )p(x, t)

56

taking y = (x - j )
M

=
j =1 y S

y и aj (y, cj )p(y, t) j aj (y, cj )p(y, cj ) -
y S x S

+

x и aj (x, cj )p(x, t)

the y and x terms will cancel
M

=
j =1

j
y S

aj (y, cj )p(y, cj ) .

Consequently,  E (Xt ) = t where aj (y, cj )p(y, t) = E (aj (cj )).
y S M

j
j =1 y S

aj (y, cj )p(y, t)

We obtained [30],  E ( Xt ) = t
M

j E (aj (ci ))
j =1

(3.28)

The above equation is derived when expectation is taken in the CME. However, the reaction rate equations are

 E ( Xt ) = t

M

aj (E (xt ), cj ))
j =1

(3.29)

or, if we denote y (t) = E (Xt ), then,

57

dy(t) = dt

M

j aj (y(t)).
j =1

At this point, it is critical to mention that equation (3.28), as it is written, may be different than equation (3.29). The two equations coincide for systems with at most order one reactions. For second order reactions they may differ, nevertheless empirical results attest that the RRE maintains its purpose. The reason behind this discrepancy is that, in general,

E (cj Xi Xk ) = cj E (Xi Xk ) = cj E (Xi )E (Xk ).

as Xi and Xk may not be independent.

As was briefly mentioned in the introduction, the reaction rate equations was the conventional model of biochemical systems until stochastic model of the CME proved to be more accurate. An additional drawback to the RRE, as demonstrated earlier, is the fact that there are inconsistencies between the theoretical base and empirical results. Nevertheless, for simulating systems with very large numbers of chemical species, the RRE remains the gold standard to this day.

3.6

Potential Applications

In the introduction we briefly touched on the overall impact of the medical and biomedical industry on society and what role biochemical systems have to play within that. The future of systems biology will continue to be intertwined with 58

medicine and biomedical engineering [17]. In this section we will delve into the specific projects for which systems biology is utilized as well as other industries that can benefit from this research.

The medical field that has perhaps benefited most from the advancement of computational biology is cancer research [17]. Cancer is the biggest medical challenge of our time. As the global population continues to live longer, cancer rates are rising and according to the Canadian Cancer Statistics 2017 report by the Canadian Cancer Society, it is said that half of the Canadian population will develop it during their lifetime. Biochemical systems strive to predict the future stages of the disease, as well as the response to medication in hopes of a cure. Just in recent history we have seen that patients are living longer with their cancers and some forms of it which we considered untreatable are now being if not treated at the very least managed. Other promising applications for systems biology include treatment or possible cure for inflammatory diseases, diabetes and disorders of the nervous system [24].

With that we conclude this chapter. We started by outlining the basic assumptions and constructs, followed by an exhaustive examination that took us from the Chemical Master equation (a stochastic model, discrete in time and space) all the way to the reaction rate equations (a deterministic model continuous in time and space). Along the way we provided a step-by-step dissemination of each of the methods we used, in tandem with rigorous proofs. In the next chapter, we focus on

59

the development of effective tau-leaping strategies.

60

Chapter 4 Algorithms and Models
This Chapter will serve as the apex of this research endeavour and is based upon the following framework. Building on the content from Section 3.3, the explicit and implicit tau-leaping techniques will be discussed at length. Subsequently, we will introduce and analyze adaptive tau-leaping methods, using the explicit and implicit tau-leaping schemes. In this chapter we propose an innovative adaptive explicit-implicit tau-leaping method which generalizes the state of the art variable tau-selection strategy of Cao et al. [3]. We use a pseudo-Newton's scheme to approximate the solution of the tau-leaping method, by employing finite-difference strategies to approximate the Jacobian. This eliminates the need for user's intervention. The final act will serve as a review of methods and tools we use in the analysis of the results, followed by the results themselves.

61

4.1

Explicit Tau-Leaping

Recall that, if the leap condition is satisfied on [t, t +  ), then the explicit tau-leaping method is
M

X(t +  ) = x +
j =1

j Pj (aj (x) ),

(4.1)

given that X(t) = x.

The implicit tau-leaping scheme operates on a similar principle. It is known that the explicit method is well-suited to handle non-stiff problems. In the next section, we devote a paragraph to stiffness, which plays a central role in selecting the appropriate tau-leaping simulation strategy: explicit for non-stiff systems or implicit for stiff ones. For now, let us return to explicit tau-leaping. The explicit tauleaping strategy requires that the step-size  is chosen such that the leap condition is obeyed. Applying this condition leads to a sequence of non-uniform step-sizes on each trajectory. Such a method is said to incorporate adaptivity. A constant stepsize implementation of the tau-leaping scheme is not justified theoretically and it may lead to inaccurate results. The next two paragraphs will contain the algorithm for each respective  selection process.

We begin with the "vanilla", or the explicit method sans adaptivity. The first step, as with all other algorithms, is to choose the initial conditions X(t) = x0 at t = 0. For this vanilla method, the leap-size  is fixed, thus it is chosen at this step. Secondly, within a time loop ranging from time t = 0 to the preconditioned

62

final time, the propensities are calculated. Subsequently, using the Poisson random number generator with the parameter aj (x(t)) и  (still within the loop), the solution is advanced according to (4.1) and t is updated by t +  . Thus, one trajectory of the biochemical system is computed. Finally, return to step one and continue the process until an appropriate number of trajectories is available.

Our attention will now shift to the adaptive explicit tau-leaping method. As the reader might have guessed, the only difference between the algorithm above and the current method is the selection strategy of  . First of all, automatic selection of  is now performed over each step of the time-loop. In this method, the choice of  is contingent upon the type of reaction that will fire next. In this context, there are two types of reactions, critical and noncritical [2]. Critical reactions are those, for which the population of reactant falls below a certain threshold; non-critical reactions are those that do not satisfy this condition. Throughout the simulation, the algorithm categorizes reactions in this manner at each time-step. Based on whether or not a reaction is critical, the algorithm has to make a decision, either proceed with the explicit tau-leaping method with an associated  for non-critical reactions or the SSA with also an accompanying  for critical reactions. Adaptive switching between tau-leaping and the SSA is the result of the SSA being better equipped to simulate smaller population sizes; this is evident given that the SSA progresses one reaction at a time. Finally, once the algorithm has classified the reactions and chosen the appropriate  , steps three and four are identical to the prior paragraph.

63

Everything discussed previously can be algorithmized into a multi-step procedure and formally integrated into the following algorithm due to Cao et al [2].

1. Initialize t = 0, X (0) = x0 ; set the simulation parameters: tolerance , critical threshold nc , the final time T and the reaction rate constants cj .

2. At each time t categorize the reactions. We begin, by introducing Lj which represents the number of times a reaction can fire before one of the reactants is exhausted Lj =
i[1,N ],i,j <0

min

xi , |i,j |

where i,j represents the state-change vector. A reaction Rj is deemed to be critical if Lj < nc ; for our purposes the generally accepted value of nc = 10 was used .

3. Armed with the knowledge of which reactions adhere to which class, we introduce Jcr and Jncr , respectively denoting the set of critical and non-critical reaction indices. If all the reactions happen to be critical, then set  =  and proceed to step 5.

4. If non-critical reactions are present, then the following multi-step is used to calculate the explicit candidate for  .

I) First the highest order of reaction (or HOR(i)) for a chemical species Si is determined. As was discussed earlier, reactions found only on the left side of the

64

equation will be considered. The order of a reaction is equivalent to the number of times a reactant is seen in each reaction; the highest such number is the HOR(i).

II) In this step the value of

i

is computed, where,

i

=

gi

.

(4.2)

is the tolerance and gi = gi (xi ) is calculated in the following way,

i) If HOR(i)=1, gi = 1. (4.3)

ii) If HOR(i)=2, gi =2, unless two Si molecules are used in one reaction, in which case, gi = 2+ 1 xi - 1 (4.4)

iii) If If HOR(i)=3, gi =3, unless two Si molecules are used in one reaction, in which case, gi = 1 3 2+ . 2 xi - 1 (4.5)

65

If a third-order reaction requires three Si molecules, then,

gi =

3+

2 1 + . xi - 1 xi - 2

(4.6)

III) Calculation of the explicit candidate for the next time step requires the following two quantities, i (x) :=
j Jncr

ij aj (x),
2 ij aj (x). j Jncr

(4.7)

i (x) :=

(4.8)

IV) The explicit candidate for the next time step is given by  ,

 = min

max{ xi /gi , 1} max{ xi /gi , 1}2 , . |i (x)| [i (x)]2

(4.9)

5) Compute the sum ac 0 (x) of all the propensities of critical reactions. Using this generate a second candidate  , with equation (4.10)

 =

1 ac 0 (x)

и ln

1 , r1

(4.10)

for r1 a sample of U (0,1), the uniform distribution over (0,1).

6) If  < 

then  =  and we proceed with the explicit tau-leaping (4.1),

else  =  followed by simulation with the SSA of the slow reactions.

66

7) Exit this sequence, return to step 2 and repeat until a sufficient number of trajectories has been generated.

This marks the end of adaptive explicit tau-leaping scheme, for now. As we shall soon see, it also features prominently in adaptive tau-leaping.

4.2

Implicit Tau-Leaping

The main objective of the implicit tau-leaping method is to efficiently simulate stiff systems, which are expensive to solve numerically by the explicit tau-leaping strategy.

4.2.1

Stiffness

The primary reason for different variants of tau-leaping is stiffness. Stiffness is defined as the presence of slow and fast dynamics in the system, with the fast ones being stable [27]. Often biochemical systems arising in applications involve slow and fast reactions. After a short transient, fast reactions reach a partial equilibrium. To be more precise, in order for a system to be considered stiff, there has to be at least two orders of separation between the fast and slow reaction propensities. For problems that are stiff we use implicit methods because they are better suited than their explicit counterparts. The issue with explicit methods is that the step size has to be kept small in order to ensure stability [27]. Implicit methods on the other hand have no such restriction on the step size in order to be stable. 67

4.2.2

Newton's Method

In many ways the discussion regarding the implicit tau-leaping method is very similar to the previous Section. For this very reason we shall focus only on the major discrepancy between the two, the implicit part. The implicit tau-leaping equation is given by [27],

. X(t +  ) = x +

M

[Pj (aj (x) ) -  aj (x) + aj (X (t +  )) ]j ,
j =1

(4.11)

if X (t) = x. Note that only the deterministic part is implicit, while the stochastic component is in an explicit form. Equation (4.11) may be written as

F (X(t +  ), x) = 0

(4.12)

where

F (X(t +  ), x) = X(t +  )-
M M

aj (x(t +  ))  j j - x +
j =1 j =1

[Pj aj (x),  ) -  aj (x) j . (4.13)

Equation (4.12) will be solved to find X(t +  ), the system state at the future time, t +  . Note that (4.12) is an implicit equation in X(t +  ). To solve this implicit problem numerically, we use Newton's method.

Newton's method was first developed by its namesake Isaac Newton. The method originally proposed by Newton through the years has evolved into a version that 68

varies from the original. An important contributor to Newton's method was Joseph Raphson, so much so that the method is often referred to as the Newton-Raphson method. Newton's method for a generic equation F (X ) = 0 is
-1

X

(n+1)

=X

(n)

F X (n) - X

и F X (n) ,

(4.14)

where the n-th iteration X (n) is an N -dimensional array and F is an N -dimensional function of X . What we have is a system composed of N equations and N unknowns, where    X1       X   2   X=  .   .   .      XN and   F1 (X )    F (X )  2 F (X ) =   .  . .    FN (X )       .     

X (0) = X(t) serves as the initial guess for the implicit tau-leaping method (4.11) on [t, t +  ). A challenge of this strategy is the computation of the Jacobian. This portion of the method has to be derived symbolically, which may be challenging, especially in the presence of large systems. Also, it requires the user's intervention, which is a drawback of this technique. Nevertheless, the Jacobian is given by the

69

following matrix,      F2 (X )  X1 J =  .  .  .  
F1 (X ) X1 F1 (X ) X2 F2 (X ) X2

 ... ... .. .
F1 (X ) XN

. . .
FN (X ) X2

FN (X ) X1

...

FN (X ) XN

   F2 (X )  XN    . . .    

(4.15)

The algorithm outlined in Section 4.1 will form the skeleton of the implicit algorithm. A novelty is the addition of Newton's method. This addition is expressed in the form of one extra step. This step consists of solving the implicit equation (4.12) using Newton's method. We iterate until ||X (n+1) - X (n) ||  T OL, for some given tolerance T OL. As before the simulation continues until one trajectory is attained, at which time we return to the first step. More formally, the algorithm will adhere to the following structure (see also [27]).

1. Specify the parameters, including the stoichiometric matrix, number of species, reaction channels and simulations, tolerances T OL and , rate constants, final time T , nc and empty arrays capable for storing critical and non-critical reaction indices.

2. Initialize the time t = 0 and the state X (0) = x0 .

3. Compute the propensity functions and consequently update after each simulation.

4. For each trajectory, at each time t categorize the reactions. We begin, by

70

introducing Lj which represents the number of times a reaction can fire before one of the reactants is exhausted:

Lj =

i[1,N ],i,j <0

min

xi . |i,j |

A reaction Rj is deemed to be a critical reaction if Lj < nc ; for our purposes the generally accepted value of nc = 10 was used.

5. Armed with the knowledge of which reactions adhere to which class, we introduce Jcr and Jncr , respectively denoting the set of critical and non-critical reaction indices. If all the reactions happen to be critical, then set  =  and proceed to step 7.

6. Identify reversible reactions in the system at hand. Our objective is to create a set of indices which correspond to reversible reactions not in partial equilibrium, which will be signified by Jne . Partial equilibrium is defined as the condition where correspondent, a+ (x) and a- (x) are close to each other. The difference between the two must be smaller than each respective propensity. Specifically, the partial equilibrium condition is,

|a+ (x) - a- (x)|   min{a+ (x), a- (x)},

where the generally accepted value of  is 0.05. If the system is already in equilibrium then  can be chose to be sufficiently large.

71

7. If reactions that are non-critical and not in partial equilibrium are present, then the following multi-step is used to calculate the implicit candidate for  .

I) We first determine the highest order of reaction (or HOR(i)) for a chemical species Si . As was discussed earlier, only the reactant species are considered. The order of a reaction is equivalent to the number of times a reactant is seen in each reaction; the highest such number is the HOR(i).

II) In this step the value of

i

is computed, where,

i

=

gi

.

(4.16)

is the tolerance and gi = gi (xi ) is calculated using 4.3, 4.4, 4.5 and 4.6.

III) Calculation of the implicit candidate for the next time step requires the following two quantities, i (x) :=
j Jnecr

ij aj (x),
2 ij aj (x), j Jnecr

(4.17)

[i (x)]2 :=

(4.18)

where Jnecr = Jncr  Jne is the set of non-critical reactions which are not in partial equilibrium.

72

IV) The implicit candidate for the next time step is  ,

 = min

max{ xi /gi , 1} max{ xi /gi , 1}2 , . |i (x)| [i (x)]2

(4.19)

8. Compute the sum ac 0 (x), of the propensities of all critical reactions. Using this generate a second candidate  according to,

 =

1 ac 0 (x)

и ln (1/r1 ),

(4.20)

for r1 a sample of U (0,1), the uniform distribution over (0,1).  is the step to the next slow reaction.

9. If  <  then  =  and we proceed with the implicit tau-leaping method, else  =  followed by simulation with the SSA.

10. Applying the implicit tau-leap step by solving (4.11) with respect to X (t +  ), using Newton's method (4.14) for F given by (4.13).

11. Exit this sequence, return to step 2. Repeat until a sufficient number of trajectories has been generated.

The next section will entail a detailed discussion regarding an adaptive tau-leaping

73

method, a strategy which fuses the explicit, implicit schemes and the SSA.

4.3

Adaptive Explicit-Implicit Tau-Leaping Method

This Section commences with what brought the preceding one to a close, fusion of the three central schemes into what is known as an explicit-implicit adaptive tau-leaping method. A discussion regarding the effectiveness of this algorithm will be given at the end of the section.

The adaptive tau-leaping algorithm is similar to the implicit method with adaptivity, with some exceptions. Let us present the adaptive implicit-explicit tau-leaping algorithm (see also [3]).

1. Specify the parameters, including the stoichiometric matrix, number of species, reaction channels and simulations, tolerances T OL and , rate constants, final time T , nc and empty arrays capable for storing critical and non-critical reaction indices.

2. Initialize the time t = 0 and the state X (0) = x0 .

3. Compute the propensity functions and consequently update after each simulation.

4. For each trajectory, at each time t categorize the reactions. We begin, by introducing Lj which represents the number of times a reaction can fire before one 74

of the reactants is exhausted:

Lj =

i[1,N ],i,j <0

min

xi . |i,j |

A reaction Rj is deemed to be a critical reaction if Lj < nc ; for our purposes the generally accepted value of nc = 10 was used.

5. Armed with the knowledge of which reactions adhere to which class, we introduce Jcr and Jncr , respectively denoting the set of critical and non-critical reaction indices. If all the reactions happen to be critical, then set  =  and proceed to step 8.

6. Identify reversible reactions in the system at hand. Our objective is to create a set of indices which correspond to reversible reactions not in partial equilibrium, which will be signified by Jne . Partial equilibrium is defined as the condition where correspondent, a+ (x) and a- (x) are close to each other. The difference between the two must be smaller than each respective propensity. Specifically, the partial equilibrium condition is,

|a+ (x) - a- (x)|   min{a+ (x), a- (x)},

where the generally accepted value of  is 0.05. If the system is already in equilibrium then  can be chose to be sufficiently large.

75

7. If non-critical reactions are present, then the following multi-step sequence is used to calculate the explicit candidate for  . Note in the adaptive explicit implicit method, we now have two  candidates,  (ex) and  (im) corresponding to the explicit and implicit candidates respectively. The way we compute the quantities changes. The explicit scheme will still correspond to the indices set Jncr , while the implicit method will draw from the set Jnecr . The latter is set that we have not yet seen, and represents the set that is non-critical and not in partial equilibrium, in the set theory notation this is formulated as Jnecr = Jncr  Jne .

I) We first determine the highest order of reaction (or HOR(i)) for a chemical species Si . As was discussed earlier, reactions found only on the left side of the equation will be considered. The order of a reaction is equivalent to the number of times a reactant is seen in each reaction; the highest such number is the HOR(i).

II) In this step the value of

i

is computed, where,

i

=

gi

.

(4.21)

is the tolerance and gi = gi (xi ) is calculated using 4.3, 4.4, 4.5 and 4.6.

III) Calculating the explicit candidate for the next time step requires the following two quantities, i (x)(ex) :=
j Jncr

ij aj (x),

(4.22)

76

i (x)(ex)

2

:=
j Jncr

2 ij aj (x).

(4.23)

IV) Calculating the implicit candidate for the next time step requires the next two quantities, i (x)(im) :=
j Jnecr 2

ij aj (x),

(4.24)

i (x)(im)

:=
j Jnecr

2 ij aj (x),

(4.25)

where Jnecr = Jncr  Jne is the set of non-critical reactions which are not in partial equilibrium.

IV) The explicit candidate for the next time step is given by  (ex) ,

 (ex) = min

max{ xi /gi , 1} max{ xi /gi , 1}2 , . |i (x)(ex) | [i (x)(ex) ]2

(4.26)

And the implicit candidate by  (im) ,

 (im) = min

max{ xi /gi , 1} max{ xi /gi , 1}2 , . |i (x)(im) | [i (x)(im) ]2

(4.27)

8. Compute the sum of ac 0 (x), the propensities of all critical reactions. Using

77

this generate a second candidate 2 , with equation (4.37)

2 =

1 ac 0 ( x)

и ln (1/r1 ),

(4.28)

where r1 is a sample from the unit-interval uniform distribution. 2 represents the step to the next slow reaction.

9. If  (im) is greater than Nstif f  (ex) , where Nstif f usually takes on the value 100, then the system is considered to be stiff, we let 1 =  (im) . Otherwise 1 =  (ex) .

10. If 2 > 1 then  = 1 and we proceed with explicit tau-leaping (4.1) if 1 =  (ex) or implicit tau-leaping (4.11) if 2 =  (im) . Else  = 2 followed by simulation with the SSA for the slow reactions.

11. Update X(t +  ), set time to t = t +  and exit this sequence. Return to step 2 and repeat until a sufficient number of trajectories has been generated.

This adaptive strategy is considered to be the state-of-the-art tau-leaping method [3]. Aside from the fact that adaptivity along with explicit and implicit schemes is far more efficient than the SSA, there are compelling reasons that underscore the superiority of this algorithm. The first such reason is harmonization of explicit, implicit methods and the SSA. Throughout this thesis we have demonstrated time and time again that each of the aforementioned schemes is well designed for specific degrees of stiffness of the system. For the regions where the problem is non-stiff, the 78

algorithm uses the explicit tau-leaping scheme, while in the regions of stiffness, it switches to the implicit tau-leaping strategy. In the regions where some molecular amounts are below the threshold, the SSA is the preferred strategy, as it prevents negative population numbers. Thus, the adaptive explicit-implicit method expands the computational horizons and broadens the scope of problems that can be solved using it. This elicits a smooth transition to the second reason, automatization; for without it the marriage of the three strategies would be difficult at best. However, in the case of the implicit tau-leaping method, a Newton step is employed to solve a non-linear system of equations. Symbolic computation of the Jacobian maybe expensive or it may require the user's input. This is a drawback of Newton's method for the implicit tau-leaping step. The method we proposed in the next Section will address this issue.

Despite its computational prowess, the adaptive explicit-implicit tau-leaping method is not with out faults, however, we will leave this discussion for the subsequent Section and the conclusion.

4.4

Modified Adaptive Tau-Leaping Method

In the previous section we heaped praise upon the increased automatization observed in the adaptive explicit-implicit method but also noted that room for improvement exists. Automatization is the proverbial double-edged sword. On one hand, the checks and balances are carried out mechanically, one the other, the Jacobian has to be inputted by the user. Approximating the Jacobian using the 79

finite-difference method limits the need for symbolic computation.

The finite-difference strategy will be used to approximate the Jacobian in Newton's method. The results we publish in Chapter 5, will affirm the accuracy of the new user-friendly modified algorithm.

From the previous paragraph it is apparent that the difference between this amalgamated method and its adaptive predecessor lies in Newton's method. As such, all of the steps outlined in the previous section apply here as well. While other finite-difference schemes may be used to estimate first order derivatives, we apply the forward finite-difference scheme, for simplicity. Thus we estimate
Fk Xi

by,

Fk F (X1 , . . . , Xi , Xi + h, Xi+1 , . . . , Xn ) - Fk (X1 , . . . , Xn ) (X1 , . . . , Xn ) = Xi h

for any 1  i  N and 1  h  N . Here 0 < h

1. Recall that X is an

N -dimension array and F is N -dimensional function of X . The Jacobian will be approximated by matrix (4.29)

F ( x) X

1 F (X1 + h, X2 , . . . , Xn ) - F (X ), F (X1 , X2 + h, . . . , Xn ) - F (X ), h . . . , F (X1 , X2 , . . . , Xn + h) - F (X ) (4.29)

with F (X ) = [F1 (X ), F2 (X ), . . . , FN (X )]T .

80

In the case of the implicit tau-leaping method F (X ) is given by formula (4.13) in Section 4.2.2. We note that for large biochemical systems, the computation of the exact Jacobian is challenging, while the finite-difference approximation is straightforward.

With this approximation Newton's step in the implicit scheme becomes a pseudoNewton's method, which may, theoretically, be less accurate per iteration and therefore it may require more iterations to achieve the same accuracy. However, the numerical tests performed (see Chapter 5) show that the same accuracy is obtained with very similar computational costs. The implementation of the new method is straightforward, even for large systems.

Leaning upon the foundation built in the previous three Sections we are now ready to present the algorithm for the modified explicit-implicit tau-leaping method.

1. Specify the parameters, including the stoichiometric matrix, number of species, reaction channels and simulations, tolerances T OL and , rate constants, final time T , nc and empty arrays capable for storing critical and non-critical reaction indices.

2. Initialize the time t = 0 and the state X (0) = x0 .

3. Compute the propensity functions and consequently update after each simulation.

81

4. For each trajectory, at each time t categorize the reactions. We begin, by introducing Lj which represents the number of times a reaction can fire before one of the reactants is exhausted:

Lj =

i[1,N ],i,j <0

min

xi . |i,j |

A reaction Rj is deemed to be a critical reaction if Lj < nc ; for our purposes the generally accepted value of nc = 10 was used.

5. Armed with the knowledge of which reactions adhere to which class, we introduce Jcr and Jncr , respectively denoting the set of critical and non-critical reaction indices. If all the reactions happen to be critical, then set  =  and proceed to step 7.

6. Identify reversible reactions in the system at hand. Our objective is to create a set of indices which correspond to reversible reactions not in partial equilibrium, which will be signified by Jne . Partial equilibrium is defined as the condition where correspondent, a+ (x) and a- (x) are close to each other. The difference between the two must be smaller than each respective propensity. Specifically, the partial equilibrium condition is,

|a+ (x) - a- (x)|   min{a+ (x), a- (x)},

82

where the generally accepted value of  is 0.05. If the system is already in equilibrium then  can be chose to be sufficiently large.

7. If non-critical reactions are present, then the following multi-step sequence is used to calculate the explicit candidate for  . Note in the adaptive explicit implicit method, we now have two  candidates,  (ex) and  (im) corresponding to the explicit and implicit candidates respectively. The way we compute the quantities changes. The explicit method will still correspond to the indices set Jncr , while the implicit scheme will draw from the set Jnecr . The latter represents the set of reactions which are both non-critical and not in partial equilibrium, in the set theory notation this is formulated as Jnecr = Jncr  Jne .

I) We first determine the highest order of reaction (or HOR(i)) for a chemical species Si . As was discussed earlier, reactions found only on the left side of the equation will be considered. The order of a reaction is equivalent to the number of times a reactant is seen in each reaction; the highest such number is the HOR(i).

II) In this step the value of

i

is computed, where,

i

=

gi

.

(4.30)

is the tolerance and gi = gi (xi ) is computed using 4.3, 4.4, 4.5 and 4.6.

III) Computing the explicit candidate for the next time step requires the following 83

two quantities, i (x)(ex) :=
j Jncr 2

ij aj (x),

(4.31)

i (x)(ex)

:=
j Jncr

2 ij aj (x).

(4.32)

IV) Computing the implicit candidate for the next time step requires the next two quantities, i (x)(im) :=
j Jnecr 2

ij aj (x),

(4.33)

i (x)(im)

:=
j Jnecr

2 ij aj (x),

(4.34)

where Jnecr = Jncr  Jne is the set of non-critical reactions which are not in partial equilibrium.

IV) The explicit candidate for the next time step is given by  (ex) ,

 (ex) = min

max{ xi /gi , 1} max{ xi /gi , 1}2 , . |i (x)(ex) | [i (x)(ex) ]2

(4.35)

And the implicit candidate by  (im) ,

 (im) = min

max{ xi /gi , 1} max{ xi /gi , 1}2 , . |i (x)(im) | [i (x)(im) ]2

(4.36)

84

8. Compute the sum of ac 0 (x), the propensities of all critical reactions. Using this generate a second candidate 2 , with equation (4.37)

2 =

1 ac 0 ( x)

и ln (1/r1 ),

(4.37)

where r1 is a sample from the unit-interval uniform distribution. 2 represents the step to the next slow reaction.

9. If  (im) is greater than Nstif f  (ex) , where Nstif f usually takes on the value 100, then the system is considered to be stiff, we let 1 =  (im) . Otherwise 1 =  (ex) .

10. If 2 > 1 then  = 1 and we proceed with explicit tau-leaping (4.1) if 1 =  (ex) or implicit tau-leaping (4.11) if 2 =  (im) . The implicit system (4.11) with F given by (4.13) is solved by the pseudo-Newton method using the approximate Jacobian (4.29). Else  = 2 followed by simulation with the SSA for the slow reactions.

11. Update X (t +  ), set time to t = t +  and exit this sequence. Return to step 2 and repeat until a sufficient number of trajectories has been generated.

Before presenting the numerical results, let us review the content of this Section. We began with the explicit tau-leaping method and introduced the concept of adaptivity. We described the automatic selection of the time-step based on quasi-steady states when stiffness is present; we also presented the implicit tau-leaping method, which is well-suited to approximate stiff systems, due to the absence of step-size 85

restriction. In the same subsection, we also introduced Newton's method, a numerical solution to the implicit component of equation (4.11). Using the algorithms developed in Sections 4.1 and 4.2 the state-of-the-art adaptive explicit-implicit tauleaping method was presented. It is considered to be state-of-the-art because it combines the explicit and implicit methods with the SSA to form an algorithm designed to approximate a wide spectrum of systems.

Finally, our contribution is the modification of the adaptive explicit-implicit tauleaping scheme is the creation of a new algorithm that is far more user-friendly in the absence of symbolic computation that is equally accurate and efficient in comparison to the original adaptive strategy. The new strategy is designed for larger systems, but not large enough where simulation with the CLE or RRE would preferable, and systems featuring complex propensity functions (for instance, propensity functions that are not polynomials).

86

Chapter 5 Numerical Results
Numerical results presented in this thesis will be underpinned by three models, stiff, decay-dimerization and cycle systems. We illustrate the advantages of our new adaptive explicit-implicit tau-leaping strategy for the Chemical Master equation over the state-of-the-art tau-selection scheme by Cao et al. [3] and the exact Stochastic Simulation Algorithm developed by Gillespie.

5.1

Stiff Model

The first model we consider is a stiff model [28].

87

1 S1 + S2 -  S3

C

2 S3 -  S1 + S2

C

3 S1 + S3 -  S2

C

4 S2 -  S1 + S3

C

4 S3 + S2 -  S1

C

4 S2 -  S3 + S2

C

The simulation interval is [0,0.01], the stoichiometric matrix is,  

1 -1 -1 1 -1 1       = -1 1 1 -1 -1 1  ,     1 -1 -1 1 -1 1 with rate constants,           c=           25    104     -3  10  ,  10-1      10-2    2

88

and propensities,           a=           c1 x 1 x 2    c2 x 3     c3 x 1 x 3   .  c4 x 2      c5 x 2 x 3    c6 x 1

For this model the initial conditions are,  

 1000       X (0) =  1000  ,     10

with a tolerance (TOL) of 0.0275 and h = 0.1.

After simulating 10,000 trajectories of the exact SSA, the explicit-implicit variable step-size tau-leaping scheme and the modified explicit-implicit variable step-size tau-leaping strategy, we present our findings below. Figure 5.1 shows the histograms at t = 0.01 of species X1 generated with the above three methods. Figures 5.2 and 5.3 present the histograms for species X2 and X3 respectively. The accuracy of the modified adaptive tau-lap method matches very well that of the standard adaptive tau-leaping scheme, both matching well the accuracy of the exact SSA. Moreover,

89

the speed-up of the modified adaptive tau-leaping scheme is defined as,

speed - up(%) =

CP U (SSA) и 100. CP U (mod  - leap)

For this model speed - up(%) = 1496.82

for the modified adaptive explicit-implicit tau-leaping scheme.

Figure 5.1: Stiff Model: Histogram of the X1 species (SSA vs. Adaptive TauLeaping vs. Modified Adaptive Tau-Leaping) at t=0.01

90

Figure 5.2: Stiff Model: Histogram of the X2 species (SSA vs. Adaptive TauLeaping vs. Modified Adaptive Tau-Leaping) at t = 0.01

Figure 5.3: Stiff Model: Histogram of the X3 species (SSA vs. Adaptive TauLeaping vs. Modified Adaptive Tau-Leaping) at t = 0.01

91

5.2

Decay-Dimerization Model

We have partially familiarized ourselves with this model through our demonstration of fundamental concepts in Section 3.1. Nonetheless, we restate it while filling in missing information such as the rate constants and propensities. It should be noted this model is inherently stiff. The model is subjected to the following reaction channels [20],

1 S1 -  0

C

3 S1 + S1 -  S2

C

2 S2 -  S1 + S1

C

4 S2 -  S3

C

The model operates on the time interval [0,3], with the stoichiometric matrix,  

0 -1 -2 2 , =   0 1 -1 -1 rate constants,   1    0.1  c=   25    0.04       ,     

92

and propensities,  c1 x 1     c x1 (x1 -1)  2 2 a=   c3 x 2    c4 x 2 The initial conditions are,    1000   X (0) =    1000 with a tolerance (TOL) of 0.04 and h = 0.1.       .     

We ran simulations on 10,000 trajectories for the SSA, the adaptive explicit-implicit method and the modified adaptive explicit-implicit strategy. The histograms obtained with the above techniques at t = 3 are presented in Figure 5.4 for species X1 and in Figure 5.5 for species X2 . We remark the good agreement among these methods, showing the accuracy of the adaptive explicit-implicit algorithms. The modified variable step-size tau-leaping is as accurate as the state-of-the-art adaptive tau-leap strategy. From Table 5.2, we see that the speed-up of the modified tau-leaping scheme over the SSA is

speed - up(%) = 463.86.

93

Figure 5.4: Decay-Dimerization model: Histograms of X1 species (SSA vs. Adaptive Tau-Leaping vs. Modified Adaptive Tau-Leaping) at t = 3

Figure 5.5: Decay-Dimerization model: Histograms of X3 species (SSA vs. Adaptive Tau-Leaping vs. Modified Adaptive Tau-Leaping) at t = 3

94

5.3

Modified Cycle Model

The last model we ponder upon is the cycle model [29].

1 S1 -  S2

C

2 S2 -  S3

C

3 S3 -  S1

C

4 S1 + S4 -  S5

C

5 S5 -  S1 + S4

C

The model operates on the time interval [0,0.05], with the stoichiometric matrix,  

1 -1 1  -1 0       1 -1 0 0 0       . = 0 1 - 1 - 1 0       0 0 0 -1 1        0 0 0 1 -1 With rate constants,

95

        c=        and propensities,         a=        The initial conditions are,         X (0) =        

 1.50 и 103    5.00 и 103     1.00 и 103  ,   -4  1.66 и 10    8.00 и 10-2

 c1 x 1    c2 x 2     c3 x 3  .   c4 x 1 x 4     c5 x 5

 1000    800     400  .   40     50

with a tolerance (TOL) of 0.05 and h = 0.1.

For this model, we performed 10,000 simulations with each of the following algorithms: SSA, the state-of-the-art adaptive explicit-implicit algorithm and the modified explicit-implicit technique. The histograms for at t = 0.05 of the three simulation methods for the species X1 are shown in Figure 5.6, Figure 5.7 for 96

species X2 and Figure 5.8 for species X3 . These results demonstrate that the modified variable step-size tau-leaping performs as well as the state-of-the-art adaptive tau-leaping scheme, both consistent with the results obtained using the SSA. This demonstrates that the leaping techniques are accurate. The CPU times of the three methods are given in Table 5.1. We note that the speed-up of the modified adaptive tau-leaping scheme over the SSA is,

speed - up(%) = 395.53.

Figure 5.6: Modified Cycle model: Histograms of X1 species (SSA vs. Adaptive Tau-Leaping vs. Modified Adaptive Tau-Leaping) at t = 0.05.

97

Figure 5.7: Modified Cycle model: Histograms of X2 species (SSA vs. Adaptive Tau-Leaping vs. Modified Adaptive Tau-Leaping) at t = 0.05.

Figure 5.8: Modified Cycle model: Histograms of X3 species (SSA vs. Adaptive Tau-Leaping vs. Modified Adaptive Tau-Leaping) at t = 0.05.

98

5.4

Table of Results
SSA (s) Stiff Decay-Dimerization Cycle 4926.64 8398.42 8696.21 Adaptive (s) 375.41 1856.73 2260.13 Modified Adaptive (s) 329.14 1810.57 2198.61

Table 5.1: Computational times of the SSA, Adaptive and Modified Tau-Leaping Methods.

Adaptive vs. SSA (%) Stiff Decay-Dimerization Cycle 1312.33 452.50 382.70

Modified Adaptive vs. SSA (%) 1496.82 463.86 395.53

Table 5.2: Improvement in computational speed of the adaptive tau-leaping method vs. SSA.

99

Chapter 6 Conclusion and Further Research Topics
This thesis studied effective simulation methods for stochastic models of well-stirred biochemical systems, with a focus on the explicit and implicit tau-leaping strategies for the Chemical Master equation. Since many biochemical systems in applications have stiff mathematical models, it is essential to develop effective computational tools to study them. One critical tool for overcoming stiffness is adaptive timestepping. Our first conclusion is confirmation of time-wise computational improvement of the adaptive explicit-implicit tau-leaping scheme in contrast to the exact stochastic simulation algorithm for the Chemical Master equation. The numerical results substantiated that the state-of-the art adaptive tau-leaping method was faster than the SSA up to an order of magnitude of 14. This strategy uses Newton's method for the implicit tau-leaping steps. Arguably the most important result garnered is the performance of the modified adaptive explicit-implicit tau-leaping

100

algorithm in terms of accuracy and efficiency. The proposed modified scheme employs a pseudo-Newton's method. The numerical results and computational cost of the modified method were closely aligned with the state-of-the-art adaptive scheme. This is vital given that the modified adaptive explicit-implicit tau-leaping technique is more user-friendly following the replacement of the symbolic computation component, a challenge to potential users, with the finite-difference approximation of the Jacobian. The method is ideal for larger systems not well-suited for simulation using the CLE or RRE and those with complex propensity functions, particular propensities not in polynomial form. Finally, we have demonstrated using smaller and larger models that the methods are well-suited to handle a large class of problems.

Reflecting on this work three topics of interest immediately come to mind. First is the issue of negative species populations. This scenario presented itself on more than one occasion. This phenomenon becomes especially problematic in systems with species with low population numbers, which remain close to zero on some time-interval. Species populations falling below zero contaminate the results; intuitively it is evident that negative populations do not exist. A second topic worth exploring is a machine learning problem. A parameter that did not receive much attention and was left largely unchanged, was the tolerance. Using machine learning techniques one could condition the algorithm to optimize the relationship between efficiency and accuracy, by adjusting the tolerance according to system predispositions. Finally, there remains a need for further automatization to minimize the

101

amount of human input. Additionally, this would allow the algorithm to handle more complex systems, perhaps even problems beyond biochemical kinetics.

102

Bibliography
[1] H.L. Anderson,1986. Metropolis, Monte Carlo, and the MANIAC. Los Alamos Science 14, 96-107. [2] Y. Cao, D.T. Gillespie, L.R. Petzold, 2006. Efficient step size selection for the tau-leaping simulation method. J. Phys. Chem. 124, 044109-1-11. [3] Y. Cao, D.T. Gillespie, L.R. Petzold, 2007. Adaptive explicit-implicit tauleaping method with automatic tau selection. J. Phys. Chem. 126, 224101. [4] J.L. Doob, 1953. Stochastic Processes. Wiley,New York. [5] R. Eckhardt,1987. Stan Ulam, John von Neumann, and the Monte Carlo method. Los Alamos Science 15, 131-137. [6] M.B. Elowitz, A. Levine, E. Siggia, & P. Swain (2002). Stochastic Gene Expression in a Single Cell. Science 297(5584), 1183Г1186. [7] N. V. Fedoroff, W. Fontana, 2002. Small Numbers of Big Molecules. Science 297, 1129Г1131.

103

[8] M.A. Gibson, J. Bruck, 2000. Efficient Exact Stochastic Simulation of Chemical Systems with Many Species and Many Channels. J. Phys. Chem. A 104, 18761889 [9] D.T. Gillespie, 1976. A general method for numerically simulating the stochastic time evolution of coupled chemical reactions. Journal of Computational Physics 22, 403434. [10] D.T. Gillespie, 1992. A rigorous derivation of the chemical master equation. Physica A 188, 402Г425. [11] D.T. Gillespie, 1992. Markov processes, an introduction for physical scientists. Academic Press, New York. [12] D.T. Gillespie, 2000. The chemical Langevin equations. J. Phys. Chem. 113, 297Г306. [13] D.T. Gillespie, 2001. Approximate accelerated stochastic simulation of chemically reacting systems. J. Chem. Phys. 115, 1716Г1733. [14] D.T. Gillespie, 2007. Stochastic Simulation of Chemical Kinetics.

Annu. Rev. Phys. Chem. 58, 35Г55. [15] D.T. Gillespie, L.R. Petzold, 2003. Improved leap-size selection for accelerated stochastic simulation. J. Chem. Phys. 119, 8229Г8234. [16] D.J. Higham, 2008. Modeling and Simulating Chemical Reactions. SIAM Review. 50, 347Г368.

104

[17] T. Ideker, T. Galitski, L. Hood, 2001. NEW APPROACH TO DECODING LIFE: Systems Biology. Annu. Rev. Genom. Hum. Genet. 2, 343Г372. [18] T. G. Kurtz, 1972. The relationship between stochastic and deterministic models for chemical reactions. J. Chem. Phys. 57(7), 29762978. [19] R. Kyle, 1979. Jean Baptiste Perrin. JAMA: The Journal of the American Medical Association, 242 (8), 744. [20] H. Li, L.R. Petzold, 2005. Stochastic Simulation of Biochemical Systems on the Graphics Processing Unit. Bioinformatics 00, 1-5. [21] H. H. McAdams, A. Arkin, 1997. Stochastic mechanisms in gene expression. Proc. Natl. Acad. Sci. U.S.A. 94(3), 814Г819. [22] H. H. McAdams, A. Arkin, J. Ross, 1997. Stochastic kinetic analysis of developmental pathway bifurcation in phage lambda-infected Escherichia coli cells. Genetics 149(4), 1633Г1648. [23] H. H. McAdams, A. Arkin, 1999. It's a noisy business!. Trends in Genetics 15(2), 65Г69. [24] Q. Meng, V-P. Mkinen, H. Luk, X. Yang, 2013. Systems Biology Approaches and Applications in Obesity, Diabetes, and Cardiovascular Diseases. Current Cardiovascular Risk Reports. 7(1), 73-83. [25] N. Metropolis, 1987. The beginning of the Monte Carlo method, Los Alamos Science 15, 125-130.

105

[26] R. Feynman, 1964. The Brownian Movement. The Feynman Lectures of Physics, 1,s 41-1. [27] M. Rathinam, Y. Cao, D.T. Gillespie, L.R. Petzold, 2003. Stiffness in stochastic chemically reacting systems: The implicit tau-leaping method. J. Phys. Chem. 119, 12784Г12794. [28] Y. Sotiropoulos, Y.N. Kaznessis, 2008. An adaptive time step scheme for a system of stochastic differential equations with multiple multiplicative noise: Chemical Langevin equation, a proof of concept. J. Chem. Phys. 128, 014103. [29] Y. Sotiropoulos, M. N. Contou-Carrere, P. Daoutidis, Y.N. Kaznessis, 2009. Model Reduction of Multiscale Chemical Langevin Equations: A Numerical Case Study. IEEE/ACM Trans. Comput. Biol. Bioinf. 6, 470. [30] D.J. Wilkinson, 2006. Stochastic Modeling for Systems Biology. Chapman & Hall/CRC Mathematical and Computational Biology Series, 45Г89, 91Г 92, 109Г133, 157Г160.

106

