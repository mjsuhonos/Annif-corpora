Ryerson University

Digital Commons @ Ryerson
Theses and dissertations

1-1-2011

Non linear estimation of returns on hedge funds with scarce observations
Akram Samarikhalaj
Ryerson University

Follow this and additional works at: http://digitalcommons.ryerson.ca/dissertations Part of the Applied Mathematics Commons Recommended Citation
Samarikhalaj, Akram, "Non linear estimation of returns on hedge funds with scarce observations" (2011). Theses and dissertations. Paper 754.

This Thesis is brought to you for free and open access by Digital Commons @ Ryerson. It has been accepted for inclusion in Theses and dissertations by an authorized administrator of Digital Commons @ Ryerson. For more information, please contact bcameron@ryerson.ca.

Non Linear Estimation of Returns on Hedge Funds with Scarce Observations

by

AKRAM SAMARIKHALAJ Bachelor of Science, Shahrood University Of Thechnology, 1999

A thesis presented to Ryerson University in partial fulfillment of the requirements for the degree of Master of Science in the program of Applied Mathematics

Toronto, Ontario, Canada, 2011

c

Akram Samarikhalaj, 2011

Author's Declaration

I hereby declare that I am the sole author of this thesis. I authorize Ryerson University to lend this thesis to other institutions or individuals for the purpose of scholarly research.

I further authorize Ryerson University to reproduce this thesis by photocopying or by other means, in total or in part, at the request of other institutions or individuals for the purpose of scholarly research.

ii

Abstract

Non Linear Estimation of Returns on Hedge Funds with Scarce Observations Master of Science 2011 Akram Samarikhalaj Applied Mathematics Ryerson University

Explaining the behavior of a financial portfolio like a Hedge Fund is challenging for many reasons, one of those reasons is scarce observations. One possibility to circumvent these issues is to find simple relationships between the portfolio and financial factors. These factors are observed more frequently so it is valid to assume that one can estimate not only the conditional expectation with respect to single factors, but also the joint law of all the underlying factors. The problem, then, is to recover the conditional expectation of the portfolio's return given all the factors.The author of the paper,"Measuring Risk With Scarce Observation" prescribes a reasonable criteria which provides existence and uniqueness to this problem also characterizes the solution under the assumption of Gaussian distribution among the factors( Independent factors). In our thesis, we present a solution for the case when the joint law of factors is a multivariate t-student distribution.

iii

Acknowledgments
I would like to thank my supervisor, Dr. Marcos Escobar, and my co supervisor, Dr. Sebastian Ferrando, who guided me along the way. It would not have been possible to complete this project without their help and advice. I would also like to thank my parents and my sisters for their support during my studies at Ryerson University.

iv

Contents
1 Introduction: 1.1 Hedge Funds . . . . . . . . . . 1.2 Regression Analysis . . . . . . 1.2.1 Linear Regression: . . 1.2.2 Non-Linear Regression: 1.2.3 Least Squares . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 3 5 8 11 12 19 19 20 24 26 27 31 32 33 36 40 45 49 50 51

2 Cherny's Framework 2.1 Problem and General Solution . . . . . . . . . . . . . . . . . . . . . 2.2 General Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 Solution for Gaussian and Independent Measures . . . . . . . . . . 3 Alternative Mathematical Framework 3.1 Modified Setup . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1.1 Gaussian and Independent Measures in our Framework 3.2 Results Under a t-Student Measure . . . . . . . . . . . . . . . 3.2.1 Orthogonal Polynomials and Properties . . . . . . . . . 3.2.2 Multivariate t-Student . . . . . . . . . . . . . . . . . . 3.2.3 Solution for a t-Student. Three Particular Cases . . . 3.2.4 Solution for a t-Student. General Case . . . . . . . . . 3.3 Other Probability Measures . . . . . . . . . . . . . . . . . . . 4 Conclusions 5 References

. . . . . . . .

. . . . . . . .

. . . . . . . .

v

1

Introduction:

Our interest is a special type of investment companies called hedge funds. These are loosely regulated companies which could invest in a variety of complicated products, making their performance different to that of common stocks. These companies report returns (Y ) on a monthly basis leading to scarce data bases therefore making statistical analysis more challenging. In general, investments are interested in many financial objectives related to hedge funds. For example, they may want to measure the risk of a Hedge fund and therefore trying to explain the returns (Y ) and the variation in Y in terms of the variations of a set of variables X which could represent macroeconomic variables, fundamentals of a company or simple stocks and indexes. In general these variables are called factors for simplicity. Note that to measure the risk of a financial product with scarce observation, the simplest way is to relate it to the values of certain financial factors which are more popular or stable therefore leading to a more robust analysis. They also may want to find ways to hedge the performance of hedge funds companies on which they may have large investment allocations. This hedging exercise protect them, in particular, against downward movements of the returns of the hedge funds companies in their portfolios. A hedging exercise could be created by investing on common stocks, indexes (X ) or even on financial derivatives (f (X )), the later are basically nonlinear functions on the underlying stocks and therefore can be seen as quadratic or higher order functions. This means that these investment companies would like to know how to combine allocations on polynomial functions of separate stocks (g (f1 (X1 ), ..., fN (XN ))) in order to hedge the hedge fund returns (Y ). The mathematical problem would be easily solvable via regression analysis assuming enough data is available not only for the factors (X ) to be used in the hedging but also for the hedge funds to be hedged. This later conditions is the one that fails as there are only dozens of data points available from hedge funds companies which come from based monthly performances (Y ). On the other hand stocks or indexes are pretty much available on a daily or even intra-day basis making the analysis of their relationships (X = (X1 , ..., XN )) easier to describe as opposed to describing the relationship between the hedge funds and the stocks (Y = g (X )). In this context a recent paper: "Measuring Risk With Scarce Observation" [1], proposed an alternative approach to standard regression analysis with the purpose 1

of building an optimal multidimensional function g . The method proposed by the authors uses strongly the joint probability distribution of the factors X and put forward a new concept of optimality based on a two steps approach: it first finds the best relationship between each marginal factor (Xi ) and the dependent variable Y , this is achieved via standard regression analysis therefore minimizing the fitting error. In a second step it looks for the multivariate function g with minimum variance such that the marginal fittings are satisfied. The requirement of minimum variance, is a way of finding a reasonably well behaved multidimensional function among all possible candidates avoiding at the same time the use of the joint distribution of X and the scarce variable Y . This thesis studies in detail the paper by [1] and the statistical relationships and methods provided in there. It covers the topic of regression (linear and non-linear), which is one of the steps in the targeted methodology. In this case the regression could be performed between some financial factors and the returns of hedge fund companies. The statistical relationship between the factors altogether and a set of given hedge fund returns will be studied using the second step in [1]. Even though the authors provide some theoretical results about this two steps procedure, they fail to completely provide a methodology that could be used beyond the two simple cases they managed in their examples. The cases they used as examples were those when the factors X follow a multivariate Gaussian distribution (also a multivariate Gaussian Copula or dependence structure was studied) and the case where the factors were assumed independent. In finance and economics Gaussian assumptions among variables are extremely unrealistic. This is due to the presence of asymmetries (skewness), high probability of extreme events (fat tails) and the presence of tail dependence as a non Gaussian copula feature. One of the most popular non-Gaussian random variable is the t-student, this is because the distribution satisfies some of the previously mentioned stylized facts on its univariate and multivariate variants. In order to adapt the aforementioned paper [1] to a context beyond Gaussian, several changes were performed. Among them, finding polynomials that has orthogonality relationship under a given measure can be a first step. This leads to some drawbacks in terms of the family of functions in which the optimal solution is found as well as on the possible marginal regression fittings that were compatible with the methodology. This modified analysis was then applied to the case of a univariate and therefore a multivariate extension of the t-student distribution. Some other possible families 2

of probability measures and the associated orthogonal polynomials were also mentioned. The thesis is organized as follows: the next section provides an overview of hedge funds and regression techniques (linear and nonlinear). Chapter 2 reviews the most important results from [1] with emphasis on the results that will be extended or modified. Chapter 3 provides the novel results in the thesis, starting with Section 3.1 and the modifications to the existing framework as well as the methodology to build solutions. In Section 3.2, the application of the results from the previous section are developed. This involves defining the orthogonal polynomials under a t-student distribution, then selecting an appropriate multivariate t-student distribution and considering inner products under this measure. After that, we study in detail three particular cases which correspond to the smallest degrees of freedom, and therefore, representing the cases farther away from the Gaussian measure. We also consider the general case of any given number of degrees of freedom. Section 3.3 motivates some other measures and orthogonal functions for future research. Chapter 4 concludes.

1.1

Hedge Funds

A hedge fund is a fund that can take both long and short positions, look for arbitrage opportunities, buy and sell undervalued securities, trade options or bonds, and invest in almost any opportunity in any market where it foresees impressive gain at reduced risk. The primary aim of most hedge funds is to reduce risk while attempting to preserve capital and deliver positive returns under all market conditions. Hedge funds are investment vehicles that explicitly pursue absolute returns on their underlying investments. The description "Absolute Return Fund" would be more accurate, since not all hedge funds contain an explicit hedge on their portfolio of investments. However the "Hedge Fund" definition has come to incorporate, any absolute return fund investing within the financial markets (stocks, bonds, commodities, currencies, derivatives, etc) and/or applying non-traditional portfolio management techniques including, but not restricted to, shorting, leveraging, arbitrage, swaps, etc. Hedge funds can invest in any number of strategies. These are perhaps identifiable by their structure, a limited partnership (the manager acting as the general partner and investors acting as the limited partners) with 3

performance related fees, high minimum investment requirements and restrictions on types of investors and entry and exit periods [6]. Investors decide to allocate funds to hedge funds for several reasons: 1. To increase the return on the portfolio. Many hedge funds have performed well in both absolute and relative return to aggregate stock and bond returns which is enough to make them appealing to many investors. 2. To diversify the returns of assets within the portfolio. Diversification involves a statistic called correlation. Correlation is a single number that describes the degree of relationship between two or more variables. For example, A correlation of one means that the two numbers related and if one grows so does the other. Two assets in the same industry provide less risk reduction from diversification than a combination of unrelated companies. A well diversified portfolio combines the returns of many assets often with some effort devoted to identifying returns that are not correlated. 3. To reduce risk. Many hedge fund have lower risk than traditional assets [15]. Note that the return of the hedge fund is published monthly and it reduces the size of the sample to estimate. For example if there are two years history of a hedge fund then there are two dozen observations. The confidence (or prediction) interval is an estimate of an interval in which future observations will fall and often used in regression analysis. It is used to indicate the reliability of an estimate. A major factor determining the length of the confidence interval is the size of the sample used in the estimation procedure. For a smaller confidence interval more precise results will be obtained. The greater the sample size the smaller the size of confidence interval and the greater the number of variable the greater the size of confidence interval. In hedge fund analysis the size of the sample is small as the return is monthly and we have small data.

4

1.2

Regression Analysis

In statistics, regression analysis includes any technique used for modeling and analyzing several variables when the focus is on the relationship between a dependent variable and one or more independent variables. The relationship is expressed in the form of an equation or a model connecting the response or dependent variable and one or more explanatory or predictor variables. Regression analysis estimates the conditional expectation of the dependent variable given the independent variables. The conditional expectation is the average value of the dependent variable when the independent variables are held fixed. The estimation target is a function of the independent variables called the regression function[16]. A large body of techniques for carrying out regression analysis has been developed. Familiar methods such as linear regression and ordinary least squares regression are parametric. This means that a regression function is defined in terms of a finite number of unknown parameters that are estimated from the data. Nonparametric regression refers to techniques that allow the regression function to lie in a specified set of functions, which may be infinite-dimensional. The performance of regression analysis methods in practice depends on the form of the data-generating process, and how it relates to the regression approach being used. Since the true form of the data-generating process is not known, regression analysis depends to some extent on making assumptions about this process. These assumptions are sometimes (but not always) testable if a large amount of data is available. Regression models for prediction are often useful even when the assumptions are moderately violated, even though they may not perform optimally. In many applications, especially with small effects or questions of causality based on observational data, regression methods give misleading results[20].

Regression models involve the following terms: 1 - The unknown parameters,  . It can be a scalar or a vector. 2 - The independent variables, X.

5

3 - The dependent variable, Y.

A regression model relates Y to a function of X and  . Y  f (X,  ) The approximation is usually formalized as E (Y |X ) = f (X,  ). To carry out parametric regression analysis, the form of the function f must be specified. Sometimes the form of this function is based on knowledge about the relationship between Y and X that does not rely on the data. If no such knowledge is available, a flexible or convenient form for f is chosen. Assume now that the vector of unknown parameters  is of length k . In order to perform a regression analysis the user must provide information about the dependent variable Y: - If n data points of the form (Y, X ) are observed, where n < k , most classical approaches to regression analysis cannot be performed, since the system of equations defining the regression model is undetermined, there is not enough data to recover  . - If exactly n = k data points are observed, and the function f is linear, the equations Y = f (X,  ) can be solved exactly rather than approximately. This reduces to solving a set of N equations with N unknowns (the elements of  ) , which has a unique solution as long as the X are linearly independent. If f is nonlinear, a solution may not exist, or many solutions may exist. - The most common situation is where n > k data points are observed. In this case, there is enough information in the data to estimate a unique value for  that best fits the data in some sense. In the last case, the regression analysis provides the tools for: Finding a solution for unknown parameters  that will, for example, minimize the distance between the measured and predicted values of the dependent variable Y (also known as method of least squares). 6

Under certain statistical assumptions, regression analysis uses the surplus of information to provide statistical information about the unknown parameters  and predicted values of the dependent variable Y [20]. Decomposition property: Any random variable y can be expressed as y = E (y |x) +  where E (y |x) is the expected value of y for given values of random variable X and  is a random variable satisfying i) E (|x) = 0 ii)E (h(x)) = 0 where h(.) is any function of x. This means any variable can be decomposed in two parts: conditional expectation and orthogonal error term. Prediction property: Let m(x) be any function of x. Then E (y |x) = argminm(x) E [(y - m(x))2 ] intuition : the conditional expectation is the best prediction where ' best ' means minimum mean squared error. Proof 1.1 (y - m(x))2 = [(y - E (y |x)) - (E (y |x) - m(x))]2 = (y - E (y |x))2 + (E (y |x) - m(x))2 - 2(y - E (y |x))(E (y |x) - m(x))

· The first term is not affected by the choice of m(x). · The third term (y - E (y |x))(E (y |x) - m(x)) = (x)h(x) and E ((x)h(x)) = 0 by the decomposition property. Hence the whole expression is minimized if m(x) = E (y |x) see [18].

7

1.2.1

Linear Regression:

In linear regression, data is modeled using linear functions, and unknown model parameters are estimated from the data. Given a data set {yi , xi1 , . . . , xik }n i=1 of n statistical units, a linear regression model assumes that the relationship between the dependent variable yi and the k-vector of independent variables xi is linear. This relationship is modeled through a term i , an unobserved random variable that adds noise to the linear relationship between the dependent variable and independent variables. Thus the model takes form: yi = 1 xi1 + · · · + p xik + i = xi  + i , i = 1, . . . , n,

where xi is the ith. (row) vector of predictors for n observation and  is the vector of regression parameters to be estimated and i is a random error. denotes the transpose, so that xi  is the inner product between vectors xi and  . Some remarks on general use:

· yi , is called dependent variable. · The decision as to which a variable in a data set is modeled as the dependent variable and which are modeled as the independent variables may be based on a presumption that the value of one of the variables is caused by, or directly influenced by the other variables. Alternatively, there may be an operational reason to model one of the variables in terms of the others, in which case there need be no presumption of causality. · xi , are called predictor variables, or independent variables. · Usually a constant is included as one of the independent variables. For example we can take xi1 = 1 for i = 1, ..., n. The corresponding element of  is called the intercept. Many statistical inference procedures for linear models require an intercept to be present, so it is often included even if theoretical considerations suggest that its value should be zero. 8

· Sometimes one of the independent variables can be a non-linear function of another independent variables or of the data, as in polynomial regression and segmented regression. The model remains linear as long as it is linear in the parameter vector  . · The independent variables xi may be viewed either as random variables, which we simply observe, or they can be considered as predetermined fixed values which we can choose. ·  , is a k-dimensional parameter vector. Its elements are also called regression coefficients. i , is called the error term, or noise. This variable captures all other factors which influence the dependent variable y other than the independent variables xi .The relationship between the error term and the independent variables, for example whether they are correlated, is a crucial step in formulating a linear regression model, as it will determine the method to use for estimation [13]. Assumptions of linear regression: There are some principal assumptions which justify the use of linear regression models for purposes of prediction: (1) Linearity of the relationship between dependent and independent variables (linearity on the parameters). Violations of linearity are extremely serious if we fit a linear model to data which are nonlinearly related, our predictions are likely to be seriously in error, especially when we extrapolate beyond the range of the sample data. (2) Independence of the errors (no serial correlation). The errors also assumed to be uncorrelated across observations, so that for two observations i and j, the covariance between i and j is zero. Violations of independence are also very serious in time series regression models: serial correlation in the residuals means that there is room for improvement in the model, and extreme serial correlation is often a symptom of a badly miss-specified model. Serial correlation is also sometimes a by-product of a violation of the linearity assumption­as in the case of a simple (i.e., straight) trend line fitted to data 9

which are growing exponentially over time. (3) Homoscedasticity (constant variance) of the errors. The errors are assumed to be homoscedastic, which means that for a given x, the errors have a constant variance. Formally , V ar(i |xi ) =  2 f or all i. When the variance differs across observations, the errors are heteroscedastic and 2 f or all i V ar(i |xi ) = i Violations of homoscedasticity make it difficult to gauge the true standard deviation of the forecast errors, usually resulting in confidence intervals that are too wide or too narrow. In particular, if the variance of the errors is increasing over time, confidence intervals for out-of-sample predictions will tend to be unrealistically narrow. Heteroscedasticity may also have the effect of giving too much weight to small subset of the data (namely the subset where the error variance was largest) when estimating coefficients. (4) Normality of the error distribution. Violations of normality compromise the estimation of coefficients and the calculation of confidence intervals. Sometimes the error distribution is "skewed" by the presence of a few large outliers. Since parameter estimation is based on the minimization of squared error, a few extreme observations can exert a disproportionate influence on parameter estimates. Calculation of confidence intervals and various significance tests for coefficients are all based on the assumptions of normally distributed errors. If the error distribution is significantly non-normal, confidence intervals may be too wide or too narrow. (5) Zero Condition Mean of . E (i |xi ) = 0. (6) The x's are linearly independent. This means that none of the x's is a linear combination of remaining x's. If any of these assumptions is violated (i.e., if there is nonlinearity, serial correlation, heteroscedasticity, and/or non-normality), then the forecasts, confidence 10

intervals, and economic insights yielded by a regression model may be (at best) inefficient or (at worst) seriously biased or misleading [13].

1.2.2

Non-Linear Regression:

The basic idea of nonlinear regression is the same as that of linear regression. Nonlinear regression is characterized by the fact that the prediction equation depends nonlinearly on one or more unknown parameters. Whereas linear regression is often used for building a purely empirical model, nonlinear regression usually arises when there are physical reasons for believing that the relationship between the response and the predictors follows a particular functional form. In the more general normal nonlinear regression model, the function f (.) relating the response to the predictors is not necessarily linear: yi = f (xi ,  ) + ri As in linear model,  is a vector of parameters and xi is a vector of predictors (but in the nonlinear regression model, these are not generally of the same dimension)and the ri are random errors. In nonlinear regression the data is fitted by a method of successive approximation [13]. Assumptions of nonlinear regression: There are some principal assumptions which justify the use of non-linear regression models for purposes of prediction: (1) The model is correct. Nonlinear regression adjusts the variables in the equation you chose to minimize the sum-of-squares. It does not attempt to find a better equation. (2) The variability of values around the curve follow a Gaussian distribution. Even though no biological variable follows a Gaussian distribution exactly, it is sufficient that the variation be approximately Gaussian. (3) Homoscedasticity (constant variance) of the errors. The errors are assumed to be homoscedastic, which means that for a given x, the errors have a constant variance. Formally , 11

V ar(i |xi ) =  2 f or all i. It means the SD (standard deviation) of the variability is the same everywhere, regardless of the value of X. The assumption is termed homoscedasticity. If the SD is not constant but rather is proportional to the value of Y, you should weight the data to minimize the sum-of-squares of the relative distances. (4) The model assumes that you know X exactly. This is rarely the case, but it is sufficient to assume that any imprecision in measuring X is very small compared to the variability in Y. (5) The errors are independent. The deviation of each value from the curve should be random, and should not be correlated with the deviation of the previous or next point. If there is any carryover from one sample to the next, this assumption will be violated [13].

1.2.3

Least Squares

The method of least squares is a standard approach to the approximate solution of overdetermined systems, i.e. sets of equations in which there are more equations than unknowns. "Least squares" means that the overall solution minimizes the sum of the squares of the errors made in solving every single equation. The most important application is in data fitting. The best fit in the least-squares sense minimizes the sum of squared residuals, a residual being the difference between an observed value and the fitted value provided by a model. Least squares problems fall into two categories: linear or ordinary least squares and non-linear least squares, depending on whether or not the residuals are linear in all unknowns. The linear least-squares problem occurs in statistical regression analysis. The nonlinear problem has no closed-form solution and is usually solved by iterative refinement; at each iteration the system is approximated by a linear one, thus the core calculation is similar in both cases. The method of least squares assumes that the best-fit curve of a given type is the curve that has the minimal sum of the deviations squared (least square error) from a given set of data. Suppose that the data points are (x1 , y1 ), ..., (xn , yn ) where x is the independent variable and y is the dependent variable. The fitting curve f (x) has the error d for each data point, i.e. d1 = y1 - f (x1 ), d2 = y2 - f (x2 ), ..., dn = yn - f (xn ). 12

According to the method of least squares, the best fitting curve has the property that: n n
2 2  = d2 1 + d2 + ... + dn = i=1

d2 1 =
i=1

[yi - f (xi )]2  min

Polynomials are one of the most commonly used curves in regression. When using an mth degree polynomial y = a0 + a1 x + a2 x2 + ... + am xm to approximate the given set of data, (x1 , y1 ), ..., (xn , yn ), where n  m + 1, the best fitting curve f (x) has the least square error, i.e.,
n n

=
i=1

[yi - f (xi )] =
i=1

2

m 2 [yi - (a0 + a1 xi + a2 x2 i + ... + am xi )] = min

Note that a0 , a1 , ..., am are unknown coefficients while all xi and yi are given. The unknown coefficients can be obtained by solving linear equations below [11].  /aj = 0 j = 1, ..., m

Solving linear least square Problem: The general problem: Consider a system
n

Xij j = yi , (i = 1, 2, ..., m)
j =1

of m linear equations in n unknown coefficients 1 , 2 , ..., n , with m > n. This can be written in matrix form as X = Y The goal is to find the coefficients  which fit the equations "best" in the sense of solving the quadratic minimization problem ^ = argminS ( )  13

where the objective function S is given by S ( ) =
m i=1

|yi -

n j =1

Xij j |2 = ||y - X ||2 .

where || . || is the standard L2 -norm in the n-dimensional Euclidean space Rn . A justification for choosing this criterion is given in properties below. This minimization problem has a unique solution, provided that the n columns of the matrix X are linearly independent, given by solving the normal equations ^ = X y. (X X ) Define the ith residual to be
n

ri = y i -
j =1

Xij j .

Then S ( ) can be rewritten
m

S ( ) =
i=1

2 ri

S is minimized when its gradient vector is zero. The elements of the gradient vector are the partial derivatives of S with respect to the parameters: S =2 j The derivatives are ri = -Xij . j Substitution of the expressions for the residuals and the derivatives into the gradient equations gives S =2 j
m n m

ri
i=1

ri (j = 1, 2, ..., n). j

yi -
i=1 k=1

Xik k 14

(-Xij ) (j = 1, 2, ..., n).

^ minimizes S, we have Thus if 
m n

2
i=1

yi -
k=1

^k Xik 

(-Xij ) = 0 (j = 1, 2, ..., n).

Upon rearrangement, we obtain the normal equations:
m n m

^k = Xij Xik 
i=1 k=1 i=1

Xij yi (j = 1, 2, ..., n).

The normal equations are written in matrix notation as ^ = X y. (X X ) ^ of the optimal parameter The solution of the normal equations yields the vector  values [12]. Properties of the least-squares estimators: The gradient equations at the minimum can be written as ^)X = 0 (y - X  ^ is orthogonal to the column space of X, since the The vector of residuals, y - X  ^).X is equal to zero. This means that y - X  ^ is the shortest dot product (y - X  ^, that is, the variance of the residuals is the minimum of all possible vectors y - X  possible[19]. The Gauss Markov theorem states that in a linear regression model in which the errors have expectation zero and are uncorrelated and have equal variances , the best linear unbiased estimator of the coefficients is given by the ordinary least squares estimator. Here "best" means giving the lowest possible mean squared error of the estimate. The errors need not be normal, nor independent and identically distributed (only uncorrelated and homoscedastic).see [21] Limitations and Alternatives: The independent variable, x, is free of error.

15

In practice, the errors on the measurements of the independent variable are usually much smaller than the errors on the dependent variable and can therefore be ignored. When this is not the case, total least squares also known as errors-invariables models, or rigorous least squares, should be used. This can be done by adjusting the weighting scheme to take into account errors on both the dependent and independent variables and then following the standard procedure. In some cases the (weighted) normal equations matrix is ill-conditioned. When fitting polynomials the normal equations matrix is a Vandermonde matrix. Vandermode matrices become increasingly ill-conditioned as the order of the matrix increases. In these cases, the least squares estimate amplifies the measurement noise and may be grossly inaccurate. Various regularization techniques can be applied in such cases, the most common of which is called ridge regression. If further information about the parameters is known, for example, a range of possible values of  , then various techniques can be used to increase the stability of the solution. Another drawback of the least squares estimator is the fact that the norm of the residuals, ||y - X || is minimized, whereas in some cases one is truly interested in ^||. However, obtaining small error in the parameter  , e.g. , a small value of || -  since  is unknown, this quantity cannot be directly minimized. The least squares method is often applied when no prior is known. Surprisingly, when several parameters are being estimated jointly, better estimators can be constructed, an effect known as Stein's phenomenon. Solving Nonlinear least squares Problem Non-linear least squares is the form of least squares analysis which is used to fit a set of m observations with a model that is non-linear in n unknown parameters (m1 > n1 ). It is used in some forms of non-linear regression. Consider a set of m data points, (x1 , y1 ), (x2 , y2 ), ..., (xm1 , ym1 ), and a curve (model function) y = f (x,  ), that in addition to the variable x also depends on n parameters,  = (1 , 2 , ..., n1 ), with m1  n1 . It is desired to find the vector of parameters such that the curve fits best the given data in the least squares sense, that is, the sum of squares:
m1

S=
i=1

2 ri

is minimized, where the residuals (errors) ri are given by ri = yi - f (xi ,  ) i = 1, 2, ..., m1 16

The minimum value of S occurs when the gradient is zero. Since the model contains n parameters there are n gradient equations: S =2 j ri
i

ri = 0 (j = 1, . . . , n1 ). j

ri In a non-linear system, the derivatives  are functions of both the independent j variable and the parameters, so these gradient equations do not have a closed solution. Instead, initial values must be chosen for the parameters. Then, the parameters are refined iteratively, that is, the values are obtained by successive approximation, k+1 k j  j +  j . = j

Here, k is an iteration number and the vector of increments,  , is known as the shift vector. At each iteration the model is linearized by approximation to a first-order Taylor series expansion about  k xi , k ) k j - j  f (xi ,  k ) + j Jij j . f (xi ,  )  f (xi ,  k ) + j f ( j
xi , ) is the first-order partial derivatives of a function f (xi ,  k ) Where Jij = f ( j with respect to j where i = 1, 2, ..., m1 and j = 1, . . . , n1 and k is an iteration number,so J is a function of the independent variable and the parameters, so it changes from one iteration to the next. Thus, in terms of the linearized model, ri = -Jij and the residuals are given by: j n1
k

ri = yi -
s=1

Jis s ; yi = yi - f (xi ,  k )

Substituting these expressions into the gradient equations, they become
m1 n1

-2
i=1

Jij

yi -
s=1

Jis s

=0

which, on rearrangement, become n simultaneous linear equations, the normal equations
m1 n1 m1

Jij Jis s =
i=1 s=1 i=1

Jij yi

(j = 1, . . . , n1 ).

The normal equations are written in matrix notation as (see [12]): JT J  = JT y. 17

Differences between LLSQ (linear least squares) and NLLSQ (non-linear least squares): (1) The model function, f, in LLSQ (linear least squares) is a linear combination of parameters of the form f = Xi1 1 + Xi2 2 + · · · The model may represent a straight line, a parabola or any other linear combination of functions. In NLLSQ the parameters appear as functions, such as  2 , e x and so forth. If the derivatives f /j are either constant or depend only on the values of the independent variable, the model is linear in the parameters. Otherwise the model is non-linear. (2) Algorithms for finding the solution to a NLLSQ problem require initial values for the parameters, LLSQ does not. Like LLSQ, solution algorithms for NLLSQ often require that the Jacobian be calculated. Analytical expressions for the partial derivatives can be complicated. If analytical expressions are impossible to obtain either the partial derivatives must be calculated by numerical approximation or an estimate must be made of the Jacobian. (3) In NLLSQ non-convergence (failure of the algorithm to find a minimum) is a common phenomenon whereas the LLSQ is globally concave so non-convergence is not an issue. (4) NLLSQ is usually an iterative process. The iterative process has to be terminated when a convergence criterion is satisfied. LLSQ solutions can be computed using direct methods. (5) In LLSQ the solution is unique, but in NLLSQ there may be multiple minima in the sum of squares. Under the condition that the errors are uncorrelated with the predictor variables, LLSQ yields unbiased estimates, but even under that condition NLLSQ estimates are generally biased[21].

18

2
2.1

Cherny's Framework
Problem and General Solution

In Cherny's et al [1], the authors considered the problem of measuring the risk of a hedge fund with scarce observations. The simplest way of doing so is by taking the empirical distribution but with scarce data this could be challenging. A more advanced procedure, proposed by the authors, consist in relating the return of the hedge fund to the values of certain financial factors, like the price of the oil or macroeconomic factors. These factors are observed more frequently so it is assumed that one can estimate the joint law of all factors from the available data. Moreover, if the marginal conditional expectation of the hedge fund with respect to each of the factors separately are known, like in a regression, then the problem become that of recovering the conditional expectation of the hedge fund's return given all financial factors from the joint law and the marginal conditional expectation. For a fixed time period [0, T ], let R denote the return of the hedge fund over this period and let (X1 , X2 , ..., XN ) be the returns of the factors over this period. In general, one useful mathematical problem linking R to (X1 , X2 , ..., XN ) would be to estimate the conditional distribution P (R|X1 , X2 , ..., XN ). If the joint law (probability) of (X1 , X2 , .., XN ) is known, then the above problem is equivalent to estimating the joint law of (R, X1 , X2 , .., XN ). If the hedge fund has a two year history, then there are only two dozen observation of R. This may be sufficient to estimate R and the joint distribution Law(R, Xn ) for n = 1, ..., N but the data would be too scarce to try to estimate Law(R, Xi , Xj ) or the Law of more than three variables simultaneously. A simpler problem would be to recover Law(R, X1 , X2 , .., XN ) from the knowledge of Law(X1 , X2 , .., XN ) and Law(R, Xn ) n = 1, 2, .., N but this is still a challenging problem. A simpler mathematical problem is that of recovering the conditional expectation E (R|X1 = x1 , X2 = x2 , ..., XN = xn ) instead of the Law(R|X1 = x1 , X2 = x2 , ..., XN = xN ). For that, the authors use the Law(X1 , X2 , ..., XN ) and the marginal conditional expectation E (R|Xn = x) instead of the Law(R, Xn ). Here, we present their framework in the most general way. Details are provided next:

19

Let us assume a measure P on RN (this is the law of (X1 , X2 , ..., XN )) and the functions n : R  R [ means E (R|Xn = x)] are given. The problem is to find a function  : RN  R ((x1 , x2 , .., xN ) which has the meaning of E (R|X1 = x1 , X2 = x2 , .., XN = xn )) such that: E ((X1 , X2 , ..., XN )|Xn ) = n (Xn ) n = 1, 2, .., N (1)

Here Xn denotes the n-th coordinate projection of RN on R. In order to obtain a unique solution Cherny [1] imposed additional conditions on the function . They proposed to look for the solution which is the most moderate one as measured by its variance. M inimize var (X1 , X2 , .., XN ) E ((X1 , X2 , ..., XN )|Xn ) = n (Xn ) n = 1, 2, .., N, (2)

where var denotes the variance and the minimization is performed with respect to a family of integrable functions  which is defined according to a given measure P on the factors.

2.2

General Setup

In this section, we will study problem for an arbitrary measure P . Let us set  = {(1 , 2 , ..., N ) : E2 n (Xn ) <  and En (Xn ) = 0 n = 1, ..., n} Let P rE denote the orthogonal projection on a space E and ||.|| the L2 - norm. The following lemma sheds light on the structure of the solutions. In page 11 of [1], the solution of the case when the distribution between of the factors is Gaussian, the application of next Lemma completed the proof. Lemma 2.1 Let (1 , 2 , ..., N )   and suppose that n : 2 able functions with En (Xn ) <  such that the function
N

-

are measur-

(x1 , ..., xN ) =
n=1

n (xn )

(3)

satisfies (1); then it is the unique solution of (2). 20

The following proof is a more detailed version of the proof in the paper. Proof 2.2 Denote En = {  L2 :  is Xn - measurable, E = 0} Note Xn is a random variable define in a probability space ( , F, P), therefore the function  is Xn-measurable iff  = g (Xn) for some g such that the preimage of each measurable set (on the Borel Algebra on R) is in F (so g is a measurable function). ~ satisfies (1) , this means that Let us assume  ~ |Xn ) = n (Xn ) = E (|Xn ) E ( As E (|Xn ) is the projection of  onto En then for any Y  En : < Y,  - E (|Xn ) >= To see this, Y ( - E (|Xn ))dp = E (Y ) - E (Y E (|Xn )) = E (Y ) - E (E (Y |Xn )) (sinceY  En ) = E (Y ) - E (Y ) Therefore ~ = n (Xn ) = P rEn  P rEn  This implies that ~ - P rEn  = P rEn ( ~ - ) = 0 P rEn  n = 1, ..., N ~ - ) is orthogonal to En for n = 1, ..., N So ( ~ - , n >= 0, , n  En < 21 Y ( - E (|Xn ))dP = 0

as   E1 + E2 + ... + EN (which is the space of sums 1 + ... + N where i  Ei ) ~ -  is orthogonal to . so  = 1 + ... + N we can imply  To see this: ~ - ,  >=<  ~ - , N i >= N <  ~ - , i >= 0 < i=1 i=1 ~||  ||||, which is a direct result from the following equations: This implies || ~ =  - ( -  ~)  ~||2 = ||||2 + ||( -  ~)||2 || Hence ~||2  ||||2 || ~ =. This allows us to conclude that the linear and the equality is possible only if  combination minimizes the norm. The differences between non-linear regression and Cherny's et al solution is explained next. According to [1], R is the return of the hedge fund over a fixed period, X1 , X2 , ...XN are the returns of the factors over this period. The problem is to estimate the conditional expectation E (R|X1 , ..., XN ) from the marginal conditional expectations E (R|Xn = x) n = 1, ..., N and the joint probability for (X1 , . . . , XN ). The authors assumed that R, X1 , X2 , ...XN are random variables with mean zero and E (Xn )2 = 1, with R a dependent variable and X1 , X2 , ...XN the independent variables. In general a linear or nonlinear regression of R on each Xn , n = 1, ..., N would lead to: R = i (Xi ) + i i = 1, ..., N where i s are errors. By the method of least square, the best-fit is the curve that has a minimal sum of the deviations squared (least square error) from Xn . So, the best n (Xn ) n = 1, ..., N leads to a minimum value for E (R - n (Xn ))2 22

As regression analysis estimates the conditional expectation of the dependent variable given the independent variables that is, the average value of the dependent variable when the independent variables are held fixed: n (x) = E (R|Xn = x). After finding the best fitted curves, [1] found a E (R|X1 , ..., XN ) which is a function  : N  and (X1 , ..., XN ) = E (R|X1 , ..., XN ) such that E ((X1 , ..., XN )|Xn ) = n (Xn ). The differences between [1] and a linear/nonlinear regression of R on X1 , ..., XN is that the former selects  with the minimun variance among a wide family of possible choices. On the other hand regression minimizes the error E (R - (X1 , X2 , ..., XN ))2 based on a least square method for a specific set of parametric fucntions . In other words, note that in a regression the error and the X variables are assumed independent (uncorrelated), hence from the expression: R = (X1 , ..., XN ) +  we could see that: V ar(R) = V ar((X1 , ..., XN ) + ) = V ar((X1 , ..., XN )) + V ar() Therefore by minimizing the V ar((X1 , ..., XN )) (the target in [1]) the V ar() will be maximized as variance of R is fixed, while in regression  is chosen such that V ar() is minimum. V ar() = E ([R - (X1 , ..., XN )]2 ). Concluding, the objectives are different between regression and [1], the former minimizes error within a narrow set of functions, while the later selects the solution among a wide set of possible fitting functions by requiring to have minimum variance.

23

2.3

Solution for Gaussian and Independent Measures

The solution of (2) for the case when the distribution between the financial factors is Gaussian has the form:
N 

(X1 , X2 , ..., XN ) =
n=1 m=1

nm Hm (Xn ) , xn 

where nm are found through solving certain N-dimensional linear system and Hm (Xn ) are Hermite polynomials as Hermite polynomials have orthogonality relationship under gaussian measure. One way to define Hermite polynomials is as follows: f (a) = exp{ax - a2 } 2

1 m Hm (x) =  |a=0 f (a) x  m! am An an example, H0 (x) = 1 H1 (x) = x H2 (x) = H3 (x) = Denote amn = E [n (xn )Hm (xn )] where the expectation is with respect to the Gaussian Measure with density func-x2 2 dx so tion f (x) = 1 e R 2
-x2 1 n (x)Hm (x)e 2 dx amn =  2 R n = 1, 2, ..., N, m  N

(x2 - 1)  2 (x3 - 3x)  6

24

Denote by C the covariance matrix of (X1 , X2 , ..., XN ) and C m its m-th componentwise power. Each C m is symmetric, positively definite, and non-degenerate. For each m  , the vector     1m a1m  .   -1  .  . =C  .  . . N m aN m is well defined. (the proof is in [1] page 11). Suppose that X1 , X2 , ..., XN are independent under P then the solution of (2) is given by:
N

(X1 , X2 , ..., XN ) =
n=1

n (Xn )

(the proof is in [1] page 10). Independent components (special case of Gaussian). One special case of Gaussian is when X1 , ..., XN are Gaussian and independent. In this case E (XY ) = E (X )E (Y ) = 0 then  = 0 so according to lemma A.1 of [1] Hm (X ), Hk (Y ) = 0 and also 1 if m = k Hm (X ), Hk (X ) = 0 if m = k In page 11 of [1] we have
  N

n (Xn ) =

(
m=1 s=1 k=1 

ks Hs (Xk ), Hm (Xn ) )Hm (Xn )

=

(nm )Hm (Xn )
m=1  m=1 (nm )Hm (Xn )

In the solution of the Gaussian case when we substitute n (Xn ) = then we have
N

(X1 , X2 , ..., XN ) =
n=1

n (Xn )

which is equal to the solution of (4.1) of [1].

25

3

Alternative Mathematical Framework

As seen in the previous section, where a summary of the results in [1] was provided, their method is applied to two simple joint measures, the independent measure and the multivariate Gaussian measure. The authors also explore the case of a Gaussian dependence structure, therefore a Gaussian Copula (see [1]). These examples are too simplistic for their targeted applications, which are financial variables. It is well known that financial instruments like stock prices, company's fundamentals and macroeconomic variables show non-gaussian features like fat tails (Kurtosis) in the marginals as well as tail dependence on the joint. One of the most popular distribution in the world of finance and economics, that allows for fat tails and tail dependence is the t-student and their multidimensional counterparts. This is why our main objective is to explore the applicability of the work developed by the previous authors beyond normality and in particular for a t-student case. The mathematical setting used in [1] has several limitations which become clear once the method is applied to non gaussian measures like that of the t-student. For example, even though the authors show existence of the solution, they do not provide a methodology to build this solution given a probability measure, some hints can be extracted from their applications and this is one of the new developments in our thesis. Moreover, their main results strongly use the space of L2 functions under the given probability measure, while the applications described [1] make use of a basis under this measure. The theory and applications come together very conveniently under a Gaussian measure (PG ) as the well known Hermitian polynomials are not only orthogonal with respect to PG but also they represent a basis of the space of L2 (PG ) functions. This is unfortunately too much to ask when a different probability measure is selected leading to two further challenges, first the need to search of a set of orthogonal functions under the given measure P and secondly the shrinkage of the space L2 (P ) to a subspace in which the orthogonal functions become a basis. The latest have the strongest implications as it cut short the space of functions used for matching the marginal functions (conditional expectations) obtained from a regression between the dependent variable y and each of the independent variables X. In this section we first explore changes on the setting developed in [1] in order to accommodate for non-Gaussian measures. In a second step a methodology to build a solution under a measure P is provided. This is based on the knowledge of 26

a basis of orthogonal functions, and their corresponding space , under the fixed measure as well as a set of marginal functions, which are assumed elements of this space . Then a solution for the case when the distribution between the factors is a multivariate t-student is developed. To accomplish that we describe a class of hypergeometric orthogonal polynomial which has the orthogonality relationship under the t-student measure. The space set up to solve the problem is therefore different from the Gaussian case. In our approach the parameters of the chosen measure plays a role on the space under analysis and therefore on the set of suitable marginal functions. This is not the case for the Gaussian measure as the space is always that generated by the Hermitian polynomials regardless of the correlations, covariances of the underlying variables. This is why we also provide details of the problem for different degrees freedom starting from 4, 5 and 6 to conclude with a general solution.

3.1

Modified Setup

In this section we set up a new space which is a variation of that of ([1]) and suitable for our purpose in Theorem 3.4. The new space is a subspace of L2 (p) such that we can obtain a set of orthogonal functions {pm (X ) }Z m=1 under a given measure P (i.e. pn (x)pk (x)dP (x) = 0 iff n = k ) and therefore a basis for this subspace. Let n be the space generated by, a possible infinite, set of orthogonal polynomials under a unidimensional probability measure P on variable Xn (span{(pm (Xn ))Z m=1 }). Let us define  = {(1 , ..., N ), n  n , En (xn ) = 0 n = 1, ..., N } . Note n  L2 (P ) so elements in n are L2 integrable under the probability measure P . And finally the space of multivariate functions suitable to our purposes: =H L2 (P (N ) ), where H =
N

n (xn ) | (1 , ..., N )   . In this new setn=1

ting, the space  represents all functions L2 integrable under a given N -dimensional measure P (N ) that can be written as a product of elements in n . Remark 3.1 For clarity, we can think of the case where P (N ) is a N -dimensional gaussian measure then  plays the role of all L2 (P (N ) ) functions. This is due to the fact that the product of the spaces n generated by the Hermitian polynomials 27

(denoted space H ) form a basis of all multivariate polynomials and therefore of all L2 (P (N ) ) functions (H = L2 (P (N ) )). Note also H plays an important role in our setting as it is basically the space of functions from which the optimal solution to problem (2) will be obtained. The richer this space this space the stronger and more general the result would be. Next we extend Lemma 2.1 to show uniqueness of the solution to problem (2) on this modified space . Recall that we use P rE to denote the orthogonal projection on a space E and . the L2 (P (N ) ) norm. ^ and  satisfy (1) and are elements in  with Lemma 3.2 Assume 
N

=
n=1

n (xn )

^ . then    Proof 3.3 The proof follows similarly to that of Lemma 2.1 but using a different space: Denote En = {  n L2 (P ) :  is Xn - measurable, E = 0}

~ satisfies (1) , this means: Let us assume  ~ |Xn ) = n (Xn ) = E (|Xn ) E ( or alternatively ~ = n = P rEn  P rEn  which implies ~ - P rEn  = P rEn ( ~ - ) = 0 P rEn  n = 1, ..., N ~ - ) is orthogonal to En for n = 1, ..., N and all n  En : Hence ( ~ - , n = 0 

28

As   E1 + E2 + ... + EN (which is the space of sums 1 + ... + N where i  Ei ) ~ -  is orthogonal to  using the linearity so  = 1 + ... + N so we can imply  of expectation (scalar product): ~ - ,  =  ~ - , N i = N  ~ - , i = 0  i=1 i=1 ~||  ||||, which follows easily from the Pythagorean theorem: This implies || ~|| = |||| + ||( -  ~)|| || ~ =. and the equality is possible only if  Now we present the main theorem of this section which provides a methodology to build the minimum variance solution (problem (2)) given the following inputs: · A marginal probability measure P and a set of orthogonal functions under this measure: {pm (X ) }Z m=1 . · A multivariate probability measure P (N ) with marginals P . · A set of marginal conditional expectations (1): {n (Xn )}N n=1 . These are obtained by, for example, marginal regressions (linear or nonlinear) between Y and each of the variables Xn . The functions n (Xn ) must be in the space n = span{(pm (Xn ))Z m=1 }.

Theorem 3.4 Let P (N ) be a probability measure in N with equal marginal P . Let pm (X ) , m = 1, .., Z (may be infinite) be a set of orthogonal polynomial under the probability measure P . A set of functions n (xn ), n = 1, .., N which are elements Z of n the vector space spanned by {pm (X ) }Z m=1 with coefficients {anm }m=1 . Then the solution of (2) is given by the series:
N Z

(X1 , X2 , ..., XN ) =
n=1 m=1

nm pm (Xn ) , Xn 

whenever the series converges in L2 (P (N ) ). Here nm are scalars solutions of:
N Z sm ks Ckn n=1 m=1

anm = 29

(4)

sm and Ckn is defined in terms of a scalar product: sm Ckn = ps (Xk ), pm (Xn )

where n, k = 1, ..., N and m, s = 1, ..., Z Proof 3.5 Thanks to our previous lemma, we only need to show that  satisfies condition (1): E ((x1 , x2 , ..., xN | xn ) = n (xn ) n = 1, ..., N consider the spaces En which is defined in proof 3.3. Using the fact that each n (xn ) belongs n :
Z

E [|xn ] = P rEn  =
m=1

, pm (xn ) pm (xn )

We next substitute our guess solution:
Z Z N

E [|xn ] =
m=1 s=1 k=1 Z Z N

ks ps (xk ), pm (xn ) pm (xn )

=
Z

(
m=1 s=1 k=1 N,Z

ks ps (xk ), pm (xn ) pm (xn ))
Z sm ks Ckn )pm (xn )

=

(
m=1 k,s=1

=
m=1

anm pm (xn )

= n (xn ) , n = 1, ..., N An application of Lemma (2.1) completes the proof. Remark 3.6 Note the meaning of the coefficients anm in our framework: anm = n , pm (xn ) therefore
Z

n (xn ) =
m=1

anm pm (xn )

In order to gain better intuition about the solution and how it connects to the one found in [1] we show similarities between both approaches in the next section. 30

3.1.1

Gaussian and Independent Measures in our Framework

If we assume that P is a Gaussian distribution in Theorem 3.4 then pm (X ) becomes the Hermitian polynomials Hm (X ), because the Hermite polynomials have an orthogonality relationship under a Gaussian measure and they are L2 integrable. Hn (x) nth-degree polynomials for n = 0, 1, 2, 3, .... These polynomials are orthogonal with respect to the weight function (measure) w ( x) = e - x i.e., we have

2 /2

Hm (x)Hn (x) w(x) dx = 0
-

when m is not equal to n. Furthermore,


Hm (x)Hn (x) w(x) dx =
-



2 n!nm

Therefore we have that Z =  and the solution of (2) is given by the L2 - convergent series
N,

(X1 , ..., XN ) =
n,m=1

nm Hm (Xn )

where anm =

N m km Ckn k=1

m and Ckn is defined, as before, in terms of a scalar product under the multivariate Gaussian measure: m Ckn = Hm (Xk ), Hm (Xn )

In this case Hm (Xk ), Hk (Xn ) = 0 from Lemma A.1 from [1] simplifies the solution significantly.

31

Note that if the multivariate measure is an independent measure and therefore is the product of univariate probability distributions we will have: ps (xk ), pm (xn ) = 0 if {n = k }  {m = s} which leads to the following simplification:
N,Z

anm =
k,s=1

sm ks Ckn

=

n (xn ), pm (xn )

meaning the solution (X1 , X2 , ..., XN ) is:
N

(X1 , X2 , ..., XN ) =
n1

n (Xn ),

which reproduces the findings in [1]. The advantage of reaching this same conclusion using Theorem 3.4 is that it becomes clear what type of functions would be best to use as n (Xn ) candidates (those in n ). If a set of orthogonal functions under P is known then we could build n (Xn ) using elements in n in the regression analysis. On the other hand functions on the larger set L2 (P ) may be difficult to identify requiring trial and error on the type of functions n (Xn ) to be used for the marginal regressions.

3.2

Results Under a t-Student Measure

This section focuses on an application of Theorem 3.4 that goes beyond the Gaussian probability measure and its multivariate generalization. We explore the case of the univariate t-student and one of the few bivariate t-student generalizations from [7], that allows for univariate t-student marginals. In general multivariate distributions may not give marginals from the same family, this is unfortunately the case of the t-student, and even though there are many proposed multivariate generalizations (see [9]) few of them have the standard univariate t-student as their marginal counterpart. Some well known properties about the moments of the t-student are provided next. After that, a set of orthogonal polynomials under this measure is presented 32

together with the particularities about the orthogonality relationship and some implications from them. Suppose that P is a t-student non degenerate distribution. We will assume that E (Xn ) = 0 and v stands for the parameters, the degree of freedom with v > 3. The student's t-distribution has the probability density function f (x) = ( ) ( v+1 2 v ( v ) 2 )(1 + x2 - 1 (1+v) ) 2 v

The moments of the t-distribution are given as follow:  0 if k odd ,0 < k < v     1 [( k+1 )( v-k )v k/2 if k even ,0 < k < v 2 2  (v/2) E ( xk ) =  undef ined if k odd ,0 < v  k    if k even ,0 < v  k. 3.2.1 Orthogonal Polynomials and Properties

Orthogonal functions under a given probability measure is a key component of Theorem 3.4. Here we present a set of orthogonal polynomials under the univariate t-student distribution (see [14]). Consider the third finite class of classical hyperp (x), n = 0, 1, 2, ... One way to define them is geometric orthogonal polynomials In as follows: [n ] 2 p-1 n-k p (-1)k In (x) = n! (2x)n-2k n-k k
k=0

which is equivalent to:
p In (x) =

(-2)n (p - n)n dn ((1 + x2 )n-(p-1/2) ) (1 + x2 )p-1/2 (2p - 2n - 1)n dxn (a)n = a(a + 1)(a + 2)...(a + n - 1)

n = 0, 1, 2, ...

where

A third way of defining them is as follows:
p In (x)

1 m  = |a=0 (1 + 2ax - a2 )p-1 m m! a 33

Each definition provides a useful tool when manipulating these polynomials. The expression for the first 4 polynomials are:

p I0 (x) p I1 (x) p (x) I2 p I3 (x)

= = = =

1 2(p - 1)x 4(p - 2)(p - 1)x2 - 2(p - 1) 8(p - 3)(p - 2)(p - 1)x3 - 12(p - 2)(p - 1)x

The subindex means the degree of the polynomial and the supraindex is the p parameter of the measure in which they are orthogonal: In (x) is nth-degree polynomial for n = 0, 1, ... with parameter p. These polynomials are orthogonal with respect to the weight function (x, p) = (1 + x2 )-(p-1/2) . In general we have the following orthogonality relationship:   n!2(2n-1)  2 (p)(2p - 2n) p p (x, p)In (x)Im (x)dx = ( )n ,m (p - n - 1)(p - n)(p - n + 1/2)(2p - n - 1) - This equation holds if and only if m, n = 0, 1, 2, ..., N < p - 1 and N = max{m, n}:   0 if m = n   p p n!2(2n-1)  2 (p)(2p-2n) (x, p)In (x)Im (x)dx =  ( (p-n-1)(p-n)(p-n+1/2)(2p-n-1) if m = n
-

In particular when m = n: ( (p - n - 1)(p - n)(p - n + 1/2)(2p - n - 1)  ) n!2(2n-1)  2 (p)(2p - 2n)
 p (x, p)(In (x))2 dx = 1 -

which means the functions are not orthonormal but just orthogonal. The weight function of the orthogonality relation above is related to the univariate t-student distribution as follows:

34

(v +1)/2 x T ( x; v ) = (  )(  ; v/2 + 1)) v (v/2) v

-  < x < .

Therefore the orthogonality relationship in terms of a t-student with v degrees of freedom becomes:
 v/2+1 v/2+1 T (x; v )In (x)Im (x)dx -

)n!2(2n-1) 2 (v/2 + 1)(v + 2 - 2n) ( v+1 2 )n ,m ( v )(v/2 - n)(v/2 + 1 - n)(v/2 - n + 3/2)(v - n + 1) 2 = A(v, n)n ,m = ( if and only if m, n = 0, 1, 2, ..., Z < p - 1 therefore using the relation p = v/2 + 1 we have the following constraints for the number of orthogonal polynomials under this measure (Z ) v -1 v N 2 2 Z= v ]  /N [v 2 2 We could orthonormalize these polynomials by simply dividing by the function A(v, n). The first four orthonormalized polynomials would look as follow: I0 (x) = ( (v/2)(v/2 + 1)(v/2 + 3/2)(v + 1)(v/2) ).1 2!2(-1) ((v + 1)/2)2 (v/2 + 1)(v + 2)

I1 (x) =

(

(v/2 - 1)(v/2)(v/2 + 1/2)(v/2) x ).(v ) 2 2!((v + 1)/2) (v/2 + 1) (v )

I2 (x) =

(

(v/2 - 2)(v/2 - 1)(v/2 - 1/2)(v - 1)(v/2) .[(v - 2)x2 - v ] 2!2(3) ((v + 1)/2)2 (v/2 + 1)(v - 2)

I3 (x) =

(

(v/2 - 3)(v/2 - 2)(v/2 - 3/2)((v - 2)(v/2) x3 x   )[( v - 4)( v - 2) - 3( v - 2)( v ) ] 3!2(5) ((v + 1)/2)2 (v/2 + 1)(v - 4) v v

An important difference with the Gaussian case described before is the finiteness of the number of orthogonal polynomials under the t-student probability measure for a given degree of freedom v , this results from the upper limit for Z . 35

Remark 3.7 For example, if v = 4 then Z = 1 so there are only two orthogonal polynomials under this t-student. Curiously if v = 5 or 6, then Z = 2 so we will have three orthogonal polynomials or those two cases (same number per case). On the other hand if the degrees of freedom increases to infinity then the number of polynomials also increases and not only the t-student becomes a Gaussian distribution but also the hypergeometric orthogonal polynomials converges to the Hermitian polynomials (see [14]). 3.2.2 Multivariate t-Student

As mentioned before, most multivariate t-students are not built to imply univariate t-students on their marginals, this is why we are constrained to a very specific family of bivariate t-students which does have this condition, see [7]. Even though the chosen family has not been extended beyond two dimensions, we can still use it within the context of Theorem 3.4. The reason for this is that in order to apply Theorem 3.4, we only need to compute bivariate moments coming from the scalar ms products in matrix Ckn ( pm (Xk ), ps (Xn ) ) and a bivariate distribution is all the requirement to do so. Still a multivariate distribution is needed for lemma 4.1 to ensure existence and uniqueness of the solution, so we make use of a theory still in its origins which blend univariate, bivariate and lower order dimensional distributions into a multivariate distribution of any dimension. This theory is that of Frechet classes ([8]). Frechet classes or classes of multivariate distributions with some given marginals is one of the concepts that allows to combine the chosen bivariate into a N dimensional multivariate distribution. The most popular Frechet classes are the classes of copulas. Copulas combine univariate distributions into a feasible multivariate probability measure. Another Frechet class is the class of multivariate distribution in which trivariate marginals are given. The idea is extended to any lower dimensional distribution. In our case we assume there is a multivariate tdistribution that can be obtained, using Frechet classes and properties, from a set of bivariate distribution.[8]. In other words, for the variables (X1 , ...., XN ) and for a given set of bivariate distributions {Pij (Xi , Xj )}N i,j =1 , in this case the bivariate t-student [7], we assume a multivariate probability measure P (X1 , ...., XN ) that can be built satisfying the given bivariate marginals. Conditions for this multivariate to exist are under study in the mathematical literature as it remains an open problem. The only fully solve case is when the marginals are univariate (the case of Copulas mentioned before). 36

Remark 3.8 It should be pointed out that there are sufficient conditions available for the existence of N-dimensional probability distributions from a given set of 2dimensional marginals. This, in a worst case scenario, would only mean additional constraints on the parametric space for the selected bivariate distributions. The next remark is about the implications that result from using only bivariate to the space of functions H from which the optimal function solving problem (2) is obtained. Remark 3.9 Note that in a worst case scenario, where the implied multivariate distribution has few L2 (P (N ) ) integrable elements we would still have functions of the type {pm (Xk )ps (Xn )}Z m,s=1 for all n, k = 1, .., N due to the existence of the joint moments from our chosen bivariate marginal to be described later. The bivariate t-distribution we selected, was derived in [7] as a sampling distribution from the bivariate normal distribution and chi-square distribution. The joint probability density function is given by the following expression:
2 X 2 + X2 - 2X1 X2 -(v+2)/2 (1 - 2 ) (1 + 1 ) 2 (1 - 2 )v where v represents the degrees of freedom in the one dimensional marginals and  is the dependence parameter with happens to coincide with the Pearson correlation between the variables. Therefore, in principle we assume that the joint probability measure P (X1 , ...., XN ) is such that every bivariate (Xi , Xj ) with i, j = 1, ..., N follows a two-dimensional t-student with parameters (v, ij ). The joint moments of this bivariate t-distribution are given in general by a recursive system as:

f (X1 , X2 ; v, ) =

µ(a, b; v ) = (a + b - 1)µ(a - 1, b - 1)2 +(a - 1)(b - 1)(1 - 2 )µ(a - 2, b - 2)4 if v > 4 For even and odd joint moments the expression can be simplified to:  min(a,b) )!(2b)! (2)2j   µ(2a, 2b; v ) = (2a a+b  j =0 ( a - j )!( b-j )!(2j )! 2a+2b 2         (2)2j  µ(2a + 1, 2b + 1; v ) = (2a+1)!(2b+1)!  min(a,b)  a + b j =0 (a-j )!(b-j )!(2j +1)! 2a+2b+2 2       µ(2a, 2b + 1; v ) = µ(2a + 1, 2b; v ) = 0,      37 if v > 2a + 2b if v > 2a + 2b + 2

b where µ(a, b; v ) = E (xa 1 x2 ), and 1 = 2 = 1 and a is the a-th moment of new distribution defined by the authors as "T" (see [7], Theorem in page 7).

T is a random variable with an inverted chi-square distribution given by Theorem 1.1 in [7]. The a-th moment of T is given by:  a/2 (v/2-a/2)  a = E (T a ) = (v/2) ( for v > a  v/2)          -2a = E (T -2a ) = (v/2)-a (v/2)(v/2 + 1) · · · (v/2 + a - 1) if v > 2a       2a = E (T 2a ) =     
(v/2)a (v/2-1)(v/2-2)···(v/2-a)

for v > 2a

v Note 2 = v- and therefore v > 2 (see [7] Corollary 1.1, page 2). 2 In order to provide a bit more of insight regarding this bivariate t-distribution we will show the connection to the bivariate Gaussian distribution. Recall the raw product moments of the bivariate normal distribution with pdf:

-q (x1 , x2 ) (1 - )-1/2 exp( ) 21 2 2 x1 - µ1 x2 - µ2 2(x1 - µ1 )(x2 - µ2 ) (1 - 2 )q (x1 , x2 ) = ( + - ) 1 2 1 2 f (x1 , x2 ) = are given by
a b µ(a, b) = 1 2 (a, b)

where in general there is a recursive expression for (a, b):
2 2 (a, b) = (a + b - 1)(a - 1, b - 1) + (a - 1, b - 1)(1 - 2 )1 2 µ(a - 2, b - 2)

38

which is solvable for odd and even joint moments as:  min(a,b) (2)2j )!(2b)!  (2a, 2b) = (2a  j =0 (a-j )!(b-j )!(2j )! 2a+b (a,b) (2)2j b+1)!  min (2a + 1, 2b + 1) = (2a+1)!(2 j =0 (a-j )!(b-j )!(2j +1)! 2a+b   (2a, 2b + 1) = (2a + 1, 2b) = 0 (see [7] page 2, Theorem 3.1) These equations help us realize that when v  , the pdf of the bivariate tdistribution converges to the one of a bivariate normal distribution. This is a plus for the chosen bivariate t-student selected as it is a natural extension not only of a univariate gaussian but also of its bivariate counterpart. Note that in setting the stage for the application of Theorem 3.4 to the t-student case, we also need the scalar product of the orthogonal functions under a bivariate ms t-student probability measure, this is the matrix Ckn = pm (Xk ), ps (Xn ) . The next result gives the values for this matrix: Lemma 3.10 The inner product with respect to the joint t-student of the orthogonal polynomials is:
ms Ckn = p p Is (xk ), Im ( xn ) [m/2] [s/2] -1 s-b p-1 m-a (-1)a+b 2s+m-2a-2b µ(s - 2b, m - 2a; v )(p ) s-b )(b )(m-a )(a a=0 b=0

= m!s!

Where µ(a, b; v ) was previously defined. Proof 3.11 The proof follows easily from the joint moments under the bivariate t-student distribution. At this point we have all the inputs needed to use Theorem 3.4 and therefore to present the solution for the t-student case. Still due to the richness of cases implied by the dependence of the solution to the degrees of freedom we will then developed three particular cases as a first step in the next section.

39

3.2.3

Solution for a t-Student. Three Particular Cases

We started with 4 degrees of freedom which is the most nongaussian case of all because when v <= 3 then Z = 0 and there is no basis for the space so v => 4 and when v  , the pdf of the bivariate t-distribution converges to the one of a bivariate normal distribution. We provide the results for the case of 5 and 6 degrees of freedom. The reason behind developing both the v = 5 and the v = 6 is to show that even though the numbers of polynomials are the same we still obtain different solutions to our main problem (2) from Theorem 3.4. Solution for v=4 Let us assume the N variables (X1 , .., XN ) follow a multivariate t-student distribution such that every pair (Xi , Xj ) follows a bivariate t-student as defined previously with parameters (v, ij ). = 2 so Z = v/2 - 1 = 1. The When the degree of freedom is 4 we have v 2 condition of zero expected value for the input functions n (X ) implies that we do not need or use the orthogonal function I0 = 1. Therefore I1 is the only orthogonal basis in the space En , the vector solution from Theorem 3.4 would then be:     11 a11  .   -1  .  . =C  .  . . N 1 where an1 = E (n (Xn )I1 (Xn )) = n (X )I1 (X )dP (X ) n = 1, 2, ..., N. aN 1

and P (X ) is the univariate t-student measure. The degree of n must be 1 because n  span{I1 } and I1 is a polynomial of degree one. The matrix C is as follows: C (i, j ) = which corresponds to C = a.R 40 I1 (xi ), I1 (xj )



   R =    1N 2N

1 12 . . .

21 1

 ... N 1 ... N 2      ... 1 

a is a constant and R is the correlation matrix. The determinant of the correlation matrix will equal 1 only if all correlations equal 0, otherwise the determinant will be less than 1. It is not zero either since the factors are different Xi = Xj when i, j = 1, ..., N (non degenerated distribution) so C is invertible therefore the vector  is well defined. The solution of (2) is given by
N

(X1 , X2 , ..., XN ) =
n=1

n1 I1 (Xn ) , Xn 

Solution for v=5 We uses the same setting as in the case v = 4. Here when = 2.5 so Z = [2.5] = 2 therefore I1 and I2 are degree of freedom is 5 then we have v 2 the orthogonal elements in the space En . Using Theorem 3.4, the vector solution would be:     11 a11  .   .  . .   .   .     a  N 1   N1    = C -1    12   a12   .   .   .   .  . . N 2 aN 2 where anm = E (n (Xn )Im (Xn )) = n (X )Im (X )f (X )dX

n = 1, 2, ..., N m = 1, 2

41

and f (X ) is the probability density function of the t-student: f (x) = ( ( v+1 ) 2 ) v ( v 2 )(1 + x2 - 1 (1+v) ) 2 v

The degree of polynomial n can be 1 or 2 because n  span{I1 , I2 }, I1 is a polynomial of degree one and I2 is a polynomial of degree two. From the orthogonality relationship of hypergeometric polynomials we obtained the following form for C :
mn = Im (xj ), In (xi ) = Cji

0 Im (xj ), Im (xi )

if m = n if m = n

which can be simplified further and C becomes a  11 11 0 C11 . CN 1  .  11 11  C1N . CN 0 N C= 22  0 0 0 C11   . 22 0 0 0 C1 N Therefore for each m = 1, 2 we can write:    1m  .  -1   .  = Bm  . N m where
mn Bm (i, j ) = Cji =

block diagonal matrix:  . 0   . 0   22  . CN 1   22 . CN N

 a1m  . .  . aN m

Im (xi ), Im (xi )

In the case of m = 1, we have: B1 = b.R where b is a constant and R is the correlation matrix hence B1 is invertible. For

42

m = 2:    B2 =   22 7 + 152 12 . . . 7 + 152 21 22  ... 7 + 152 N1  ... 7 + 152 N2    ... 22

2 7 + 152 1N 7 + 152N

Proposition 3.12 The determinant of the matrix B2 is nonzero if the matrix  =
N

(ij ) is nondegenerated and
j =1,=i

ij < 1 for all i.

2 Proof 3.13 Note the determinant of the matrix is B2 zero if 2 21 = ... = N N -1 = 1 which is not possible (the factors are not equal) but this is only a sufficient condition. In general B2 can be written as B2 = aO + b where a and b are scalars (7 and 15), O is a matrix of ones and  is a nondegenerated covariance matrix, this last statement comes from using  = (ij ) is nondegenerated and realizing that a matrix with square correlations can be obtained using factor analysis (see [2]) as follows (for simplicity we show only in dimension 3):  2  W1 = 12 M1 + 13 M2 + 1 - 2 12 - 13 Z1 2 W = 12 M1 + 23 M3 + 1 - 2 12 - 23 Z2  2 2 W3 = 13 M2 + 23 M3 + 1 - 2 13 - 23 Z3

where M and Z are uncorrelated therefore the resulting covariance matrix is well defined (the argument in the square roots are positive) and nondegenerated. Moreover B2 can be interpreted as representing the covariance matrix of a vector    Z  in a separate factor model ZN x1 = aM1x1 + bWN x1 , where M and W are independent with unit variance and W has covariance matrix , factors models lead to nondegenerated matrixes (see [2]). Therefore B2 is invertible. The solution from Theorem 3.4 and v = 5 would be:
N,2

(X1 , X2 , ..., XN ) =
n,m=1

nm Im (Xn ) , Xn 

43

Solution for v=6 Using the same setting as before, the solution for v = 6 is similar to the case of v = 5. When the degree of freedom is 6 then we have v =3 2 so Z = 2 (same as for v = 5). The vectors I1 and I2 are the only elements in the orthogonal basis of En and the vector solution would be:     11 a11  .   .  . .  .   .      a  N 1   N1  -1  =C    12   a12   .   .   .   .  . . N 2 aN 2

The degree of polynomial n is again either 1 or 2 and C has a similar form as in the case v = 5:
mn Cji = Im (xj ), In (xi ) =

0 Im (xj ), Im (xi )

if m = n if m = n

which can be written as a block diagonal. Therefore for each m = 1, 2 we can state:    1m  .  -1   .  = Bm  . N m where  a1m  . .  . aN m

Bm (i, j ) =

Im (xi ), Im (xj )

In particular, for m = 1 we have the same expression as for v = 5: B1 = b.R 44

so B1 is invertible. But for m = 2 the solution differs case:  204 60 + 1442 ... 21  60 + 1442 204 ... 12   . B2 =  .  . 2  60 + 1442 1N 60 + 1442N ...

from that found in the v = 5  60 + 1442 N1  60 + 1442 N2      204

The reason for this is that scalar products depend on the probability measure, under a measure with 6 degrees of freedom are different that under a measure with 5 degrees of freedom. B2 is still invertible using similar arguments as in the case v = 5. The solution of (2) using Theorem 3.4 for v = 6 is
N,2

(X1 , X2 , ..., XN ) =
n,m=1

nm Im (Xn ) , Xn 

Once again note that even though the solutions look similar for v = 5 and v = 6 they do not give exactly the same optimal solutions. 3.2.4 Solution for a t-Student. General Case

In this section we target the solution for any degrees of freedom of the multivariate t-student introduced before. The key condition in order to reach a solution based on mn ( Im (xj ), In (xi ) ). If this is proved Theorem 3.4 is invertibility of the matrix Cji then the values  can be obtained from a given set of a's. Let us write C as a column matrix C = (a1 a2 ...aQ ) therefore ai is defined as: ai = E (yi y1 ) ... E (yi yQ )

Here E (yi yj ) = yi yj f (xl , xm )dxl dxm where f (xl , xm ) is the probability density function of the 2-dimensional t-student distribution of xl and xm , l, m = 1, .., N . In our case the vector y is related to the Z orthogonal polynomials and N variables as follows (Q = Z · N ) y = (y1 , y2 , ..., yQ ) = (I1 (X1 ), ..., I1 (XN ), I2 (X1 ), ..., I2 (XN ), ..., Iz (X1 ), ..., Iz (XN )) 45

In the next theorem we show that the invertibility of C can be implied from the linear independence of the y . Theorem 3.14 The determinant of matrix C is zero iff one of the y is a linear combination of the other y s. Proof 3.15 Without losing generality we will show the proof for the case yQ = -1 Q i=1 bi yi . () From -1 yQ = Q i=1 bi yi we can says that:
-1 EpN [y.yQ ] = Q i=1 bi EpN [y.yi ]

or
-1 E (yQ yj ) = Q i=1 bi E (yi yj ) f or j = 1, ..., Q

Hence,      E (yQ y1 ) E (y1 y1 ) E (y2 y1 )  .  = b1   + b2   + ... . . E (yQ yQ ) E (y1 yQ ) E (y2 yQ )
-1 Therefore aQ = Q i=1 bi ai since one column of matrix C is a linear combination of the other columns of matrix C the determinant of C is zero. ()



detC = 0 It means that one of the column of matrix C is a linear combination of other columns of matrix C . Therefore
-1 an = Q i=1 bi ai with bi = 0 f or i = 1, ..., Q - 1

Hence
-1 E (yk yQ ) = Q i=1 bi E (yk yi ) f or k = 1, ..., Q

This implies:
-1 Q-1 Q i=1 bi E (yk yi ) - E (yk yQ ) = E (yk (i=1 bi yi - yQ ))) = 0 f or k = 1, ..., Q -1 Q Now let z = (Q i=1 bi yi ) - yQ = i=1 bi yi when bQ = -1 then -1 E (yk (Q i=1 bi yi - yQ ))) = E (yk z ) = 0 f or k = 1, ..., Q

46

0= Hence

...

yk zf (x1 , ..., xN )dx1 ...dxN , k = 1, ..., Q z 2 f (x1 , ..., xN )dx1 ...dxN = 0 ||z ||2 = 0

... As a result which implies:

-1 yQ = Q i=1 bi yi .

The next remark shows that the previous result holds for the matrix C in our framework. Remark 3.16 As the hypergeometric polynomials are linearly independent then none of the polynomials for a fixed variable xj can be written as a linear combination of the remaining polynomials, for simplicity:
Z -1

IZ (xj ) =
i=1

ci Ii (xj )

with j = 1, .., N . This is obvious due to the degree of the polynomials. Note moreover that the variables themselves are linearly independent from each other -1 for any power (for simplicity Ii (xN ) = N j =1 bj Ii (xj ) for any i), this comes from assuming a non singular covariance matrix for the variables x and therefore a nondegenerated multivariate probability measure for the vector (x1 , ..., xN ). Then we can conclude that C is invertible. The application of Theorem 3.4 to the case of the t-student is presented in the next corollary. Corollary 3.16.1 The solution of (2) for the case when the distribution between the x (financial factors) is the joint t-student and (Im (Xn )Z m=1 forms an orthogonal basis in n then the solution has the form:
N,Z

(X1 , X2 , ..., XN ) =
n,m=1

nm Im (Xn ) , Xn 

47

where nm is the solution of
N,Z

anm =
k,s=1 v 2

sm ks Ckn

Z=

-1 ] [v 2

v 2 v 2

N  /N

anm = E (n (Xn )Im (Xn )) =

n (X )Im (X )f (X )dX

n = 1, 2, ..., N m = 1, 2, ..., Z and n  span{I1 , I2 , ..., IZ }. Some details about the solution are square ZN matrix) as follows:  B11  B21  C=  B31  . BZ 1 where each Bij is a N × N matrix such  ij C11  C ij 12 Bij =   . ij C1 N therefore Bij (m, n) = Ii (xn ), Ij (xm ) , m, n = 1, 2, ..., N provided next: Let us rewrite matrix C (a B12 . B1Z B22 . B2Z B32 . B3Z BZ 2 . BZZ that:
ij C21 ij C22 ij  . CN 1 ij  . CN 2  

     

ij ij C2 N . CN N

Note that Ii (xn ), Ij (xm ) = 0 if i + j is odd (µ(1, 0; v ) = µ(1, 2; v ) = ... = µ(i, j ; v ) = 0 when i + j = odd) so matrix C becomes:   B11 0 B13 0 .  0 B22 0 B24 .   C=  B31 0 B33 0 .  . 48

C is invertible therefore the vector  11  .   1Z   2Z   . N Z so the solution of (2) for any v is:

solution of  would then be defined as:    a11  .         a 1 Z  = C -1    a2Z       .   aN Z

N,Z

(X1 , X2 , ..., XN ) =
n,m=1

nm Im (Xn ) , Xn  .

3.3

Other Probability Measures

The analysis presented in this chapter could be extended to other distributions and orthogonal functions. Some examples are given below: Laguerre Polynomials (Ln ) under an exponential probability measure (P ). Here Ln is the solution of a second-order linear differential equation: xLn (x) + (1 - x)Ln (x) + nLn (x) = 0 P (x) = e-x There are many multivariate generalizations of the exponential distribution, see [9] so there is plenty of room for exploring our setting under this measure. Associated Laguerre Polynomials (LAn ) under a Gamma probability measure (P ). Here LAn is also the solution of a second-order linear differential equation: xLAn (x) + ( + 1 - x)LAn (x) + nLAn (x) = 0 P (x) = x e-x There are only few multivariate variants of the Gamma distribution (see [9]) which could be worth exploring under our setting. These last two cases are defined in (0, ) so they apply better to positive relationships between financial variables as those observed between stock prices and index values. 49

4

Conclusions

The work in this thesis tackles an important problem in financial mathematics, which is that of explaining a given variable, like a stock price, using a vector of other financial/economical variables like Fundamentals, Indexes or Factors under few observations. Finding relationships between a single dependent variable and a set of independent variables is a standard problem of Regression analysis. It is well known that Regression leads to reliable results in the presence of medium to large sample sizes. Unfortunately there are several cases in applications were the data available is only of few dozen observations and the number of variables of interest is almost 50% of the number of observations, this is the case of Hedgefund data. A recently published paper, [1], tried to overcome this lack of data and the shortfalls of regression by providing an alternative method that uses the joint distribution of the independent variables (P (N ) ) on a kind of non-parametric construction of the best fit function between the dependent and indepedendent variables. This optimal fit function is obtained as the one with minimum overall variance among all L2 integrable N -dimensional functions under the given multivariate probability measure P (N ) . His work is applied to the case of an independent measure and the multivariate Gaussian measure. Our work focuses on the shortfalls of [1] and provides two main outcomes: it first makes the results in [1] more flexible by slightly modifying the type of functions on which the optimality is found. This modification allows for an explicit construction of the optimal solution based on a given univariate probability measure, a set of orthogonal functions under this measure and a multivariate probability measure that has given univariate as marginals. The second component of our thesis is the application of the first result on a specific measure, the t-student distribution. This distribution has fat tails and tail dependence, therefore it represents a more realistic probability measure for economic/financial factors. These two contributions could be used mainly in the context of risk management (as [1]), in particular it should approximate better the true relationship between returns and economical factors and therefore allow for building hedge portfolios closer to the targeted assets. The alternative approach of nonlinear regression should lead to unreliable estimators due to small sample sizes making our suggestion more appealing to practitioners.

50

5

References

References
[1] Cherney, Alexander, Raphael Douady, and Stanislav Molchanov. "Measuring Risk With Scarce Observation." (March 2008). [2] Andrew Laurence Comrey, Howard Bing Lee. 1992 A first course in factor analysis. Taylor and Francis. [3] Fox, John. "Nonlinear Regression And Nonlinear Least Square. "(January 2002) [4] Goodman, David J. "Probability and Stochastic Processes."Second Edition. John Wiley and Sons, Inc (2005). [5] Ho-nam, Eric. "Simple linear regression. "lecture 37. [6] "Hedge Fund Databases" EUREKAHEDGE. copyright 2011 Eurekahedge Pte Ltd. [7] Joarder, Anwar H. "Moments Of the T-Distribution." (2006). [8] Joe, Harry."Multivariate Models and Dependence Concepts."(1997). [9] Samuel Kotz, N. Balakrishnan, Norman L. Johnson (2000). Continuous Multivariate Distributions, Models and Applications, 2nd Edition. [10] Kahane, Leo H. " Regression Basics." copyright 2001 by sage publications inc [11] Kleinbaum, Kupper."Applied Regression Analysis And Other Multivariable Methods." Boston , Massachusetts.(1978) [12] Lawson, C.L ;Hanson, R.J. (1974). Solving least Square Problems. Englewood Cliffs, NJ:Prentice-Hall [13] Long, Scott. "Regression Models for Categorical and Limited Dependent Variables."(1997)

51

[14] Masjedjamei, Mohammad. "Three Finite Classes Of Hypergeometric Orthogonal Polynomials And Their Application In Functions Approximation. " Integral Transforms and Special Functions, 2002, Vol.13, pp.169-190. [15] McCrary, Stuart A. "Hedge Fund A Professional's Guide."(2002) [16] Samprit Chatterjee, Ali S.Hadi. "Regression Analysis By Example."Hoboken, N.J :Wiley-Interscience, 4th ed, c2006. [17] Shaw, W.T, K.T.A. Lee. "Copula Method Vs Canonical Multivariate Distributions The Multivatiate Student T Distribution With General Degree Of Freedom." (2007). [18] Sosa, Walter. "Conditional Expectation and linear regression."(2009) [19] Van Huffel, Sabine. Vandewalle, Joos. "The total least square Problems".(1991) [20] Weisberg, Sanford. "Applied Linear Regression."Third edition. (2005) [21] Wonnacott, Thomas. H. "Regression:A Second Course In Statistics." (1981)

52

