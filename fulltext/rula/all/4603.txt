INNOVATIVE APPROACH FOR AUTOMATIC LAND COVER INFORMATION EXTRACTION FROM LIDAR DATA
by

Nagwa Taha Hamdy El-Ashmawy
B.Sc., Cairo University, Faculty of Engineering, Egypt, 1993 M. Sc., Cairo University, Faculty of Engineering, Egypt, 1998 M. Sc., ITC, the Netherlands, 2003 Ph. D., Cairo University, Faculty of Engineering, Egypt, 2005

A dissertation
presented to Ryerson University in partial fulfilment of the requirements for the degree

Doctor of Philosophy
In the Program of

Civil Engineering

Toronto, Ontario, Canada, 2015 © Nagwa El-Ashmawy, 2015

Author's Declaration For Electronic Submission of A Dissertation
I hereby declare that I am the sole author of this dissertation. This is a true copy of the dissertation, including any required final revisions, as accepted by my examiners.

I authorize Ryerson University to lend this dissertation to other institutions or individuals for the purpose of scholarly research

I further authorize Ryerson University to reproduce this dissertation by photocopying or by other means, in total or in part, at the request of other institutions or individuals for the purpose of scholarly research.

I understand that my dissertation may be made electronically available to the public.

ii

Abstract
An Airborne laser scanning (ALS) system with LiDAR (Light Detection And Ranging) technology is a highly precise and accurate 3D point data acquisition technique. LiDAR technology has been extensively used in digital surface/terrain modelling (DSM/DTM), and related applications such as 3D city modelling and building extraction. The capability of LiDAR systems to record the intensity of the return laser pulse backscattered energy in addition to the range data has motivated researchers to investigate the use of LiDAR intensity data for extracting land cover information. The main goal of this research is to maximize the benefits of the use of LiDAR data independently of any external source of data for automatically extracting accurate land cover information. Several new approaches are introduced in this research: a) classifying and filling the LiDAR intensity point cloud to produce a land cover image, b) combining multiple classified data of multiple LiDAR data-strips, c) statistical analysis segmentation technique that uses the concept of the kurtosis change curve algorithm for automatic classification of LiDAR data, and d) accelerating the classification process of large datasets by partitioning the large datasets into small, manageable datasets. Applying the traditional image classification techniques on LiDAR elevation and intensity data exclusively is included. Pixel-based, object-based, and point-based classification logics are conducted, and their results are compared to reference data. The results indicated that LiDAR data (range and intensity) can independently be used in land cover classification. By applying traditional pixel-based, supervised image classification techniques, the classification results show that auxiliary layers, which are extracted from range and intensity data, can be used for land cover classification. However, applying the supervised classification techniques on the LiDAR point cloud data without converting the data into images (Point-based logic) produced more accurate land cover classification results. The experiments on the proposed classification approach using the statistical analysis segmentation technique (based on the concept of the kurtosis change curve algorithm) show that it can be used to classify LiDAR data for land cover mapping.

iii

Acknowledgements
I would like to express my sincere gratitude to all those who have contributed directly or indirectly in successfully completing my study. I extend my grateful to Ryerson University, the Discovery Grant from the Natural Sciences and Engineering Research Council of Canada (NSERC) and the GEOIDE Canadian Network of Excellence for providing the financial assistance to support my study. I would like also to thank McElhanney Consulting Services Ltd, BC, Canada, and Optech Canada for providing the real LiDAR data and image datasets. I would like to express my thankful to Dr. Ahmed Shaker, he is not only my supervisor, but he is a friend who kindly gives me hands of help long time before my arrival to Canada. He and his family were as a family for me who make life easy and warm. They offer all support during the course of this work. Ahmed's correct guidance, valuable discussions and constructive suggestions cannot be denied and helps in achieving this research in professional way. I would like also to express my sincere gratitude to Prof. Ahmed El-Rabbany, he was the Graduate Program Director when I enrolled in 2010. Dr. El-Rabbany did his best to support the students. He gave me real support several times and encouraged me during hard times. Dr. El-Rabbany is also a member of the Supervisory Committee, who gave me valuable guidance. I also would like to thank Prof. Mike Chapman, the Professor of Photogrammetry and a member of my Supervisory Committee. I cannot forget Dr. Chapman support before and during my candidacy exam. I also appreciate his remarkable suggestions during discussions and committee meeting. My thanks also go to the graduate program administrators, Ms. Rachel Harpley and Ms.Rachel Trozzolo for their immediate support whenever I required. Also my grateful thanks go to Ms. Alicia Vandewghe in the International Student Service for her guidance during my stay in Canada. Also I would like to thank Kim Kritzer, Dianne Mendonca and Robin Luong for their warm and friendly moments whenever we meet. I appreciate very much my classmates and roommates for their friendly relationship, and for their frequent support. They really made my life much easier and full of joy.

iv

Finally, I would like to dedicate this dissertation to my dear mother, who encouraged me to finish this work despite her illness. Her last commandment to me was to not give up and to continue in my studies. I feel her compassion and blessing around me everywhere, and I am working hard to satisfy her commandment; may Allah bless her. Nagwa El-Ashmawy

v

Table of Contents
INNOVATIVE APPROACH FOR AUTOMATIC LAND COVER INFORMATION EXTRACTION FROM LIDAR DATA.......................... i Author's Declaration For Electronic Submission of A Dissertation ............................................................... ii Abstract ........................................................................................................................................................ iii Acknowledgements...................................................................................................................................... iv Table of Contents ......................................................................................................................................... vi List of Tables ................................................................................................................................................ ix List of Figures ................................................................................................................................................ x List of Appendecies .....................................................................................................................................xiv Acronyms and Definitions ........................................................................................................................... xv 1. Introduction ............................................................................................................................................ 1 1.1 1.2 1.3 1.4 Overview ....................................................................................................................................... 1 Research Motivation ..................................................................................................................... 2 Research Objectives ...................................................................................................................... 3 Dissertation Structure ................................................................................................................... 4

2. Literature Review ................................................................................................................................... 7 2.1 Airborne Laser Scanning System ................................................................................................... 7

2.1.1 Overview of Airborne Laser Scanning System .......................................................................... 7 2.1.2 Data Correction ....................................................................................................................... 11 2.2 Information Extraction and Image Classification ........................................................................ 17

2.2.1 Overview ................................................................................................................................. 17 2.2.2 Classification Logics................................................................................................................. 17 2.2.3 Supervised and Unsupervised Classification Techniques ....................................................... 19 2.2.4 Decision Tree Classifier ........................................................................................................... 25 2.2.5 Multiple Classifier System ....................................................................................................... 25 2.3 Classification of LiDAR Data ........................................................................................................ 28

2.3.1 Classification of LiDAR Range Data ......................................................................................... 28 2.3.2 Classification of LiDAR Range and Intensity Data ................................................................... 31 3. Methodology ........................................................................................................................................ 37 3.1 3.2 Overview ..................................................................................................................................... 37 Raster Image Classification (Part-1) ............................................................................................ 39

3.2.1 Pixel-Based Classification ........................................................................................................ 40

vi

3.2.2 Object-Based Classification ..................................................................................................... 43 3.3 Point Data (Point-Based Logic) Using Existing Classification Algorithm (Part-2) ........................ 45

3.3.1 Data Preparation Stage ........................................................................................................... 48 3.3.2 Classification Stage.................................................................................................................. 48 3.3.3 Evaluation Stage...................................................................................................................... 55 3.4 Part-3: Classification of LiDAR Point Data Using Statistical Analysis Technique ......................... 56

3.4.1 Unsupervised Segmentation Stage ......................................................................................... 57 3.4.2 Supervised Classification Stage ............................................................................................... 60 3.4.3 Proposed Classification Approach for Large Areas ................................................................. 63 4. Results and Analysis: Single Wavelength LiDAR Data .......................................................................... 65 4.1 4.2 Overview ..................................................................................................................................... 65 Study Area and Dataset .............................................................................................................. 66

4.2.1 Location of the Study Area...................................................................................................... 66 4.2.2 Dataset .................................................................................................................................... 66 4.2.3 Study Areas ............................................................................................................................. 68 4.3 Data Preparation ......................................................................................................................... 69

4.3.1 Geometric Calibration ............................................................................................................. 69 4.3.2 Radiometric Correction ........................................................................................................... 70 4.4 Experimental Work and Results .................................................................................................. 73

4.4.1 Part-1: Classification of LiDAR Data Converted into Raster Image Format ............................ 73 4.4.2 Part-2: Point-Based Classification of LiDAR Data Using (Maximum Likelihood Classifier) ..... 98 4.4.3 Part-3: Classification of LiDAR Data in Point Format Using Kurtosis Change Curve ............. 107 5. Results and Analysis: Multi-Wavelength (Multi-Spectral) LiDAR Data ............................................... 115 5.1 5.2 5.3 5.4 Overview ................................................................................................................................... 115 The Dataset and Study Area...................................................................................................... 115 Data Preparation ....................................................................................................................... 117 Classification Results and Analysis ............................................................................................ 119

5.4.1 Part-1: Classification of LiDAR Data in Raster Format........................................................... 119 5.4.2 Part-2: Point-Based Classification of LiDAR Data Using Existing Classification Algorithm .... 128 5.4.3 Part-3: Classification of LiDAR Point Data Format with Kurtosis Change Curve Classifier.... 136 5.4.4 Classification of LiDAR Point Data Using Rule-Based Method with a Developed Decision Tree ............................................................................................................................................... 144

vii

6. Summary, Conclusions and Future Work ........................................................................................... 149 6.1 Summary ................................................................................................................................... 149

6.1.1 Research Purpose.................................................................................................................. 149 6.1.2 Research Methodology and Experimental Work .................................................................. 149 6.2 Results Discussion and Conclusions .......................................................................................... 151

6.2.1 Results Discussion ................................................................................................................. 151 6.2.2 Conclusions: .......................................................................................................................... 155 6.3 Future Work .............................................................................................................................. 156

Appendices................................................................................................................................................ 157 References ................................................................................................................................................ 179

viii

List of Tables
Table  4-1: RMSE analysis............................................................................................................................. 70 Table  4-2: The Number of Reference Points in each Land Cover Class in "Area1" .................................... 79 Table  4-3: Accuracy Assessment of Classification Results for "Area1" ....................................................... 79 Table  4-4: Number of Reference Points in each Land Cover Class in "Area2" ........................................... 84 Table  4-5: Accuracy Assessment of Classification Results for "Area2" ....................................................... 85 Table  4-6: Number of Reference Points in each Land Cover Class in "Area3" ........................................... 89 Table  4-7: Accuracy Assessment of Pixel-Based Classification Results for "Area3" ................................... 89 Table  4-8: Accuracy Assessment of Object-Based Classification Results for the Three Study Areas ......... 94 Table  4-9: Comparison Between Pixel-based and Object-Based Classification Results (4 classes) ............ 95 Table  4-10: Accuracy Assessment of the Classified Grid Space for Both Data-Strips with NN and IM Filling Gaps Approaches. ..................................................................................................................................... 101 Table  4-11: Accuracy Assessment of the Combined Data-Strips by Various Approaches ........................ 106 Table  4-12: Accuracy Assessment of the Partetioned Classified Data...................................................... 113 Table  5-1: Number of Reference Points in each Distinguished Class within the Study Area for MultiSpectral LiDAR data ................................................................................................................................... 125 Table  5-2: Classification Accuracy for the Pixel-Based Classification Logic for the Multi-Spectral LiDAR data ........................................................................................................................................................... 126 Table  5-3: Classification Accuracy for the Point-Based Classification Logic for the Multi-Spectral LiDAR data ........................................................................................................................................................... 135 Table  5-4: Accuracy of Classification Results for the LiDAR Point Cloud Using the Iterative Single-Segment Separation process and the Minimum Distance to Means Classification Algorithm ............................... 140 Table  5-5: Accuracy of Classification Results for the LiDAR Point Cloud Using the Multiple-Segments Separation process and the Minimum Distance to Means Classification Algorithm ............................... 144 Table  5-6: Accuracy of Classification Results for the LiDAR Point Cloud Using the Rule-Based Classifier 148

ix

List of Figures
Figure  2-1: Airborne Laser Scanning System................................................................................................. 7 Figure  2-2: Typical Laser Scanning System Components .............................................................................. 8 Figure  2-3: Conceptual Differences between Discrete-Returns and Full Waveform LiDAR Systems ......... 10 Figure  2-4: Coordinate System and Involved Quantities in the LiDAR Equation ........................................ 14 Figure  2-5: Procedure of Skewness Balancing Algorithm ........................................................................... 23 Figure  3-1: Methodology Flow Chart .......................................................................................................... 38 Figure  3-2: Workflow of Pixel-Based Classification of LiDAR Data ............................................................. 41 Figure  3-3: Developed Decision Tree for Object-Based Classification of LiDAR Data................................. 44 Figure  3-4: Sample of Point Clouds Data of an Overlapped Area ............................................................... 46 Figure  3-5: Workflow of Point Data Classification ...................................................................................... 48 Figure  3-6: Steps of Resampling the Classified Points to the Grid Points, .................................................. 50 Figure  3-7: Example of Assigning Classes to the Unclassified Grid Points (Filling the Gaps) Using Different Methods .................................................................................................................................. 51 Figure  3-8: Procedure of the Introduced Classification Approach ............................................................. 57 Figure  3-9: Example of Kurtosis Change Curve ........................................................................................... 58 Figure  3-10: Example of Segmentation Using Kurtosis Change Curve with Iterative Single-Segment Separation Process .................................................................................................................. 59 Figure  3-11: Workflow of Unsupervised Segmentation of the Point Cloud LiDAR Data ............................ 61 Figure  3-12: Steps of Supervised Classification Process for the Produced Sub-Segments ......................... 62 Figure  3-13: Example of Segmentation Using the Multiple-Segments Separation Method ...................... 64 Figure  4-1: Study Area (Clipped from Google Map and Google Earth)....................................................... 66 Figure  4-2: Flight Directions and Data-Strips of the LiDAR Acquisition Mission ......................................... 67 Figure  4-3: Study Areas ............................................................................................................................... 69 Figure  4-4: Point Cloud Data (Intensity Values) of the Three Study Areas for the Single Wavelength LiDAR Data ......................................................................................................................................... 71 Figure  4-5: Point Cloud Data (Elevation Values) of the Three Study Areas for the Single Wavelength LiDAR Data ............................................................................................................................... 72 Figure  4-6: Interpolated Data of "Area1" ................................................................................................... 74 Figure  4-7: Extracted Layers for "Area1" .................................................................................................... 75

x

Figure  4-8: Pixel-Based Classification Results for "Area1" .......................................................................... 78 Figure  4-9: Distribution of the Reference/Check Points in "Area1" ........................................................... 78 Figure  4-10: Interpolated Data of "Area2" ................................................................................................. 81 Figure  4-11: Extracted Layers for "Area2" .................................................................................................. 82 Figure  4-12: Pixel-Based Classification Results for "Area2" ........................................................................ 84 Figure  4-13: Distribution of the Reference Points in "Area2" .................................................................... 84 Figure  4-14: Interpolated Data of "Area3" ................................................................................................. 86 Figure  4-15: Extracted Layers for "Area3" .................................................................................................. 87 Figure  4-16: Pixel-Based Classification Results for "Area3" ........................................................................ 88 Figure  4-17: Distribution of the Reference Points on, "Area3" .................................................................. 89 Figure  4-18: Workflow of the Object-Base Classification (Developed Desicion Tree) ................................ 91 Figure  4-19: Segmentation Results of the Intensity Layer .......................................................................... 92 Figure  4-20: Object-Based Classification Results for the three Study Areas .............................................. 94 Figure  4-21: Comparing Pixel-based and Object-based Classification Results for "Area1" ........................ 96 Figure  4-22: Comparing Pixel-based and Object-based Classification Results for "Area2" ........................ 97 Figure  4-23: Comparing Pixel-based and Object-based Classification Results for "Area3" ........................ 98 Figure  4-24: Classification Results of the Original Points using the Point-Based Classification Logic with ML Classifier .......................................................................................................................... 100 Figure  4-25: Classification Results of Grid Points For Strip4 ..................................................................... 101 Figure  4-26: Classification Results of Grid Points for Strip6 ..................................................................... 101 Figure  4-27: Classification Results of the Merged Data of the two data-strips ........................................ 102 Figure  4-28: Examples of the Classification Improvement after Merging the Data-Strips ....................... 103 Figure  4-29: Classification Results of the Normalized Intensity Combined Data ..................................... 104 Figure  4-30: Classification Results of the Merged Classified Data ............................................................ 104 Figure  4-31: Confusion Matrices and Normalized Confusion Matrices of both Data-Strips .................... 105 Figure  4-32: Classification Results of the Two Data-Strips Using CMCD Technique................................. 106 Figure  4-33: Comparison between Pixel-based and Point-based Classification ....................................... 107 Figure  4-34: Iterative Single-Segment Separation Method ...................................................................... 108 Figure  4-35: Classification Results Using the Proposed Classification Approach Based on Statistical Analysis Technique ................................................................................................................ 109 Figure  4-36: Comparison between the Three Implemented Classification Approaches .......................... 110 Figure  4-37: Segments of the Multiple-Segments Separation Method .................................................... 111

xi

Figure  4-38: Classification Results of the Proposed Statistical Analysis Approach Using the MultipleSegments Separation Process ............................................................................................... 111 Figure  4-39: Classification Results of the Proposed Classification Approach for the Partitioned Areas (20,000 points) Using Iterative Single-Segment Separation Process .................................... 112 Figure  4-40: Classification Results Using the Proposed Classification Approach for the Partitioned Areas (20,000 points) Using Multi-Segment Separation Process .................................................... 112 Figure  4-41: Classification Results Using the Proposed Classification Approach for the Partitioned Areas (10,000 points) Using Iterative Single-Segment Separation Process .................................... 113 Figure  4-42: Classification Results Using the Proposed Classification Approach for the Partitioned Areas (10,000 points) Using Multi-Segment Separation Process .................................................... 113 Figure  5-1: Location of the Captured Data and the Study Area................................................................ 116 Figure  5-2: Point Cloud Data (Intensity Values) of the Three Channels for the Multi-Spectral LiDAR Data ............................................................................................................................................... 117 Figure  5-3: Point Cloud Data (Elevation Values) of the Three Channels for the Multi-Spectral LiDAR Data ............................................................................................................................................... 118 Figure  5-4: Intensity images, DSM, and DTM of the Three Channels ....................................................... 120 Figure  5-5: Extracted Normalized DSM layers of the Three Channels ...................................................... 121 Figure  5-6: Pixel-Based Classification Results for Intensity images and DSM Separately ........................ 122 Figure  5-7: Pixel-Based Classification Results for Intensity and DSM images, and Intensity and NDSM images ................................................................................................................................... 123 Figure  5-8: Pixel-Based Classification Results for the Multi-Layer images ............................................... 124 Figure  5-9: The Distribution of the Reference Points within the Study Area ........................................... 125 Figure  5-10: Segmentation Results of the Intensity Layer ........................................................................ 127 Figure  5-11: Classification Results Using Object-Based Classifier ............................................................. 128 Figure  5-12: Point-Based Classification Results for the Three Channels Based on Intensity Data ........... 130 Figure  5-13: Point-Based Classification Results for the Three Channels Based on Elevation Data .......... 131 Figure  5-14: Point-Based Classification Results for the Three Channels Based on Intensity and Elevation Data ....................................................................................................................................... 132 Figure  5-15: Points of the 3 Channels ....................................................................................................... 133 Figure  5-16: Point-Based Classification Results for the Combined Points of the Classified Channels Based on Intensity and Elevation Data ............................................................................................ 133

xii

Figure  5-17: Point-Based Classification Results for the Combined Points of the Classified Channels Using CMCD Combination Approach .............................................................................................. 134 Figure  5-18: Segmentation Results of the Three Channels Using Iterative Single-Segment Separation Process Based on Elevation then Intensity Values ................................................................ 137 Figure  5-19: Classification Results of the Produced Segments by Iterative Single-Segment Separation Process Using the Minimum Distance to Means Classifier ................................................... 139 Figure  5-20: Segmentation Results of the Three Channels Using Multiple-Segments Separation Process Based on Elevation then Intensity Values ............................................................................. 141 Figure  5-21: Classification Results of the Produced Segments by Multiple-Segments Separation Process Using the Minimum Distance to Means Classifier ................................................................ 143 Figure  5-22: Rule-Based Classifier ............................................................................................................. 145 Figure  5-23: Classification Results Using the Rule-Based Classification ................................................... 147

xiii

List of Appendecies

Appendix A: Confusion Matrices for the Image Classification Results of the Single Wavelength LiDAR Data ................................................................................................................................... 158 Appendix B: Confusion Matrices for the Point Classification Results of the Single Wavelength LiDAR Data ................................................................................................................................... 165 Appendix C: Confusion Matrices for the Point Classification Results of the Single Wavelength LiDAR Data Using Kurtosis Change Curve ..................................................................................... 168 Appendix D: Confusion Matrices for the Classification Results of the Multi-Spectral LiDAR Data ........ 169

xiv

Acronyms and Definitions
ALS CM CMC CMCD DEM DSM DTM DGPS GPS IMU ISODATA LiDAR ML MW NCM NDSM NN NIR PCA RCI Airborne Laser Scanning Confusion Matrix Combined Multiple Classifier Combined Multiple Classified Dataset Digital Elevation Model Digital Surface Model Digital Terrain Model Differential GPS Global Positioning System Inertial Measurement Unit Iterative Self-Organizing Data Analysis Technique Light Detection And Ranging Maximum Likelihood Moving Window Normalized Confusion Matrix Normalized DSM Nearest Neighbour Near Infra-Red Principal Components Analysis Radiometric Corrected Intensity

xv

1. Introduction
1.1 Overview
The concepts of data collection and information extraction have changed since the 1970s. Releasing remote sensing data for civilian applications has challenged researchers to develop new techniques for image interpretation and information extraction. One of the techniques that have been used for information extraction is classification. Various approaches and algorithms have been developed for classification depending on data characteristics. Whenever a new technology for data acquisition becomes available, researchers investigate the suitability of using existing classification techniques with this data type, and/or develop new classification techniques that are more appropriate for the new data type. Several supervised and unsupervised classification algorithms have been developed to classify image data, especially for images captured by optical satellite sensors. In the 1980s, NASA developed a laser scanning system that was able to measure distances precisely. The system emits laser signals and receives the reflected energy from targets. By measuring the time difference between sending and receiving the laser signals, ranges between the scanner and targets are determined. This technology is called LiDAR, which stands for light detection and ranging. In the 1990s, the Global Positioning System (GPS) provided a solution for positioning problems, which enabled airborne laser scanners to acquire highly accurate 3D point data. These data were used, for all intents and purposes, for generating digital terrain/surface models (DTM/DSM). A few years later, airborne laser scanning systems were successfully established, and rapidly used in various practical applications (Ackermann, 1996; Baltsavias, 1999). The acquired data by LiDAR systems have been used intensively in 3D city modelling, and building extraction and recognition (Haala & Brenner, 1999; Song et al., 2002; Yan et al., 2015). LiDAR systems are capable of recording the intensity of backscattered energy from the illuminated targets in addition to the range data. Most of the commercial LiDAR sensors utilize laser signals that operate at 1.064 m wavelength, which is in the near infrared (NIR) spectrum. At this region of the electromagnetic spectrum, high separability of spectral reflectance of various land cover materials can be observed. As a result, distinguishing different ground materials based on the intensity values of LiDAR data can be achieved (Yan et al., 2012). Consequently, a new area for research was opened to investigate the applicability of including intensity in addition to range values in land cover classification of LiDAR data.

1

1.2 Research Motivation
LiDAR range data have been used to generate highly precise and accurate 3D point data in x, y, and z coordinates (Brennan & Webster, 2006). The highly precise and accurate 3D point data are used for generating digital elevation and/or surface models (DTM/DSM). Separating terrain from non-terrain points was the conventional meaning of LiDAR data classification (Antonarakis et al., 2008). By combining laser range data with other external auxiliary data, such as multispectral aerial photos, and satellite images, accurate information about the ground surface could be extracted (Haala & Brenner, 1999). The capability of LiDAR systems to record intensity data changed the definition of LiDAR data classification, where intensity data were included in the classification process. Nonetheless, extracting land cover information from LiDAR data independent of any other sources of data requires more attention. Researchers typically use LiDAR intensity data, which were acquired from the backscatter energy of the LiDAR signals, as monochrome images. However, these images are not similar to the commonly used optical images. That is because when a laser signal hits more than one object (e.g., tree leaves, tree branches, and the ground beneath), a portion of the signal reaches each object; thus, multiple returns of the same laser signal are produced with lower returned energy. As a result, a reduction of the backscattered energy of each illuminated object occurs, which affects the intensity values recorded and is displayed as an image (e.g., trees appear darker than expected) (Jensen, 2007). Therefore, the special characteristics of LiDAR intensity data have to be treated with more attention compared to the optical aerial and satellite images. To solve the problem of the reduced backscattered energy of the multiple returns of LiDAR intensity data, the exact, received energy data are required. Unfortunately, these data are not available. However, radiometric correction of LiDAR intensity data was suggested in recent literature to overcome the problem of energy attenuation, caused by several factors including atmospheric conditions, and targets geometry (Coren & Sterzia, 2006; Höfle & Pfeifer, 2007; Yan et al., 2012). The previously introduced radiometric correction methods rely on the use of the laser range equation to convert intensity data (recorded from the backscattered energy) into spectral reflectance of the illuminated targets. The radiometric correction takes into consideration the scanning geometry, the atmospheric attenuation, and the background backscattering effects. After applying the radiometric correction, the

2

intensity values of each target become more homogeneous. Thus, the performance of feature extraction and surface classification can be enhanced (Shaker et al., 2011). LiDAR systems are integrated multi-sensor systems that contain a laser scanner, GPS receivers, and inertial measurement unit (IMU). In addition, LiDAR systems are usually supported by cameras for capturing aerial images at the same time as LiDAR data acquisition. These aerial images have roles in LiDAR data application, particularly the land cover information extraction. Relying on LiDAR instruments without being accompanied by aerial cameras will reduce the cost and size required for the payload, and consequently, will be more economic and efficient. This could be one further step toward the use of economic LiDAR systems in the increasingly popular UAVs and drones. For that reason, using LiDAR data separately (independently of any external data) in land cover information extraction needs to be investigated. With the ability of LiDAR signals to penetrate tree canopies, using LiDAR data in classification of forest areas has increased over the optical sensors, which capture images for the canopy surfaces. However, multispectral images provide more information than the information extracted from the LiDAR signals with a single wavelength. Therefore, there is a need for a combination of the following two advantages, penetrating the canopies for the vertical structure of the trees and multi-spectral data for physiological measurements of the trees (Woodhouse et al., 2011; Wallace et al., 2012). Recently, researchers have been investigating the production of multi-spectral LiDAR systems with multi-wavelength laser signals that are operating at various wavelengths. Consequently, multi-spectral LiDAR data can be obtained, and can be used in land cover classification of LiDAR data (Woodhouse et al., 2011). This new technology will enrich the classification results of LiDAR intensity data, and will make it possible to eliminate the use of external optical sensor data in land cover mapping.

1.3 Research Objectives
The main goal of this research is to maximize the benefit of using LiDAR data independently of any other source of data for accurate land cover information extraction. This research proposes an innovative approach to automatically extract land cover classes from LiDAR data (i.e., elevation and intensity attribute values), exclusively. This goal can be achieved by integrating the range and intensity data measurements of LiDAR systems. The detailed objectives of this research are as follows:

3



To study the potential use of LiDAR data for land cover information extraction through evaluating the pixel-based image classification of LiDAR elevation and intensity attribute values individually. Extracting auxiliary layers from LiDAR elevation and intensity data and include them in the classification process will be studied as a step towards classification improvement.



To develop a new approach to classify LiDAR point cloud data without losing the details of the 3D points, and to fill the gaps of the LiDAR footprints. Evaluating the developed approach by comparing its results to the results of the image classification techniques (classified data after interpolating the raw LiDAR data and converting the point data into images).



To develop a new approach for land cover classification that can be used for overlapped datastrips. A Combined Multiple Classified Datasets (CMCD) approach is proposed, which is a modification of the Combined Multiple Classifiers (CMC) approach introduced in the pattern recognition field.



To develop a new approach for land cover classification of LiDAR point cloud data based on statistical analysis of the elevation and intensity values. The new approach is based on the concept of the kurtosis change curve algorithm that has been used previously to separate ground from non-ground returns collected by LiDAR systems.

1.4 Dissertation Structure
This dissertation consists of six chapters; the first chapter is a general introduction that includes the motivations, objectives, and the structure of the dissertation. Chapter 2 presents a literature review on airborne LiDAR systems and various classification techniques. It describes the concept behind the LiDAR system as a highly accurate and precise technique for 3D data acquisition. It includes a description of LiDAR systems, explains how these systems collect and generate data, and describes the characteristics of LiDAR systems. Moreover, the literature review includes the information extraction approaches, classification logics, and techniques, with some examples of the existing classifiers. At the end of the second chapter, a brief overview is provided of how researchers have used the classification techniques for land cover classification of LiDAR data. The third chapter discusses the methodology used for this research. The methodology first addresses the classification of the LiDAR data using one of the existing pixel-based classifiers and a developed decision tree for object-based classification. The second part of the methodology includes classification of single and multiple data-strips using point-based classification logic with the supervised classification techniques. The third part of the methodology is a description of

4

a proposed innovative classification approach based on a statistical analysis segmentation technique that uses the intensity and elevation attribute values of the LiDAR data. Chapters 4 and 5 describe study areas and the employed datasets. These two chapters include experimental works that have been completed to test the described methodology; in Chapter 4 a single wavelength LiDAR dataset has been investigated, while a multi-wavelength LiDAR dataset has been investigated in Chapter 5. Chapters 4 and 5 also include evaluation of the classification results, and discuss the work's achievements. The dissertation ends with a summary of the work, conclusions, and remarks for future work.

5

2. Literature Review
2.1 Airborne Laser Scanning System
2.1.1 Overview of Airborne Laser Scanning System

An airborne laser scanning (ALS) system is an active remote sensing system that transmits laser signals and records the reflected energy, and measures distances using the light detection and ranging (LiDAR) technology. The distance between the sensor and the illuminated spot on the ground (range) can be calculated based on the measurements of the travelling time of the laser signal (time difference between sending and receiving the laser signal) (Jensen, 2007). ALS includes devices to identify positioning and orientation of the laser sensor at the moment of illuminating and receiving the laser energy. With the aid of data that are collected by positioning and orientation systems, Global Positioning System (GPS) and Inertial Measurement Unit (IMU), 3D coordinates of the laser footprints can be determined. Figure 2-1 illustrates an airborne laser scanning system.

Figure  2-1: Airborne Laser Scanning System

2.1.1.1

System Components

Airborne laser scanning systems are integrated multi-sensor systems. ALS consists of several components, some of them are on-board and others are land-based (Ackermann, 1996). Figure 2-2
7

illustrates the on-board ALS system components as described in Wehr and Lohr (1999). The typical onboard components of any laser scanning system are (Baltsavias, 1999):       Laser range unit including laser transmitter and receiver, signal detector, amplifier, time counter, and the necessary electronic components. Scanner. Global Positioning System (GPS) receivers with their antennas that form with ground reference station(s), a differential GPS system. Inertial Measurement Unit (IMU). Registration units (usually two units, one for laser data and one for GPS/IMU data). Some systems complement the laser system with digital or video cameras for documentation.

Control, Monitoring, and Recording Unit

DGPS

Ranging Unit

IMU

Scanner

Laser Scanner

Laser footprint

Figure  2-2: Typical Laser Scanning System Components
(adapted from Wehr & Lohr, 1999)

The typical on-ground components of laser scanning systems are (Baltsavias, 1999):  Mission planning software and post processing software (could be one software package for both planning and post-processing).

8

  2.1.1.2

GPS reference station(s); the GPS reference station and the GPS device on board establish a differential GPS (DGPS). Radio link (for real time navigation). System Data Collection

The laser instrument mounted in an aircraft transmits laser signals toward the ground while the aircraft moves along the line-of-flight. These signals are directed across track using a scanning mirror. Some of the transmitted energy is reflected to the aircraft; this backscattered energy is recorded by the receiver electronics. The travelling time between sending the signal and receiving the backscattered energy (reflected by the objects on ground) is measured very accurately in terms of 10-10 sec, by which the distance between the aircraft and the surface can be measured based on the speed of the laser signals (speed of light is 3 × 108 m/sec) (Ackermann, 1999). The reflecting surface characteristics (such as surface materials and inclination angles) influence the return signals. The ratio between the reflected power and the irradiating power gives the surface reflectance (Hug & Wehr, 1997). The on-board GPS system is used for measuring the position of the laser scanner altimeter. There is usually another GPS receiver on a reference station on ground, together with the receiver on-board, a DGPS system is constituted and used for achieving accurate positions of the aircraft. The IMU system measures pitch, roll, and yaw rotation angles of the laser system onboard the aircraft, which define the laser sensor attitude. The combination of these three systems--GPS, IMU, and laser scanner--makes it possible to know the accurate positions of the laser sensor, the direction of the laser beam, and the distance between the laser sensor and the ground surface at the moment of sending and receiving each laser signal. Thus, using this information, the coordinates of each point on the ground that are hit by the laser beam can be calculated (Ackermann, 1999). Figure 2-1 illustrates the operational idea of a laser scanning system with its main components. There are two main operational concepts for the LiDAR systems: discrete returns and continuous wave (full waveform). The discrete returns LiDAR system is designed to send laser pulses and receive the returned signals, where a filter is applied to detect peaks in the reflected signals and then record the timing of those peaks as discrete returns. Based on the capabilities of the system and the number of peaks, the number of returns is delineated. Recently, there have been LiDAR systems capable of recording up to five returns of emitted laser signals (Morgan, 2012). A few years ago, the LiDAR systems

9

were designed to be capable of recording the complete waveform of the returned laser signals, which provide additional information that can be used for better interpretation of the data (Mallet & Bretar, 2009). Figure 2-3 illustrates the differences between the two LiDAR operation concepts as described in Weng (2011).

1st Return At 12m, 0 ns

Time Interval of Returned Intensity is 1 ns

2nd Return At 6.3m, 19 ns

Not-recorded Return 3rd Return At 0m, 40 ns

0 % Intensity

100

0 % Intensity

100

Discrete Returns

Full Waveform

Figure  2-3: Conceptual Differences between Discrete-Returns and Full Waveform LiDAR Systems (Weng, 2011)

Two major principles are used for measuring range data with LiDAR systems: a pulsed ranging principle, and a phase difference determining principle (Wehr & Lohr, 1999; Baltsavias, 1999a). Both principles measure the travel time of the laser signal precisely, and then calculate the range data () using the formula in Equation (2-1), and using the travelling time of laser signals from the laser transmitter to the object and back from the object to the laser receiver (), and the speed of the light ().
 = 
 2

(2-1)

10

The discrete return pulses (multi-return) concept uses the pulsed ranging method, where the time difference between sending and receiving discrete pulses is measured. The continuous wave (full waveform) concept uses the phase difference principle, where the phase difference between transmitted and received signals backscattered from the object surface is measured (Hug & Wehr, 1997). In this research we will focus on the pulse ranging principle because the available data are discrete pulse returns. 2.1.1.3 System Characteristics

Currently, there are many laser scanners that are produced and commercially available. The characteristics of these systems vary from one to the other. Scan angle, pulse rate, scan rate, beam divergence, number of recording returned echoes per pulse, scanning pattern, and IMU frequency are important characteristics that differentiate the LiDAR systems (Baltsavias, 1999). There are some other parameters which may vary, such as point spacing (across and along track), swath width, point density and covered area--the latter parameters depend mainly on the flying heights and system frequency. The selection of the appropriate system is based on the application for which data are requested. However, the selection of these parameters is not totally free, as there are operational restrictions, like laser power and storage capacity (Baltsavias, 1999). The airborne laser scanning systems have evolved rapidly. In the mid-1990s, the first commercial airborne laser scanning systems appeared on the market. These systems were operated with pulse rates that ranged between 5000 and 15000 pulse/sec, but recently the pulse rate of the airborne laser scanners have reached 500,000 pulse/sec. The current scanners have the capability of recording full waveform of the backscattered signals, in addition to multiple-return laser pulses. Previously, the laser wavelength has ranged between 1040 and 1060 nm (and 532 nm for bathymetric laser scanners), while a few systems have utilized a wavelength of 1550 nm, and the ScaLARS system has used 810 nm wavelength (Baltsavias, 1999a). Recently with the multi-spectral LiDAR, other wavelengths are in operation--531, 550, 660, and 780 nm. 2.1.2 Data Correction

Measurements of LiDAR point data require accurate observations of the position, and the attitude of the laser scanner, besides accurate and precise ranging measurements. The accuracy of the laser scanning system can be divided into positioning accuracy and ranging measurements accuracy. The positioning
11

accuracy is influenced by the quality of the GPS/IMU system measurements, while the ranging measurements accuracy is affected greatly by the atmosphere. The travelling laser signals through the atmosphere are subjected to diffraction, absorption, scattering, and propagation delays. The flying height has a great effect on the accuracy of the ranging measurements, as the propagation delays increase tremendously with increasing the flying height (Bottu, 1998). The scattering and absorption of the laser signals are affected by the existence of moisture and dust in the atmosphere. All these factors, beside the accuracy of the GPS/IMU system, scan angle, terrain topography, and land cover types affect the accuracy of the laser system measurements (Bottu, 1998). Furthermore, inaccurate transformation parameters from WGS84, which GPS measurements are based on, to the local coordinate system may produce inaccurate point coordinates (Wehr & Lohr, 1999). In addition, using the system without calibration may reduce the accuracy of the point coordinates. Huising and Pereira (1998) summarized the sources of the systematic errors in the laser systems, and the approximate magnitude of each one. Thus, it is important to correct the acquired data geometrically. Laser intensity indicates the amount of recorded energy by the sensors that is backscattered and reflected by the illuminated targets. However, the laser signals are attenuated during travel through the atmosphere. Additionally, the magnitude of the laser intensity is affected by several factors. The three main factors that affect the laser intensity are the geometry of targets represented by incidence angles, atmospheric attenuation, and distance between the laser sensor on-board and objects on-ground (Yan et al., 2012). Therefore, the intensity values that are recorded by the laser sensors do not represent the actual surface reflectance of the targets. Removing the effects of these factors is required for extracting more reliable information. Thus, it is important to radiometrically correct the laser signals. The following subsections provide brief information about the geometric and radiometric correction of LiDAR data. 2.1.2.1 Geometric Correction of LiDAR Data

Laser scanner systems measure ranges of a vector between the on-board laser scanner and each illuminated point on the Earth's surface. To determine the 3D coordinates of the measured points, the position and orientation of the laser scanner, with respect to a certain coordinate system, must be accurately known at the moment of sending and receiving each laser signal. The position and orientation of the laser scanner are provided by a position and orientation supporting system (i.e., integrated GPS and IMU unit on-board system). To ensure obtaining accurate positioning and orientation of the laser scanner, it has to be exactly synchronised with the positioning and orientation system. Hence, other
12

relative parameters have to be considered. These parameters are: 1) the three mounting angles of the laser scanner frame (roll, pitch, and yaw) with respect to the platform-fixed coordinate system, 2) the position of the laser scanner with respect to the IMU coordinate system, and 3) the position of the IMU with respect to the GPS coordinate system. These mounting parameters can be derived by calibrating the laser scanning system. Consequently, the 3D positions of the LiDAR footprints can be computed with the aid of the calibration parameters besides the range measurements with their respective scanning angles, and aircraft positioning and orientation data (Wehr & Lohr, 1999). The biases in the mounting parameters that relate to the system components, and biases in the measured ranges and mirror angles cause systematic errors in the determined 3D positions of the LiDAR footprints (i.e., coordinates of point cloud data). The geometric correction of LiDAR data is the process of estimating and removing systematic errors from the 3D point coordinates leaving only random errors (Habib et al., 2011). These random errors are randomly distributed in elevations and planimetric positions of LiDAR point cloud (Wehr & Lohr, 1999). To determine the 3D coordinates of the LiDAR point cloud, the position of the points with respect to the origin of a certain coordinate system has to be determined (Figure 2-4). The vector  X G , representing the position of an object point (t) can be derived using the formula in Equation (2-2) as described in Habib et al. (2010). 0   =   + ,,   + ,, ,, , [ 0 ] -  Where, ,, rotation matrix relating the ground coordinate system and the IMU coordinate system with the three mounting angles--roll, pitch, and yaw angles--which are derived through the GPS/IMU integration process (determined through the calibration procedure).  , , ,   rotation matrix relating the IMU coordinate system and the laser unit coordinate system. rotation matrix relating the laser unit and laser beam coordinate systems with  and  being the mirror scan angles. vector from the origin of the ground system to the origin of the IMU coordinate system, which is derived through the GPS/IMU integration process with consideration of the

(2-2)

13

offset vector between the IMU body frame and the phase centre of the GPS antenna (determined through the calibration procedure).
  vector from the origin of the IMU coordinate system and the origin of the laser unit coordinate system. The magnitude of the vector   equals to the offset between the laser unit and the IMU coordinate systems (measured during the acquisition process).


laser range vector. The magnitude of  is equivalent to the distance between the laser firing point in the laser unit and its footprint on the ground (measured during the acquisition process) (Habib et al., 2010; Habib et al., 2011).

,,

IMU Coordinate System

 

Laser Unit

,
Laser Beam Coordinate System

,,

 0



Z Y X
Ground Coordinate System

 

Object Point (t)

Figure  2-4: Coordinate System and Involved Quantities in the LiDAR Equation (Habib et al., 2010)

14

2.1.2.2

Radiometric Correction of LiDAR Data

Backscattered energy that is reflected by an object on the ground is recorded as an intensity value. There are several factors that reduce the backscattered energy of the laser signals, such as atmospheric attenuation and characteristics of the illuminated targets. To overcome the problem of the recorded LiDAR intensity data, the exact surface reflectance of the illuminated targets, which are not available, are required. Ideally, without any other effects such as atmospheric attenuation, the surface reflectance is defined as the ratio between reflected and irradiated powers, which can be calculated using the following equation (Hug & Wehr, 1997):  =
 

(2-3)

Where,  Prefl Pirr target spectral reflectance; reflected power; irradiating power.

Recent research introduced radiometric correction methods for LiDAR intensity data (Coren & Sterzia, 2006; Höfle & Pfeifer, 2007; Yan et al., 2012). The same as with any active remote sensing sensors, radiometric correction of LiDAR intensity data should be able to remove the attenuation in the data values due to the system settings, topographic variation, and atmospheric condition (Yan et al., 2012). Since laser scanning systems operate in the same physical principles as the microwave radar but at shorter wavelengths (Jelalian, 1992), the radar range equation can be used for the radiometric correction of LiDAR intensity data. The radar range equation, introduced in Jelalian (1992), assumes that the intensity values represent the peak values of the received power (Habib et al., 2011). The radar range equation includes sensor efficiency, target characteristics, and atmospheric parameters, which are the three main factors attenuating the transmitted power (Höfle & Pfeifer, 2007). It also considers the scanning geometry, the atmospheric influences, and the background backscattering (Höfle & Pfeifer, 2007; Yan et al., 2012). The radiometric correction of LiDAR intensity data converts the received intensity data into the spectral reflectance. The radar range equation is described as (Jelalian, 1992):  =
2   2 4 4   

  

(2-4)

15

Where;         power of received signal; power of transmitted signal; receiver aperture diameter; range from the sensor to the target; laser beam width; system-specific factor; atmospheric transmission factor; target (backscattering) cross-section.

The target cross section  implies all target characteristics: spectral reflectance  , projected footprint area  , and the scattering solid angle . The following formula can be used to calculate the target cross section (Shaker et al., 2011)  =
4      

(2-5)

The scattering solid angle  is derived as the cosine of the angle between the surface normal and the direction of the laser pulse () with the assumptions that the entire footprint is reflected on one surface, the target area  is circular, and the surface has Lambertian scattering characteristics. Then the following formulas can be used for calculating the target footprint area and the target cross section (Höfle & Pfeifer, 2007; Jutzi & Stilla, 2006; Rees, 2001).  =
2  2  4

(2-6) (2-7)

2  =    2  cos 

By substituting the target cross section into the radar range equation (Equation (2-4)) leads to an inverse range square dependency of the received signal power (Equation (2-8)), independent of the laser beam width (Höfle & Pfeifer, 2007).  =
2     4 2

  cos 

(2-8)

16

After removing the effects of the atmosphere and the surfaces geometry, the surface reflectance can be calculated. Normally, the calculated surface reflectance is scaled to appropriate digital numbers (e.g., 8bit) for displaying purposes.  = 
4 2   cos 

2   

(2-9)

2.2 Information Extraction and Image Classification
2.2.1 Overview

Remote sensing systems acquire data of the Earth's surface and objects above it. Spectral reflectance of objects on the Earth's surface is one of the essential data that remote sensing systems acquire. Nevertheless, the acquired data must be analysed with a certain processing scheme in order to extract useful information. Generally, there are two ways to extract information from data collected by remote sensing sensors. First, methods depend on visual interpretation and manual digitization. They are simple methods to manipulate remote sensing data; however, they are labour intensive and time consuming. Secondly, there are automatically or semi-automatically computer based data processing methods. Examples of information extracted by computer based methods are not limited to surface modelling, feature extraction, and information extracted from image classification. In this study, we focus on the use of computerized classification techniques to derive different types of land cover information from airborne LiDAR data. 2.2.2 Classification Logics

Classification is the process of grouping data to a finite number of individual information classes, which are categories of interest to the data users. Classification processes differ based on the nature and structure of the data. In the following subsections, brief descriptions of the different classification logics based on the structure and nature of the data are provided. 2.2.2.1 Pixel-Based, Object-Based, and Point-Based Classification Logics

Based on the structure of the spatial data, the classification logic is selected. For raster data, classifiers categorize the raster data elements (pixels) based on their brightness values to satisfy certain criteria (Jensen, 2005). Pixel-based classification logic is often used to assign the raster data to the distinguished classes. Pixel-based classification examines the brightness value of each pixel individually and assigns it

17

to one of the predefined classes. On the other hand, object-based classification combines the pixels of the raster data into objects based on certain homogeneity criteria. Then it assigns these segmented objects to the predefined classes. The pixel-based classification logic is more common in coarse remote sensing images (e.g., ASTER, Landsat, SPOT, etc.), whereas the object-based classification is more suitable to the fine, high spatial resolution images (e.g., Ikonos, Quickbird, Worldview, etc.), due to the high variation of spectral values within each class (Jensen, 2005; Yan et al., 2006; Huang et al., 2004; Chen et al., 2009; Blaschke, 2010; El-Ashmawy et al., 2011; Chen & Gao, 2014). Some remotely sensed data are acquired as point data, such as LiDAR point cloud data. This type of data is usually classified after resampling the points into a predefined grid space (e.g., raster grid). The resampling is usually performed for easier processing and for presentation purposes. Nevertheless, the acquired 3D point data loses some details when resampled to a 2D grid (Yunfei et al., 2008; El-Ashmawy & Shaker, 2014). Thus, classifying the point data using point-based classification logic without conversion into a raster grid is expected to preserve these details. Point-based classifiers are similar to the pixel-based classifiers. They deal with each data element (in this case, the data elements are points) individually and examine the attribute values of each point and assign it to one of the distinguished classes. The point data that have more than one attribute value are comparable to the multi-spectral raster image. 2.2.2.2 Metric and Nonmetric Classification Logics

To classify any dataset, the nature of the data values must be determined at the beginning. There are mainly four different types of data values: nominal, ordinal, interval, and ratio data values. Nominal data values are the values that enable users to differentiate between the data by providing names, categories, or identifiers to the data (e.g., names of countries, soil types, etc.), but no computation processes can be applied to this type of data value. Ordinal data values are the data that can be sorted by natural sequence (e.g., small, medium, and large; low, average, and high, etc.), yet ordinal data values do not allow any type of computation. Interval data values, however, are numbers (e.g., temperature, pressure, etc.) and allow some types of computations, but these values have no arithmetic zero and no ratios can be obtained from this type of values. However, ratio data values have natural arithmetic zero and support the multiplication and division computations (e.g., elevation, height, distance, etc.).

18

The classifiers can be divided into metric and nonmetric classifiers based on the nature of the data values that these classifiers deal with. Metric classification algorithms, such as maximum likelihood and minimum distance to means classifiers, deal only with interval or ratio data values, and cannot be applied to nominal or ordinal data values. That is because the metric classification logic depends on computation processes to classify the data. On the other hand, classifiers based on nonmetric classification logic, such as rule based and decision tree classifiers, can be applied to all types of data values, as no computations are necessary. The metric logic can use either parametric or nonparametric algorithms. Parametric classification algorithms, such as the Maximum Likelihood classifier (Subsection 2.2.3.1), assume that the form of the underlying class density function is known and that the data are normally distributedsubsection. Conversely, nonparametric classification algorithms do not depend on the distribution of data, hence they can deal with data that are not normally distributed. Minimum Distance to Means classifier (Subsection 2.2.3.1) is an example of the nonparametric classification algorithms (Jensen, 2005). 2.2.3 Supervised and Unsupervised Classification Techniques

Classification can be categorized into supervised and unsupervised classification techniques. The supervised classification technique requires prior knowledge about the area to be classified, where the analyst defines the spectral characteristics of the information classes by identifying sample (training) data for each information class. The analyst then applies the selected classifier, based on the nature of the input data and the desired output, on the entire dataset to assign each element to one of the possible defined information classes (Bakker et al., 2001; Jensen, 2005). Conversely, the unsupervised classification technique does not require any knowledge about the area to be classified, where clustering algorithms can be applied to the data to partition the feature space into a number of clusters. The clustering algorithms produce spectral groupings based on numerical operations to satisfy certain similarities (i.e., natural grouping of the spectral properties). After partitioning the data into clusters, the clusters smaller than a threshold size are eliminated, and analysis of the clusters will take place by measuring the inter-cluster distances or divergence. Based on defined threshold values such as a minimum distance between the centres of clusters, a radius of the clusters, or a minimum number of elements in each cluster, certain clusters will be merged. The user, then, can assign each cluster to information classes of interest (Bakker et al., 2001; Jensen, 2005).

19

2.2.3.1

Supervised Classification Algorithms

There are a number of classification algorithms that can be used for image classification. Among the classifiers that are intensively used in remote sensing applications are Parallelepiped, Minimum Distance to Means, and Maximum Likelihood classifiers. The following subsections give a brief description of these three classifiers. (1) Parallelepiped Classifier

The Parallelepiped classifier is one of the simplest decision rules where the classifier depends on simple Boolean logic. Although this classifier is a simple and non-parametric algorithm (i.e., not restricted to a normal distribution of data values of each class), there are limitations of using this classifier. The main limitation of this classifier is that some elements may fall outside the parallelepiped of all classes, and others may fall on an overlapped area between more than one parallelepiped (Jensen, 2005). (2) Maximum Likelihood Classifier

The Maximum Likelihood classifier is based on the probability of an element belonging to each of the predefined information classes, and then it is assigned to the class for which the probability is the highest. The probability density function is calculated based on the spectral values of the sample (training) data selected for each defined information class. The Maximum Likelihood classifier is a parametric classifier that assumes a normal distribution of data values for each class in each band. The Maximum Likelihood classifier is one of the supervised classification techniques that intensively uses in the multi-spectral remote sensing data. This classifier overcomes the drawback of overlapped classes in the Parallelepiped classifier. However, the conditions of the normal distribution of the spectral data create some limitations on its use. (3) Minimum Distance to Means Classifier

The Minimum Distance to Means decision rule is a simple algorithm as is the Parallelepiped classifier. Yet it overcomes the problem of the overlapped classes as it depends on the shortest spectral Euclidean distance between the element and the spectral mean of each class (Jensen, 2005). The minimum distance to means classifier is nonparametric; therefore, it does not require normal distribution of the data of each class.

20

2.2.3.2

Unsupervised Classification Algorithms

The Unsupervised classification technique requires minimal interference from the user/analyst, and no knowledge about the area to be classified is necessary. Clusters for different classes are formed by performing numerical operations on data values that seek natural groupings of the spectral properties of the data elements (i.e., pixels or points based on the data structure). There are two approaches for clustering or segmenting the data elements, dependent on or independent of their spatial location. The following subsections illustrate these two approaches with some examples. i. Segmentation Independent of Data Location

The idea of segmentation, independent of data location, is based on gathering data elements, pixels or points that have the similar spectral characteristics, into segments regardless of their positions. Some of the unsupervised segmentation techniques are multidimensional that examine several

attribute/spectral values of the data elements at the same time and define the clusters, such as the Chain Method and the iterative self-organizing data analysis technique (ISODATA). Conversely, other techniques are one-dimensional that examine one attribute/spectral values at a time (such as the histogram-based segmentation and skewness balancing segmentation). The histogram based segmentation is one of the simple statistical analysis segmentation algorithms, which operates on each data element independently of its spatial location. This segmentation algorithm slices the histogram of a certain attribute/spectral value, e.g., the brightness values of a panchromatic image, based on the number of modes contained in the histogram. The normal distribution is uniquely described by its mean and variance (first two moments); however, the higher order moments (skewness and kurtosis) can be used to characterize the data distribution that is not normal (Stricker & Orengo, 1995; Liu et al., 2009). The skewness () value, the third order moment about the mean, represents the degree of distribution asymmetry around the mean, where zero value indicates a symmetric distribution. Negative skewness value means skewing of the data to the left and longer tail to the right, and conversely, positive skewness value indicates skewing of the data to the right with long tail to the left. The kurtosis () value, the fourth order moment about the mean, measures the relative flatness or peakness of the distribution about its mean; it describes how tall and sharp the central peak is. The normal distribution has a kurtosis value equal to 3. Values smaller than 3 indicate a flat and broader top than the normal distribution, and a value greater than 3 indicates a peak sharper than the normal. The skewness and kurtosis can be computed using the following formulas:
21

3  = ( × 3 ×  =1(( - ) )

1

1/ 3

(2-10)

4  = ( × 4 ×  =1(( - ) )

1

1/ 4

(2-11)

Where,     value into consideration; number of points; mean of the data; and standard deviation of the data values.

Where the mean  and the standard deviation  can be calculated using the following formulas:  =
1     =1 

(2-12)

2  =   =1( - )

1 

(2-13)

The distribution of data values do not always follow a normal distribution. As explained before, skewness and kurtosis can describe the characteristics of the data distribution. Therefore, some statistical analysis segmentation algorithms were introduced recently that are based on the higher order moments (skewness and kurtosis) (Bartels & Wei, 2006; Bartels et al., 2006; Bao et al., 2007; Yunfei et al., 2008; Liu et al., 2009; Costantino & Angelini, 2011; Crosilla et al., 2011; and Crosilla et al., 2013). Bartels and Wei (2006) proposed the "Skewness Balancing" segmentation algorithm as an unsupervised segmentation technique for filtering LiDAR point cloud data, to separate object points from terrain points. The proposed approach was based on the assumption of Duda et al. (2001), indicating that the naturally measured samples will lead to a normal distribution based on the Central Limit theorem, and hence, the elevation values of terrain points will follow a normal distribution (Bartels & Wei, 2006). The authors assumed that elevation values of object points may disturb the normal distribution of terrain points, and by removing object points, terrain points will have a normal distribution. Consequently, the skewness value will be equal to (or very near to) zero if object points are filtered out. The algorithm of the skewness balancing approach works on the values of either gridded or random (irregular) points. The procedure of the algorithm, as shown in Figure 2-5, can be described as follows: first, skewness value of the entire point cloud data is calculated. If this value is positive, the point with larger elevation
22

value will be classified as object and removed from the dataset. Then, the same step will be repeated on the unclassified points in an iterative approach. The process will be terminated when the skewness value that is obtained becomes equal to or very near zero. The remaining points, then, will be classified as terrain (Bartels & Wei, 2006). An application of this approach will be described later.

Start

Load LiDAR Point Clouds

Skewness > 0

N

Y Classify Maximum Point as Object

Save Ground and Object Points

End

Figure  2-5: Procedure of Skewness Balancing Algorithm (Bartels & Wei, 2006)

Yunfei et al. (2008) introduced another statistical analysis approach for segmentation of the elevation attribute values of LiDAR point cloud data. This approach depended on skewness and kurtosis change algorithms, where the skewness and kurtosis values of the points' elevation values were used in the segmentation. The skewness/kurtosis change curve is a plot of skewness/kurtosis values versus cycle numbers. Where at the first cycle the skewness/kurtosis value is calculated based on the entire point data, for each following cycle the point with the highest value is removed and the skewness/kurtosis value is recalculated until the data are exhausted. By plotting the value of skewness/kurtosis versus the cycle number, change curves can be drafted. An underlying assumption that the change curve of terrainonly points of a flat area will change smoothly, and an inflexion point will appear in the curve if the point data contain object points as well. Based on that assumption, Bao et al. (2007) considered the last
23

inflexion point of the change curve representing the dividing line between objects and terrain (Bao et al., 2007; Yunfei et al., 2008). Crosilla et al. (2011) introduced another approach that applies the change curve algorithm on LiDAR elevation and intensity data iteratively to separate points of a single segment (Crosilla et al., 2011; Crosilla et al., 2013). To apply this approach, the distribution of elevation and intensity values were analysed for each small area to decide which attribute value will start the process. The attribute value with distinguishable modes histogram, at least bi-modal, was considered first to separate certain segments. Then based on the histogram of the other attribute value of the remaining points, another iteration is conducted. These data analyses and separating iterations continue until all distinguished features are segmented. For example, Crosilla et al. (2011; 2013) segmented a selected area that contained an asphalt road in a flat, bare ground area into two classes (i.e., road and ground) based on the intensity attribute values. Nevertheless, when the area contained several features (e.g., ground, vegetation, road, and a roof top), the decision taken to start with the elevation values to separate the building, then the intensity values of the remaining points were considered to separate the road points. The elevation values of the remaining ground points were considered again to separate the terrain and vegetation areas (Crosilla et al., 2011; Crosilla et al., 2013). The results of this approach are described in Section 2.3.2.3. ii. Segmentation Dependent on Data Location

Segments are individual regions with shape and spectral homogeneity that can provide meaningful features to be classified. Not all available segmentation algorithms can consider both the spectral and spatial information (as described in the previous subsection). Nevertheless, to form an object, its elements (points or pixels) have to be spatially connected, thus, dividing data into objects based on the spatial characteristics as well as the spectral characteristics is preferred (Jensen, 2005). One of the few algorithms that consider both spatial and spectral information was developed by Baatz and Schäpe (2000) (Jensen, 2005; Im et al., 2008; Blaschke, 2010; Chen & Gao, 2014). This algorithm compares each data element (pixel or point) to its neighbours and computes spatial and spectral criteria of homogeneity/heterogeneity. This algorithm is a region merging technique, which starts by considering each data element as a region/object, and then it merges each adjacent pair of regions/objects into one larger region/object based on local homogeneity criteria (spatial and spectral criteria, as described below). The newly merged objects are compared and any adjacent pair of objects
24

that satisfy the homogeneity criteria are merged into a larger region/object. The process continues until all homogeneous regions/objects are merged (Baatz & Schäpe, 2000). 2.2.4 Decision Tree Classifier

The decision tree classifier, also called a hierarchical decision tree classifier, is a nonmetric and nonparametric classification technique that predicts class membership by recursively partitioning a data set into more homogeneous subsets. This procedure is followed until every data element is discriminated from the elements of other classes, with all pure terminal nodes or until pre-set conditions are met for terminating the tree's growth. This can be applied through identifying certain hypotheses, and the nodes evaluate the rules and conditions to test these hypotheses. The decision tree takes objects or situations, described by a set of attributes, as input and returns a decision (Hansen et. al, 2000; Jensen, 2005). 2.2.5 Multiple Classifier System

A multiple classifier system combines different classifiers to improve the achieved results. The main idea of the combined classifier is to rely on several classifiers (decision making scheme), and to improve the confidence of the decision made. This can be done by weighing various opinions (classifiers) and combining them through some thought process to reach a final decision. There are several forms of the multiple classifier system, such as ensemble based systems, hybrid classifiers, and decision combinations. The main two groups of combination rules are combining class labels, and combining continuous outputs (Polikar, 2006). If only the classification results (class names or values) are available, then the combining class labels rule can be used. As explained in Polikar (2006), for a number of classifiers , the classification decision of the tth classifier is ,  {0, 1}, where  = 1 . . . ,    = 1, . . .  (where  is the number of classifiers, and  is the number of classes), i.e., if tth classifier chooses class  , then , = 1, and 0, otherwise. The commonly used combination rules are the majority voting and the weighted majority voting (Polikar, 2006).
  =1 , = max=1  =1 ,   =1  . , = max=1  =1  . ,

Majority Voting Rule Weighted Majority Voting Rule

(2-14) (2-15)

25

Where,  ,  number of classifiers; classification decision of the tth classifier; weighting factor of the tth classifier.

If the continuous-valued outputs of individual classifiers are available, then combining continuous outputs rule can be used. These continuous values provided by the classifiers for a given class often represent the degrees of support the classifiers give to that class. The continuous-valued outputs of a class can be considered as a posteriori probability for that class if they are appropriately normalized to add up to 1 for all classes, and if the classifiers are trained with sufficiently dense data (Polikar, 2006). The confusion matrix method can be used to determine the accuracy of a certain classifier based on a number of reference points. The values of the confusion matrix for a certain classifier represent the degree of support given to each class by this classifier (Xu et al., 1992). The degree of support given to the classes is similar to the continuous-valued outputs of the classifier. If these values are normalized to add up to 1 for all classes, they can be considered as a posteriori probabilities of these classes. The general formula of the confusion matrix ( ) for a  classifier that is classified into  classes is given by Equation (2-16): 11  = 
() (1 ()

 
()

1 
()  )

()

(2-16)



Where each column  corresponds to the class  ,  = {1, 2, ... , } and each row  corresponds to the event of assigned class  to the point into consideration,  = {1, 2, ... , }. The  is the number of sample (reference) points that are originally in class  , but incorrectly assigned to class  . For  = ,  is the number of points that are correctly assigned to class  (Xu et al., 1992). These values are accepted as an estimate of the a posteriori probability for that class after scaling them to the [0, 1] interval (Kuncheva et al., 2001; Poliker, 2006), where the elements of each row/column had to add up to 1. The process of scaling the elements of the  to the [0,1] interval is called a normalization of the confusion matrix.
() ()

26

With the knowledge of the normalized confusion matrix  for a classifier , the uncertainty in the class  assigned to the point  into consideration can be described by the conditional probabilities that    ,  = 1, 2, ... ,  are true under the occurrence of the assigning event  () =  (Xu et al., 1992).

() ()

(   |  () =  ) =

 =1 

,  = 1, ... , &  = 1, ... , 

(2-17)

Where  is the number of reference points of class  that are assigned to class  with the classifier , and  =1  is the total number of reference points of class  (summation of each column of the ). The confusion matrix can be considered as prior knowledge of an expert. This expert has a belief value with uncertainty that the point , in consideration, belongs to the class  (for all classes), which can be expressed in the form of the conditional probability as (Xu et al., 1992): (   |  (), ) = (   |  () =  ),  = 1, ... ,  (2-18)
()

()

Where,  is the environment of the common classification environments that consist of independent events. Based on the Bayesian formula, when the classifiers are independent of each other, then the events 1 () = 1 , ...  () =  will be independent of each other under either the condition of    or the environment  , which leads to (as described in Xu et al., 1992): ( ) =   =1(   | () =  ) Where  is a constant that ensures that  =1  ( ) = 1 There are other rules that can be used for combining multiple classifiers of the continuous-valued outputs (Polikar, 2006). Among the rules to be used are the algebraic combiners: mean rule, weighted average, minimum/maximum/median rule, and product rule (Polikar, 2006). The following formulas summarize these algebraic combiners. Mean Rule Weighted Average  () =
1 

(2-19)

 =1 , ()

(2-20) (2-21)

 () =  =1 , . , ()

27

Maximum/Minimum/Median Rule

 () = max=1... {, () }  () = min=1... {, () }  () = median=1... {, () }

(2-22) (2-23) (2-24) (2-25)

Product Rule Where,  ,  

 () =

1    ()  =1 ,

number of classifiers; classification decision of the tth classifier; weighting factor of the tth classifier; total support received by class j.

2.3 Classification of LiDAR Data
Classification of LiDAR data, traditionally, has implied the separation of terrain point from other objects (non-terrain point) based on the elevation values of the LiDAR point cloud (Antonarakis et al., 2008; ElAshmawy et al., 2011). With the capability of LiDAR sensors to record the backscattered energy as intensity data, the traditional definition of LiDAR data classification has changed. The following sections contain some examples of using different classification techniques for land cover classification of LiDAR data. 2.3.1 Classification of LiDAR Range Data

Kraus and Pfeifer (1998) used LiDAR data to generate DTM in wooded areas by separating terrain from vegetation (non-terrain) points. The accuracy of the generated DTM was 25 cm in flat areas, and improved to 10 cm accuracy by removing the systematic errors in the elevation data. A filter, which is based on an iterative linear prediction algorithm, was used to filter out the vegetation points. This algorithm worked well even in areas with low penetration rates (penetration rate for the laser signals within the study area was around 25%), and the number of iterations was between three and four. It was concluded that an automatic classification of the laser points into terrain and vegetation points was possible using special filtering. Moreover, the generated DTM from laser scanner data in wooded areas had an accuracy equivalent to DTM in open areas derived from 1:7000 scale aerial images. However,

28

both techniques (photogrammetry and laser scanner) were not able to detect break lines automatically (Kraus & Pfifer, 1998). Haala and Brenner (1999) combined LiDAR elevation data with a 1:5000 multi-spectral aerial image, with green, red and NIR bands, for building extraction. A mathematical morphological model using the approach described in Weidner and Förstner (1995) (as cited in Haala & Brenner, 1999), was followed to generate an approximate DTM. A normalized DSM, which represented the object heights, was extracted by subtracting the derived DTM from the DSM that was generated from the LiDAR elevation data. The NDSM was used as an additional source of information to improve the classification results. A pixel based unsupervised classification technique, ISODATA, was used for classification. Five classes were obtained: shadow, building, tree, grass-cover-area, and street. It was found that combining LiDAR elevation data with coloured-infrared aerial image improved the classification results. For 3D building extraction, the DSM (produced out of LiDAR data) was combined with existing ground plans of the buildings. It was assumed that the plans were correct and defined the borders of the roofs exactly. Although no quantitative assessment of the results was done, it was concluded that detailed reconstructions of buildings can be automatically obtained using the DSM generated from the laser data. It was also concluded that using laser data in automatic generation of urban databases was strongly recommended (Haala & Brenner, 1999). Ma (2005) extracted and regularized buildings from LiDAR data using the regression planner surface segmentation method. This method was used to separate the homogeneous planes that were representing ground, roads, and roofs within the study area. The planner surface fitting algorithm was implemented after converting the data into a raster grid with 1 m pixel size. An underlying assumption of the work was that in urban areas, the ground is flat, so it can be treated as a planar surface. A 30 cm elevation difference was selected as a threshold (double the vertical accuracy of the used LiDAR data) between any pixel and its eight neighbouring pixels (in window 3x3 pixels) to detect the planar surfaces. To separate the ground surfaces from the roofs of buildings, another assumption was made that the ground surfaces are connected and have areas greater than the largest size of buildings. Hence, the surfaces with area greater than the maximum building area (defined based on prior knowledge of the study area), were considered as ground. A DEM, then, was generated from the ground pixels, and refined by comparing the elevation of the DEM and the objects' height. The object pixels that had a height difference less than a threshold of 30 cm were considered as ground points. On the other hand, ground pixels that had elevation values greater than the DEM elevation by more than 30 cm were
29

considered object pixels. This refinement process was repeated several times until no significant number of ground pixels was added to the DEM. After producing the DEM, a normalized DSM was generated and used to detect the high objects like trees and buildings. The low objects (3 m was selected to differentiate the high and low objects) such as cars and shrubs were detected as well. Based on the assumption that buildings have planar roofs, while trees do not, the generated planar surfaces were used to separate buildings and trees. The last step was building boundary regularization, where the boundaries of the buildings were regularized. For evaluating the classification results, and because of the lack of ground truth, the classified data were compared visually to the interpretation of the DSM that was generated from LiDAR data. Buildings that were intersected with trees or covered by trees were excluded from the evaluation step. The accuracies of the detected buildings were 80­93% for two study areas (Ma, 2005). Bartels and Wei (2006) proposed a "Skewness Balancing" statistical analysis technique to filter LiDAR point data to separate object and ground points based on elevation values (see Section 2.2.3.2 for a detailed description of the proposed technique). The proposed technique was applied to simulated data and verified on real data. The real data were two different urban areas with mixed detached objects (buildings and vegetation of different heights) and various attached objects (bridges, and motorways junctions). The detached objects were detected, yet not all the attached objects were. The results were displayed clearly; however, for the accuracy assessment a small area of 0.019 km2 was selected. An overall accuracy of about 96% was achieved (Bartels et al., 2006). The achieved accuracy is pretty high, which is due to the small size of the area that was used for assessment. Furthermore, no details are included about how the accuracy was assessed. In Bartels et al. (2010), the skewness balancing algorithm was extended to include terrain separation for hilly areas. This method works iteratively on the non-terrain points to re-separate the point data into terrain and non-terrain points. The step was repeated until the number of points reached a minimum number of points defined based on the characteristics of the data and an accepted error margin. Before starting the separation process, the area had to be classified as hilly or moderate terrain area using a linear regression analysis. A training set of 44 known LiDAR tiles collected from all over the world was used to derive the regression coefficients. The regression model was assessed using the cross-validation method, and 91% accuracy was achieved in classifying areas into hilly or moderate terrain areas (Bartels & Wei, 2010).

30

Bao et al. (2007) introduced another algorithm based on the statistical analysis segmentation technique for separating terrain points of LiDAR point cloud data. This algorithm was contingent on the changes in skewness and kurtosis values, where change curves of the skewness and kurtosis of LiDAR elevation values were plotted and used in the segmentation (Subection 2.2.3.2). Bao et al. (2007) considered the last inflexion point of the change curve as the separation between objects and terrain points. The authors applied this approach to three different areas: 1) a city area with densely packed buildings with vegetation areas, 2) a city area with railway station and buildings, and 3) a forest area with steep sloped terrain and vegetation on riverbanks. It was shown that on the flat areas with objects of various elevations, the inflexion of the kurtosis change curve was clear and used to separate the objects. When there were objects with different elevations, as in the second area, more than one inflexion point appeared in the kurtosis change curve. In the case of sloped terrain as in the third area, it was not easy to determine the inflexion of the curves. The authors concluded that the kurtosis change curve was better than the skewness change curve for object/terrain separation, and that this approach was not fit with sloped areas. All the results, in this research, were evaluated visually and no quantitative assessment was obtained. 2.3.2 Classification of LiDAR Range and Intensity Data

With the capability to record the intensity of the reflected energy, intensity data were included in the classification process of LiDAR data. Using various classification techniques, intensity data were investigated to be used to distinguish different target materials as the laser signals that are utilized in LiDAR systems are in the NIR wavelength. Some examples of including LiDAR intensity values in the classification process of LiDAR point cloud data are depicted below. 2.3.2.1 Pixel-Based Classifiers

Hui et al. (2008) used LiDAR intensity and elevation data for land cover classification. First, terrain points were separated from non-terrain points using a filter algorithm on the elevation data, and no information about the filter was provided. Secondly, the separated points were classified using a supervised classification technique that was applied to the intensity data to differentiate four classes of the separated points: tree, building, bare earth, and low vegetation. It was concluded that combining the intensity data with the height data was an effective method for LiDAR data classification. However, the classification algorithm was not explained, and no quantitative accuracy assessment was included in that research.
31

Other research combined LiDAR data with other auxiliary data such as multispectral aerial photos or satellite images, USGS DEM, texture data, and multiple-returns data. Charaniya et al. (2004) used elevation data, intensity data, height variation data, and multiple-return data of LiDAR, luminance data of a panchromatic aerial imagery, and USGS DEM (to extract normalized height data) for land cover classification. USGS DEM was subtracted from the DSM to generate normalized height data. The height variation data were determined by calculating the difference between the minimum and maximum height values within a window of 3x3 pixels. It was expected that there would be significant height variation in the high vegetation areas. The difference between the first and last returns of elevation data was included. A supervised parametric classification algorithm, Expectation Maximization, was used to distinguish four classes: trees, grass, roads, and roofs. Different band combinations were investigated, and the overall accuracy ranged between 66 and 84% depending on the band combination that was used in the classification. It was concluded that height variation improved the classification results of the high vegetation areas; luminance and intensity data were useful for distinguishing roads from low vegetation areas; and the multiple-return differences slightly improved the classification of roads and buildings but reduced the accuracy of the other classes. A rule-based approach for LiDAR data was presented by Bartels et al. (2006) to improve the accuracy of the classification obtained by using supervised Maximum Likelihood classification. First and last echo DSM, and intensity data were fused with other co-registered bands--aerial image (RGB) and near infrared image (NIR). Some rules were added based on pre-known characteristics of the features in the study area (area of buildings, cars, size of the smallest object, etc.). Four classes were identified: buildings and sheds, vegetation (including trees and low vegetation), cars (as can be spotted in the high resolution dataset of 0.5 m pixel size), and ground (top layer soil, thin man-made layering). The accuracy that was obtained varied based on the bands used in the classification. The overall accuracy of using the Maximum Likelihood classifier with intensity and height data were 36 and 51%, respectively. Combining the two data sets improved the overall accuracy to 62%. The accuracy that was obtained reached 71% when the first and last echo data were used. Using the co-registered data improve the accuracy by 13­ 33%, it was concluded that the LiDAR data (intensity, first and last echo bands) improved the detection of low vegetation and grass. By applying the rule-based approach, where some criteria were set based on the characteristics of the study area, the classification accuracy was improved. The classification accuracy of the buildings, vegetation and ground ranged between 81.91 and 89.88%, and the user's and producer's accuracies of the cars class improved by 5 and 32%, respectively (Bartels et al., 2006). It is

32

observed from the quantitative assessment that the used number of pixels in the confusion matrix is more than 75000 points, over an area of 0.019 km2, which are most likely the entire dataset. The area of 0.019 km2 is too small to consider the obtained accuracy as a generic assessment of the used classification technique. 2.3.2.2 Object-Based Classifiers

The previous section depicts some examples of the studies that investigated the classification of LiDAR data using supervised and unsupervised pixel-based classifiers for distinguishing land covers. Nevertheless, these pixel-based classifiers might not be efficient with high resolution images, according to Blaschke (2010). Object-based classification using image segmentation was proposed for land cover classification of the high spatial resolution imageries. LiDAR intensity images can be considered as high spatial resolution images, especially for the dense data that are more than one point per square metre. The object-based approach for LiDAR intensity data classification has been investigated in several studies. Brennan and Webster (2006) used a rule-based, object-based classification approach for distinguishing ten classes of land covers in an urban coastal area. This coastal area included saturated and non-saturated intertidal sediments of the wetland, saturated or stressed and lush ground cover vegetation, low and tall deciduous and coniferous trees, roads and bare soil, bright-roofed structures, dark-roofed structures, and water. The classification approach was applied to five different layers generated from LiDAR data. DSM, DTM, normalised height, intensity, and multiple return data were used in the study. A data supplier provided the ground and non-ground data as well, and an orthophotography was used for collecting the ground truth. The overall accuracy of the ten classes was 94% and increased to 98% when the classes aggregated to seven classes. The authors used eCognition software for objects recognition. That software uses an Object-Oriented rule-based approach based on the algorithms developed by Baatz and others for objects separation (Brennan & Webster, 2006). It can be observed that the rule-based classification approach used in this study is very sophisticated and adopted for this particular area. Antonarakis et al. (2008) followed a different object-based approach for land cover classification in natural and planted forests. That approach used point distribution frequency criteria; skewness and kurtosis, as additional information layers to differentiate land cover classes; water, short vegetation, bare earth, gravel bars, and trees with different ages in floodplain areas. There were six information layers that were used in this study: 1) vegetation height model (VHM), which was the difference
33

between minimum and maximum elevation values within each 5m cell. 2) Percentage canopy hits model, (PCM) within 10 m cell size. 3) Intensity model (IM), average intensity within 5m cell size. 4) Intensity difference model (IDM), the difference between the first pulse minima and the last pulse maxima within each cell. 5) Skewness model (SkM), the skewness of the elevation values of each 5 m cell size; and 6) Kurtosis model (KrM), the kurtosis of the elevation values of each 5 m cell size. A decision tree was used to differentiate between the eight classes. This study verified that a combination of intensity and elevation LiDAR data can be used for multiple land cover classification in a forest area. The commission and omission errors were considered for assessing the classification results. It showed that high accuracy of 95% can be achieved for forest areas (Antonarakis et al., 2008). However, good analysis of the data for each class had to be done first to set the suitable criteria for separation. Moreover, a well-detailed decision tree is required for each area. Hu et al. (2004) have integrated LiDAR data and aerial photography to automatically extract roads in dense urban area. They used a segmentation technique to distinguish roads in both datasets (LiDAR data and the aerial photography). The height (elevation) data was used to separate low areas (roads, parking lots, and grass) from elevated objects (trees, buildings, and bridges). The iterative Hough transform was used to determine the road strips, and a grid was formed by removing the incorrect segments. Vehicle detection was used to separate parking areas from grassy areas; and the shape of large areas was used to exclude parking areas from the roads network. The work described in this paper indicated that involving multiple source of information improved the extraction results in complicated scenes. However, no quantitative evaluation of the classification results was included in this study. 2.3.2.3 Statistical Analysis Classifiers

Yunfei et al. (2008) introduced the skewness change curve algorithm to separate terrain from nonterrain points of LiDAR point cloud data. For flat areas, the skewness algorithm was applied to the elevation values to separate objects and terrain points. For mountainous areas, they improved the skewness algorithm to separate terrain and vegetation points by applying the skewness algorithm on the intensity values of the LiDAR data. Using visual assessment, the classification results were evaluated. To assess the terrain separation results, a cross section was sketched from the point cloud data before and after applying the algorithm. That cross section went through flat and mountainous areas with high and low vegetation. The cross section of the data after separation show the flat area with no vegetation points, but in the hilly area the points were of terrain, low vegetation, and few points of high vegetation.
34

Crosilla et al. (2011) introduced a new sequential procedure based on skewness and kurtosis change curve algorithms to classify LiDAR point cloud data. This procedure studied the skewness and kurtosis values of elevation and intensity values of LiDAR point cloud data, iteratively. They made an assumption that the values of a homogenous class in a LiDAR dataset is expected to follow a normal distribution. Therefore, the density function of both attribute values (intensity and elevation) had to be analysed at the beginning to decide the attribute value that would be considered first, which usually is the attribute value with bi or multi-mode distribution. The points with higher attribute value than the maximum kurtosis point, where the local maximum in kurtosis change curve was, were separated in a cluster. Successive clusters were identified by applying same iterative procedure on the unclassified points. That procedure was applied to LiDAR data of two small areas, with point density of 12 pts/m2. The first selected area contained small part of an asphalt road in a flat bare ground area. Therefore, the first experiment was for road extraction where the kurtosis change curve algorithm was applied to the intensity values to extract road points out of other ground points. Then the algorithm was applied to the elevation values of the extracted road points to refine the results. Refining the results was applied because there were some points with higher elevation, and similar intensity was classified incorrectly as roads. The second area was also a small flat area, but it contained several features--ground, vegetation, road, and part of a roof. In this experiment based on the observation of the elevation and intensity histograms, a decision was made to begin with the elevation values for separating object points. Based on the kurtosis change curve approach, the points of the building were separated from the ground points. Then the classified points as ground were classified again using the kurtosis change curve based on the intensity values to extract the roads. After that the elevation values of the remaining ground points were analysed again to be clustered into vegetation and ground points. The two described experiments showed that this procedure works well with small areas. For quantitative evaluation, the classified data were compared to a manual classification of same areas. The total error, which is the percentage of the misclassified points to all points, was considered. The total errors in the first and second experiment were 5.6% and 1.2%, respectively (Crosilla et al., 2011). It was noticed from these experiments that the algorithm was applied to very small areas with few features. Crosilla et al. (2011) introduced a method for partitioning large areas into regular subsets to enable and simplify the classification process toward improving the kurtosis change curve procedure to make it applicable for large and/or complex areas. The proposed partitioning method was applied to two complex large areas. The first area consisted of two flat areas at different elevations that were

35

connected by a sloped terrain covered by vegetation. The second area was a large area (around 10 ha) that contained a sloped terrain covered by buildings and vegetation, and flat area covered by buildings, roads, and complex vegetation. Therefore, based on the distribution of the intensity and elevation values of the points, the area was partitioned into four sub-areas. Whenever there was a multi-mode distribution of the data values, the sub-area was partitioned into four sub-areas. Then the kurtosis change curve approach was applied to the final partitions. To evaluate the classification results, the results were compared to manual classification of the same area. The total errors in the classification were 8.9% for the first complex area and 1.4% for the second complex area (Crosilla et al., 2011; Corsilla et al., 2013). By studying the classification techniques used for LiDAR data, it can be noticed that the pixel-based classification logic is still under investigation. Adding auxiliary layers improves the classification accuracy of LiDAR. It was observed that the point-based classification was used only with the statistical analysis classification approaches. Nevertheless, the statistical analysis classification was used to separate the terrain from the non-terrain or for separating certain classes. No automatic classification approach based on the point data was previously used. Therefore, the methodology introduced for this research focused on investigating the pixel-based classification with adding more layers extracted from LiDAR original data to improve the classification results of LiDAR data only. The methodology included two proposed point-based classification approaches for the 3D point data without converting them into 2D grid.

36

3. Methodology
3.1 Overview
This chapter describes the methodology that was used to achieve the research goal of developing an approach for land cover classification using LiDAR elevation and intensity data. The methodology consists of three parts as represented by dashed rectangles in Figure 3-1. It is noteworthy that the methodology described through this chapter is a generic methodology. Terms and parameters required for applying the methodology will be specified during the explanation of the experimental work. Part-1 investigated the image classification techniques where LiDAR point cloud data were converted into raster images, and then image classification techniques were applied. Pixel-based and object-based classification logics were investigated. For the pixel-based logic, one of the image classifiers was used. While for the object-based logic, a decision tree was developed to convert the image data into surfaces and classify these surfaces. The developed decision tree was designed to apply specific rules on both elevation and intensity LiDAR data to separate the data into distinguished land cover classes. The criteria used for classification were expressed out of the characteristics of each class as distinguished in the study area. Moreover, several layers were derived from the intensity and elevation images and included in the classification to improve the classification results. Point-based classification logic was followed in Part-2 and Part-3. Two different approaches were developed to classify LiDAR point cloud data without converting the data into raster images. In Part-2, the developed approach classified the point data using a supervised classification technique based on the elevation and intensity attribute values of the LiDAR point cloud data. Based on data acquisition mission planning methods, large areas are scanned from several flight lines with side lapping around 20­ 30%. That leads to having several data-strips with different acquisition characteristics for the same area. All acquired data had to be considered in land cover classification processes. Therefore, the point-based classification was conducted on both single and multiple data-strip(s). Various data combination approaches were introduced for combining the multiple data-strips in the overlapped areas. A new technique, Combined Multiple Classified Datasets (CMCD), for combining the classified data is introduced. In Part-3, the point data were classified using a new approach that depended on kurtosis change curve segmentation algorithm based on the attribute values of LiDAR point cloud data. The new classification
37

approach consisted of two parts; first, the point cloud data were segmented using the new segmentation technique. This segmentation technique was applied to the attribute values in consideration one-by-one. Then, second, the produced segments were classified using a supervised classification technique, where each segment was treated as a single object to be assigned to one of the predefined distinguished classes.

Point Cloud Data

Part-3
Raster Image Classification Point-Based Data Classification

Pixel Based

Object Based

Supervised Classifier

Kurtosis Change Curve Classifier

Auxillary layers Elevation and extracted from intensity layers elevation and Intensity Using Developed Decision Tree

Single DataStrip

Multiple DataStrips

Classification of Combined Data

Combining the Classifed Data

Merging Classified Data

Combining Data using CMCD

Part-1

Part-2
Figure  3-1: Methodology Flow Chart

Using point data in Part-2 and Part-3, LiDAR footprints have irregular spatial distribution because of the physical and geometrical characteristics of the ground targets as well as sensor and acquisition characteristics. Therefore, gaps appear between LiDAR footprints and, consequently, classified point data were expected to have irregular spatial distribution and gaps would appear within the classified points. For obtaining classified data for the whole area without any gaps, and for a reliable comparison

38

between the classification results of the three reported methods, a rectangular grid space was defined. The extent of the grid space in    directions represented the boundaries of the study area. A distance , which denoted the pixel size of the raster grid and the spacing between grid points in point grid spaces, was selected based on the point density and the footprint of the original LiDAR point cloud data in the way that the grid point density will be equal to the average original point density, and the spacing between the grid points will be equivalent to the size of the footprint. All attribute values were converted into images with pixel size  for the raster grid. However, for the point data format, the defined grid points, with spacing  in both directions, were used to fully populate the study area regularly, and to fill the gaps between the LiDAR footprints. Consequently, the different classification cases would be spatially coincided and their results could be compared. It is worth mentioning that this research was conducted on point cloud LiDAR data that were geometrically calibrated and radiometrically corrected (provided in , , and  coordinates, and intensity1, ( ) values, in ASCII format file). Each of the three parts is described in more detail in the following subsections.

3.2 Raster Image Classification (Part-1)
Part-1 is the classification of LiDAR data using one of the existing image classification techniques, where LiDAR point data, first, were converted to a predefined raster grid space. Then, the raster data were classified using either pixel-based or object-based classification logics, in Figure 3-1 (the blue dashed rectangle; Part-1). Usually, LiDAR data have two attribute values that can be considered in classification: elevation values () and intensity values (). Raster grids (images) were produced based on each one of these attribute values. The Brightness Values (BVs) of the produced pixels were determined by resampling and interpolating the point data into the defined raster grid space. The Kriging interpolation technique was used to convert LiDAR elevation and intensity attribute values into raster grid images. The elevation values of LiDAR point clouds ( coordinates) were interpolated and a digital surface model (DSM) was produced. By interpolating the intensity values of the LiDAR point clouds ( values) an intensity image was produced. By converting both elevation and intensity point data

1

In this research, the term "intensity" denotes the radiometric corrected intensity.

39

into raster grids, the gaps between LiDAR footprints were filled and the produced images, intensity, and DSM were fully populated. Two classification logics, pixel-based and object-based, were examined and applied to the raster images (intensity image and DSM). 3.2.1 Pixel-Based Classification

A supervised classification technique was conducted to investigate the possibility of producing land cover classes from LiDAR intensity and elevation data. From the literature, adding auxiliary data such as aerial photos or satellite images to LiDAR data improved the accuracy of the land cover classification results (Hala & Brenner, 1999; Charaniya et al., 2004; Bartels et al., 2006). However, the focus of this research is on using LiDAR data (elevation and intensity values) independently of any other sources of data. Other auxiliary layers that could be extracted from LiDAR data may be included in the classification process. The workflow of the pixel-based classification is divided into four steps: data preparation, layer combination, data classification, and results evaluation. Figure 3-2 illustrates the workflow of the pixelbased classification process with its four steps. The details of the workflow steps are described in the following subsections. 3.2.1.1 Data Preparation

Point data in (, , , and ) values were used to form three different files representing intensity, elevation, and terrain points data. The intensity data file contained (, , and  ) values, the elevation points file contained (, , and ) values for all points, and the terrain points file contained (, , and ) values for the terrain points only. These three files were converted into raster format (images) and interpolated to produce intensity image, digital surface model (DSM), and digital terrain model (DTM). The DSM was generated by interpolating and resampling the elevation values of LiDAR point cloud data. In the case of single return data, the DSM could be produced from the elevation values ( coordinates) of the entire dataset. Yet in the multiple return data, only first return data should be used, whereas to generate the DTM, the elevation values ( coordinates) of the last return points could be resampled and interpolated. While in the case that the multiple return LiDAR data were not available, some LiDAR points on the ground (such as roads, bare soil areas, and grass areas) could be collected to represent the terrain surface. The collected points then would be resampled to the predefined grid space and interpolated to produce the Digital Terrain Model (DTM).

40

Read LiDAR Point Cloud Data

Separate Data into intensity and elevation points

Intensity points (, , )

Elevation points (, , )

Terrain points (, , )

Data Preparation

Convert Point Data to Raster Grid

Intensity Image (Intensity)

Digital Surface Model (DSM)

Digital Terrain Model (DTM)

Subtract DTM from DSM

Normalized DSM Surface (NDSM)

Determine the Variation of Intensity values

Determine the Variation of Elevation values

Intensity Texture Image

DSM and NDSM Slope Images

Layer Combination

Combine Layers

Multi-layer images

Image Classification

Classify the Multi-Layer images

Classified Images

Results Evaluation

Assess the accuracy of the classification results

Figure  3-2: Workflow of Pixel-Based Classification of LiDAR Data

41

Adding other layers to the original LiDAR data layers was expected to improve the classification results. Yet, since the focus of this research was on the LiDAR data independently, the auxiliary layers that may be added were extracted from the LiDAR data itself. Three auxiliary layers were extracted within the data preparation stage: the texture of the intensity, the normalized digital surface model NDSM, and the slope of the elevation models. The height of objects above the terrain was expected to improve the classification results, which can be represented by the NDSM. The NDSM was generated by subtracting the DTM from the DSM. Another extracted layer from elevation data that was expected to enhance the accuracy of the classification results was the slope of the DSM or the NDSM. The slope of both the DSM and the NDSM were generated based on the differences between the elevation value of each pixel and its neighbours for both the DSM and the NDSM. The texture of the intensity data was also extracted as an auxiliary layer to represent the homogeneity of object materials on the ground. The texture of the intensity layer was generated based on the variation in the intensity values among each pixel and its neighbouring pixels in a 3x3 window. 3.2.1.2 Layer Combination

The second step was the layer combinations, where two or more layers were combined into one multilayer raster grid (image). The investigated multi-layer combination cases are as listed below. The orthorectified aerial imagery was included as another case for comparison reason. 1) Aerial Photo, 3) Original Intensity, 5) Intensity and DSM, 7) Intensity, DSM and Texture, 9) Intensity, DSM, Texture and DSM Slope, 2) DSM, , 4) Intensity 6) Intensity and NDSM, 8) Intensity, NDSM and Texture, 10) Intensity NDSM, Texture and NDSM Slope

3.2.1.3

Image Classification

In the third step, classification of the various layer combinations was executed. ERDAS Imagine software was used to apply one of the existing supervised image classification techniques. The classification process started with the selection of training sites for each of the distinguished classes. A training site was identified wherever there was a variation of the pixel brightness values within the class. Data distribution of the selected training sites was tested to ensure that it followed a normal distribution if a

42

parametric classification algorithm would be applied. Then one of the existing classification algorithms was applied to each pixel of the raster images and the appropriate class was assigned to the pixel into consideration. 3.2.1.4 Results Evaluation

The fourth and last step was the results evaluation, where the classifications results were assessed. A number of reference points that were randomly selected and well-distributed over the study area were used to evaluate the classification results using the confusion matrix approach. Finally, the accuracies that were achieved from the classification results of the different layer combinations were compared to each other to conclude the best combination of layers, and to determine which layer would affect the classification results. 3.2.2 Object-Based Classification

The main idea of object-based classification logic is to divide the data into objects and then classify them into distinguished classes based on certain criteria for each class. To apply the object-based classification logic, a decision tree was developed to divide LiDAR data into the distinguished classes. For each study area, the characteristics of each class should be studied to set the criteria for separating each class. For urban areas, examples of the expected classes might have the following characteristics: Class 1 (Houses): homogenous intensity surfaces, and are elevated above the terrain. Class 2 (Trees): heterogeneous intensity surfaces, and are elevated above the terrain. Class 3 (Roads): homogenous intensity surfaces, and are attached to the terrain. Class 4 (Grass and bare soil): heterogeneous intensity surfaces, and are attached to the terrain.

A decision tree was developed in this research to separate the distinguished classes. The developed decision tree followed three steps. a) It started with data preparation, b) followed by classification Level1, and finally, c) classification Level-2, as shown in Figure 3-3. In the data preparation step, the intensity and elevation data were interpolated and resampled to the pre-defined raster grid space (i.e., image). The intensity image was then segmented into homogenous regions, and a NDSM was generated from the DTM and DSM. The raster segmented intensity image data were divided into homogenous and heterogeneous surfaces, and the NDSM were divided into terrain and non-terrain surfaces producing four Level-1 clusters. Then, the Level-1 clusters intersected to produce Level-2 classes based on the mentioned criteria.
43

Data Preparation

Level-1 Classification
Homogeneous Surface Interpolating intensity points Segmentation of Intensity Image Area > minA

Level-2 Classification
Homogeneous Non-Terrain [Class1] Heterogeneous Non-Terrain [Class2]

Intensity Points (x, y, I) LiDAR Point Clouds Data (x, y, z, I) Elevation surface &terrain Points (x, y, z)

Heterogeneous Surfaces Interpolating Surface points (DSM) Area < minA

Non-Terrain Generation of NDSM Selection and Interpolation of Terrain Points (DTM) Terrain NDSM = MinH NDSM > MinH

Homogeneous Terrain [Class3]

Heterogeneous Terrain [Class4]

Figure  3-3: Developed Decision Tree for Object-Based Classification of LiDAR Data

Data Preparation: The intensity raster image was segmented into regions to separate the intensity data into homogenous and heterogeneous surfaces. Each region consisted of neighbour pixels that fulfilled a certain homogeneity criterion. This criterion can be specified based on the characteristics of the dataset. To separate the elevation data into terrain and non-terrain surfaces, a normalized digital surface model (NDSM) was generated by subtracting the DTM from the DSM. A minimum value of the object heights can be used to separate the non-terrain objects based on the existing features and types of the land cover. Level-1 Classification: The size of each region was used as the homogeneity criterion to separate the segmented regions into homogenous and heterogeneous surfaces. The minimum region size of the homogenous surfaces (minA) could be selected according to the characteristics of the study area. For elevation data, the values of the NDSM were used to separate the surfaces above the terrain, where any pixel has a value greater than the defined minimum object height (MinHob) can be considered as nonterrain surface. Level-2 Classification: Each intensity Level-1 cluster was intersected with each elevation Level-1 cluster, as shown in Figure 3-3. As a result, four types of objects were produced: homogeneous non-terrain,

44

heterogeneous non-terrain, homogeneous terrain, and heterogeneous terrain. These objects, then, can be assigned to Level-2 classes based on the described class characteristics.

3.3 Point Data (Point-Based Logic) Using Existing Classification Algorithm (Part-2)
LiDAR data are irregularly distributed, and more than one point may have almost the same planimetric position (,  coordinartes) with different attribute values (elevation,  and intensity,  ). When these points are resampled to a regular grid to produce an image, several points may be located on the same grid position, and only one attribute value has to be associated with this grid position. Consequently, the value of the resampled point will be either one of the original values, or the average of all point values within the area that is represented by this grid point. That will cause information losses for the rest of the points. Therefore, the classification results of LiDAR point data, which depend on the original attribute values of these points ( and  ), are expected to be more accurate than the classification results of raster image data converted from the same points. Afterwards, to produce a land cover map, the classified point data can be converted into raster maps. The second part of the methodology, which is defined by the green dashed rectangle of Part-2 in Figure 3-1, investigated the point-based classification logic for land cover classification of LiDAR point cloud data. MATLAB code was developed to perform a supervised classification of LiDAR point cloud data based on the intensity and elevation attribute values using one of the existing classification algorithms. After that, to produce a full, populated land cover image of the study area, the classified points were resampled to the pre-defined grid space, and the grid points that were covering the gaps between the resampled points were assigned to the appropriate land cover classes. Two new approaches for filling these gaps and assigning grid points to the appropriate classes were introduced in this part (Subsection 3.3.2.1). As mentioned in the introduction of this chapter, airborne LiDAR data are usually acquired from different flight trajectories in a number of strips. Combining the data of different data-strips is required to get the benefit of these dense data. Data collected in overlapped areas between different strips has discrepancies in their intensity values due to changes in flying altitude, attitude, and sensor scanning angle. These discrepancies in intensity values will affect the classification accuracy of the data. Radiometric correction of the intensity data may help to homogenize the intensity recorded in different strips. However, some discrepancies might still affect the classification process.
45

Figure 3-4 illustrates an example of a small part of a real dataset in an overlapped area. Different methods were introduced in this section to include data of multiple data-strips in the classification process. Multiple data-strips can be combined before or after the classification. When data were combined before classification, the classification process was performed on only one group of point data acting as a single data-strip in the classification process. A new combination approach was introduced in this research to combine the classification results of multiple data-strips when each data-strip was classified separately. This new approach is called the "combined multiple classified dataset" (CMCD). The CMCD approach was developed based on the concept of the Combined Multiple Classifier (CMC) technique. The CMC was introduced in different pattern recognition research fields, where particular data were classified using different classification algorithms and the results of the classification were combined based on the accuracy of each classifier (Xu et al., 1992; Polikar, 2006; Yan & Shaker, 2011). The CMCD combined the classification results of different data-strips based on the a posteriori probability of each class of the classified data and an inverse weighted distance factor (that was determined based on the distances between each grid point to be classified and the original LiDAR points of each data-strip).

a LiDAR Point Data of First Strip

b LiDAR Point Data of Second Strip

c LiDAR Point Data of the Two Strips Figure  3-4: Sample of Point Clouds Data of an Overlapped Area

The general workflow for single and/or multiple data-strips consisted of three stages as shown in Figure 3-5, first a data preparation stage, then a classification stage, and finally an evaluation stage. The data

46

preparation stage included distinguishing land cover classes of the study area and defining the class values, selecting the training sites to be used in the supervised classification, generating grid points covering the study area, and generating reference points and collecting their ground validation. The second stage was the classification stage, which consisted of two parts--Stages 2A and 2B (Figure 35). Stage 2A was applicable for all point data-strips (either single or multiple data-strips), where original points of each data-strip were classified and resampled to the grid space, and the generated grid points were assigned to the appropriate classes. Stage 2B, which represented the CMCD, was applicable to the multiple data-strips only, where it was applied to the classification results of each data-strip. Stage 2B, included calculating the a posteriori probabilities of each class for each data-strip, determining inverse distance weighting factors, and combining the classification results. The third stage was the evaluation of the final classification results where the ground validation points were used to assess the final classification results.

Stage 1

Stage 2

Stage 3

Data Preparation Stage
Defining Classes Selecting Training Sites Defining Grid Space Collecting Ground Validation Data

Classification Stage
Stage 2A
Classiying the original point clouds

Evaluation Stage
Locating Reference Points Determining Classes of Reference Points Form Confusion Matrix

Assign classes to the grid points

Stage 2B
Determining the "a posteriori Probability"

Determining the "Inverse Distance Weighting"

Combining the classification results

47

Figure  3-5: Workflow of Point Data Classification

3.3.1

Data Preparation Stage

The data preparation stage consisted of four steps: Defining Classes: According to land cover types of the study area,  distinguished classes were defined as {1 , 2 , ... ,  }. Selecting Training Sites: Based on the location of the distinguished land cover types (classes) within the study area, several sites for each land cover type were selected to be used as training information for the supervised classification processes. Defining Grid Points: Because of the irregularity of the spatial distribution of LiDAR points, and the gaps between LiDAR footprints, a rectangular grid space  was defined and grid points were generated.  represented the   point on the rectangular grid space , where  = 1, 2, ... ,  and  is the total number of points in the rectangular grid space . Collecting Ground Validation Data: Two sets of reference points were randomly selected, one from the irregular original points and another one from the regular grid points. These reference points should be well distributed over the study area. The outputs of this stage were ASCII files; a file for each data-strip, contained the data of the original LiDAR points, (point , , , , , and the classes of the reference points  , where  = 1, 2, ... , ). Another file contained the data of the grid points (point , , , and  the classes of the reference points, where  = 1, 2, ... , ). 3.3.2 Classification Stage

The classification stage was divided into two parts; Stage 2A and Stage 2B. In Stage 2A original point cloud data of each data-strip were classified into the distinguished classes using one of the existing classification algorithms. The generated grid points were then assigned to the appropriate classes based on the classification of the original points. Each grid point might be assigned to several classes, one from each data-strip. Yet, that the classification results of any grid points from different strips may not necessarily be similar. Stage 2B describes the proposed CMCD approach, where the classification results of all data-strips were combined based on specified weighting factors. Then, the final classification results of the grid points were decided based on these weighting factors. The first two steps of Stage 2B were for determining the
48

weighting factors. The first factor was the a posteriori probability of each class based on the classification results of each data-strip. The second factor was the inverse distance between the grid point into consideration and the nearest original LiDAR point in each data-strip. The last step of the CMCD was the combination of the classification results based on the calculated weights. The following sections describe the process of the point classification using the newly developed CMCD. 3.3.2.1 i. Stage 2A: Classification of a Single Data-strip Classification of Original Point Cloud Data

MATLAB code was developed to apply the selected supervised classification algorithm on LiDAR point clouds data, based on the considered attribute values. The input of this code was an ASCII file that contained the LiDAR point cloud attribute values (, , , , ,  ). The classification algorithm was applied to the point data of each data-strip  , the output was an ASCII file, for each data-strip, contained the values of the input point data in addition to a new field that contained the assigned class for each point (, , , , ,  , ). ii. Assigning Classes to Grid Points

In this step, depending on the classification results of the original LiDAR point cloud data, the grid points were assigned to one of the predefined land cover classes. First, the original classified point data were resampled to the generated grid space. The most frequent class of the classified point data within a square area of  × , where  is the spacing distance between grid points, was assigned to the resampled point that represented this  ×  area. Then, the grid points that coincide with the resampled LiDAR points were assigned to the same classes as the resampled points. The rest of the grid points remained unclassified. Figure 3-6 illustrates the steps of assigning classes to the grid points that coincide with the resampled points. Two methods were followed to assign the grid points to the appropriate classes, or to fill the gaps between LiDAR footprints. The first method was a nearest neighbour approach, where each unclassified grid point was assigned to the class of the nearest original point. The other method was an Iterative majority moving window approach, where at each iteration, the unclassified grid point that was adjacent to classified ones was assigned to the most frequent class of the eight neighbouring points (3x3 pixels window with the centre that coincides with the grid point to be classified). The iterative process continued until all grid points were assigned to one of the predefined classes. Figure 3-7 illustrates an
49

example of assigning a grid point to a different classes based on the followed filling gap method, in the first case the Nearest Neighbour approach. The nearest original point to the grid point is classified as Class 3; therefore, the grid point was classified as Class 3. In the second method, iterative majority moving window, the surrounding eight neighbour pixels were six of Class 5, one of Class 3, and one unclassified. Therefore, the grid point was assigned to Class 5.

    

    

    

    

      Unclassified Class1 Class2 Class3 Class4 Class5

a      c     

b      d          

Figure  3-6: Steps of Resampling the Classified Points to the Grid Points,
a) Original points after classification, b) the Defined Grid Space with Grid Spacing , c) the Original Points on the Defined Grid Space, d) Resampled Classified Points Based on the Most Frequent Class of the Original Points within the  ×  area

Since the Nearest Neighbour approach is not an iterative process, it is a fast process, but if the unclassified grid point is far from the nearest classified point, it will most likely be incorrectly classified. On the other hand, the iterative approach is time consuming, but the results of each one of the iterations affect the proceeding one. This is expected to lead to more accurate results of the final classification of the grid points.

50

The output of this step was added fields to the grid points ASCII file representing the assigned class for the grid points for each strip ,  = 1, 2, . .   ( ),   {1, ... , } where D is the number of data-strips, and N is the number of grid points.

    

    

    

    

      Unclassified Class1 Class2 Class3 Class4 Class5





a) Nearest Neighbour Method

b) Iterative Majority Moving Window Method

Figure  3-7: Example of Assigning Classes to the Unclassified Grid Points (Filling the Gaps) Using Different Methods
a) Nearest Neighbour Method, and b) Iterative Majority Moving Window Method

3.3.2.2 i.

Stage 2B: Classification of Multiple Data-strips Using CMCD Approach Determining the "a posteriori" Probabilities

After assigning the appropriate classes to all grid points, the accuracy of the classified points was assessed using the confusion matrix method. The predefined reference points (defined in the preparation stage) were used to form the confusion matrix. There was a confusion matrix corresponding to each data-strip. From the literature, the values of the confusion matrix represent the degree of support given to each class by the classifier after scaling its values to [0,1] interval (Xu et al., 1992; Poliker, 2006). Therefore, the values of the confusion matrices can be used as a posteriori probability of each class after
51

normalization, as described in Subsection 2.2.5. An iterative process was developed using MATLAB to normalize the confusion matrices to ensure that their elements were within the [0, 1] interval, and that each column and row added up to 1. Hence, a normalized confusion matrix (NCM) was determined for each data-strip, and the a posteriori probabilities for each class of the data-strips were calculated. ii. Determination of Inverse Distance Weightings

Distances between the grid point in consideration and the nearest original LiDAR points of each datastrip were included as another weighting factor in the proposed approach. This factor is inversely proportional to the distances between the original point and the grid point in consideration. Therefore, the distance between each of the grid points and the nearest original LiDAR points of each strip were measured. Then the Inverse Distance Weighting (IDW) method was used to calculate the weight factors of each data-strip at each grid point. These weights were used as another factor affecting the combining multiple classified datasets (CMCD). The formula for calculating the IDW for two strips is: 1 =
2 1 +2 1 1 +2

& 2 =

(3-1)

Where 1  2 are the weighting factors for the first and second data-strips, respectively, and 1  2 are the distances between the grid point and the nearest point of the first and second data-strips, respectively. iii. Combining the Classification Results

In this section, several equations were used to combine the classification results of multiple data-strips; therefore, introducing the notations that were used at the beginning will be helpful.  :  Data-strip, where d = 1, 2, ..., Number of data-strips. In the case of single strip,  = 1. For data of an overlapped area between two adjacent strips, there will be two data-strips, 1 and 2 .    : point number  of the original LiDAR point clouds (irregularly distributed points). : rectangular grid space of the study area with  ×  dimensions, as defined in Section (3.1). : distance between the grid points in both X and Y directions.

52

  {1 , 2 , ... ,  }   ( )  ( )   ( )

:

point number  of the grid points (regular points that fully populated the space ).

: : : : : : :

total number of grid points. classes within the study area, and  is the number of distinguished classes. event of assigning class to the data of the  strip. event of assigning class to the original point  of  strip. event of assigning class to the grid point  of the  strip. confusion matrix of the  strip. belief value in assigning the class  based on the a posteriori probabilities.

With the data of two overlapped strips, each grid point  was assigned to two classes with assigning class events ( ), where 1 ( ) = 1  2 ( ) = 2 . To decide which class would be finally assigned to the grid point after combining the classification results, the belief of each class has to be calculated. The grid point, then, will be assigned to the class with maximum belief. The belief of each class can be calculated using the formula in (2-18). This belief is based on the a posteriori probability of each class. (   |  (), ) = (   |  () =  ),  = 1, ... ,  (2-18)

Here, the belief was calculated based on the two mentioned factors; the a posteriori probability of the classification results in each strip, and the IDW. Based on the a posteriori probabilities of any class  , the belief in that class can be defined by the conditional joint probability that a point  belongs to that class and is true under the occurrence of the two assigning class events, 1 and 2 in the environment . This relation can be described by the Equation (3-2) (Xu et al., 1992):   (   |1 ( ), 2 ( ), ) = (   |1 ( ) = 1 , 2 ( ) = 2 ) ,  = 1, ... ,  Where,   (   |1 ( ), 2 ( )) is the belief, based on the a posteriori probability, in class  assigned to point  is true with the occurrence of , 1  2 . 1 ( ), 2 ( ) are the assigning class events, for data-strip 1 and 2. (3-2a)

53

For

simplicity

the

 (   |1 ( ), 2 ( )) will

be

denoted

as

 ( |1 , 2 ),

and

(   |1 ( ) = 1 , 2 ( ) = 2 ) as ( |1 , 2 ). Therefore, Equation (3-2)a can be expressed as:  ( |1 , 2 , ) = ( |1 , 2 ) (3-2b)

Based on the Bayesian formula, the right hand side term of Equation (3-2)b can be described as follows (Xu et al., 1992): ( |1 , 2 ) = Where, ( ) (1 , 2 ) is the a priori probability of the classifier for each data-strip, however, the a priori probabilities are considered constant for all classes. is the unconditional joint probability density.
(1 ,2 |  ) ( ) (1 ,2 )

,  = 1, ... , 

(3-3)

Since the classifier is trained by independent training sets for each data-strip, the two classified datastrips are considered independent; thus, the product rule can be applied for the joint probability case. (1 , 2 | ) = (1 | ) . (2 | ) (1 , 2 | ) = (1 | ) . (2 | ) =  =1 ( | ) (3-4) (3-5)

Where (1 |  ) and (2 | ) are the conditional probabilities of class  for the two data-strips, which can be estimated by the a posteriori probability by evaluating the classification result of the base classifier using the confusion matrix (Yan & Shaker, 2011). The unconditional probability can be expressed in terms of the conditional probability as (Kittler et al., 1998): (1 , 2 ) =  =1 (1 , 2 |  ) ( ) (3-6)

From Equations 3-2, 3-3, 3-4, 3-5, and 3-6, the belief of class  based on the a posteriori probabilities of the classification results of the two data-strips can be expressed as:   ( ) = ( |1 , 2 ) = ( ) 
((1 | ) .(2 | )) ( ).(1 | ).(2 |  )

=1

(3-7)

54

  ( ) = ( ) 

=1

( ). =1 ( | )

 =1 ( | )

(3-8)

To include the effect of the distance on the final classification decision, the weighting factors 1 and 2 can be multiplied by the a posteriori probabilities of the classification results in the calculation of the belief of each class.  ( ) = ( ) 
1 (1 | ) .2 (2 | ) ( ).((1 | ).(2 |  ))

=1

(3-9)

Then, for each point of the grid points, the belief of the available classes are compared to the point of interest, and the class with the maximum belief is assigned to that point. Since the a priori probabilities are considered constant for all classes, comparing the belief for each class is not affected by the a priori probabilities. Hence, the a priori probabilities can be omitted from Equation (3-9). Furthermore, the denominator in this formula is constant for all classes; so it will not affect the final combining decision. Therefore, only the numerator can be considered in the belief comparison.      (3-10)

1 (1 | ) . 2 (2 | ) = max  =1 1 (1 | ) . 2 (2 | )

The outputs of this step were, for each grid point, belief value of each class based on the a posteriori probability of that class. There was a different value for each strip based on the classification results and the accuracy assessment of that strip. 3.3.3 Evaluation Stage

The last stage of the point classification workflow was the evaluation of the final product. The combined classification results were assessed using the confusion matrix method based on the collected reference points for the grid points. The overall accuracy was calculated using the formula:  =
 =1   =1  =1 

× 100

(3-11)

Where,   number of reference points of class  that were assigned incorrectly to class  ; number of reference points of class  that were assigned correctly to class  .

55

3.4 Part-3: Classification of LiDAR Point Data Using Statistical Analysis Technique
The third part of the methodology, which is defined by the red dashed rectangle in Figure 3-1, is the classification of point data using a new approach built on the statistical analysis segmentation technique for land cover classification of elevation and intensity LiDAR point cloud data. Statistical analysis segmentation techniques were applied to LiDAR data to separate terrain and nonterrain points (Bartels & Wei, 2006; Bartels et al., 2006), and to detect certain features, such as roads, roofs, and vegetation (Bao et al., 2007; Yunefi et al., 2008, Crosilla et al., 2011; Crosilla et al., 2013). These previously introduced statistical analysis segmentation techniques were adequate for separating terrain points, and for detecting one type of feature at a time. Consequently, data have to be examined each time to determine the characteristics of each feature type (class) that is required to be separated. An innovative approach was developed in this research to classify LiDAR point cloud data to the distinguished land cover classes. This proposed approach involves a point-based logic classifier, which classifies the 3D point data, without resampling the points into a 2D grid, to distinguish all land cover classes together not one-by-one, nor limited to separate terrain and non-terrain data. The classified points then can be resampled to a predefined grid space and the full area will be assigned to the appropriate land cover classes. The proposed approach depends on the underlying assumptions that the kurtosis change curve changes smoothly between the cycles if the point data belong to a homogeneous class. The statistical analysis segmentation technique used in this classification approach is based on the change curve segmentation algorithm introduced by Crosilla et al. (2011). Nevertheless, the concept of that algorithm was modified to execute the idea of classifying LiDAR data into several classes at once. The proposed approach is built on a statistical analysis segmentation algorithm, and a supervised classification algorithm to assign the segments to the appropriate land cover classes. The work procedure of this classification approach consists of two stages. First, the point clouds data were segmented using a new separation method that depended on the changes in kurtosis values of each attribute values of the LiDAR point cloud data, one-by-one. Second, the produced segments were classified using any of the existing supervised classification techniques, e.g., Maximum Likelihood or minimum distance to means classifiers, with the aid of training data that were collected from the same or similar areas. Figure 3-8 shows the procedure of the introduced approach for land cover classification of LiDAR data based on two attribute values. MATLAB code was developed to apply this new approach
56

to LiDAR data for land cover classification. The details of the work procedure are described in the following subsections.

1) Unsupervised Segmentation

1) Segmentation based on First Attribute Value (Elevation Values) · Divide the point data into segments based on an attribute data using a modified kurtosis change curve algorithm. 2) Sub-Segmentation based on Complementry Attribute Values (Intensity Values) · Divide each segment into sub-segments based on the complimentry attribute data using the modified kurtosis change curve algorithm.

3) Classification of sub-segments into the distinguished land-cover classes.

2) Supervised Classification

· Determine the center of each sub-segment · Applying one of the existing supervised classification algorithms on the produced subsegments.

4) Accuracy Assessment · Based on selected reference points.

Figure  3-8: Procedure of the Introduced Classification Approach

3.4.1

Unsupervised Segmentation Stage

A segmentation algorithm based on the concept of the kurtosis change curve algorithm was developed and applied to each of the data attribute values. This means that when the elevation and intensity attribute values are considered, the kurtosis change curve algorithm will be applied to the data twice; first based on one of the attribute values, e.g., elevation, to produce a number of segments. Then, second, the same algorithm will be applied to the produced segments based on the complementary attribute values, e.g., intensity, to produce a number of sub-segments. If there are more than these two attribute values, the segmentation process will be conducted a number of times based on each of the considered attribute values. Figure 3-9 depicts an example of the kurtosis change curve, where the xaxis in the graph represents the cycle number and the y-axis represents the kurtosis value. The kurtosis values were calculated for each cycle and plotted versus the cycle number. Where at the first cycle the kurtosis value was calculated for all of the points, then for each subsequent cycle, the point with the highest value was removed and the kurtosis values were recalculated for the rest of the
57

points. The process of removing the points and recalculating the kurtosis values continued until the last point (Bao et al., 2007). The last point for the kurtosis change curve algorithm was reached when the number of points became the minimum number of points ( ) introduced by Bartels and Wei (2010). Bartels and Wei (2010) mentioned that in order to achieve a meaningful interpretable result, the minimum number of points ( ) should be similar to the minimum sample size introduced in Kotrlik et al. (2001). Equation 3-12 can be used to calculate the  (Bartels & Wei, 2010).  = ( Where, / 0 
2 / × 0 2
2



)

(3-12)

value of the confidence level (which is 1.96 for 95% confidence), standard deviation of the attribute values of the points, acceptable margin of error.



Cycle Number

Figure  3-9: Example of Kurtosis Change Curve

The segmentation process was built on an underlying assumption that the kurtosis change curve changes smoothly between the cycles if the point data belong to a homogeneous class. As a result, the prominent changes in the kurtosis change curves (peaks and pits) were considered as critical points, and used as separators between the different segments. To determine the peaks and pits of the curve, the kurtosis change curve was plotted and the local minima and maxima were detected. The attribute values corresponding to the cycles at the critical points were considered as a separator to cluster the
58

points with lower values into a unique segment. After separating this segment, the same steps were repeated iteratively on the remaining points to cluster all points into segments. We call this process an "iterative single-segment separation" process; that is because in each iteration only one segment is separated. In this method the last critical point was used to separate the points with lower attribute values in a unique segment, and then this process works iteratively on the remaining points to separate another segment. The process was repeated until all points were clustered. Figure 3-10 demonstrates an example using the iterative single-segment separation process for segmentation. In this example, five different iteration results are illustrated, where the last critical point (peak or pit) was used to separate a segment. It can be noticed that the number of cycles (x-axis) decreases after each segment separation.

a) Segment 1

b) Segment 2

c) Segment 3

d) Segment 4

e) Segment 5

Figure  3-10: Example of Segmentation Using Kurtosis Change Curve with Iterative Single-Segment Separation Process

The process of the unsupervised segmentation algorithm is illustrated in Figure 3-11. The procedure can be summarized as follows:  Point data was sorted in a descending manner based on the considered attribute value (elevation or intensity), regardless of their spatial positions.
59

  

The minimum number of points was calculated using the Equation 3-12. For determining the kurtosis change curve, the kurtosis value was calculated in iterative cycles, as described earlier. The values of the kurtosis in each cycle were plotted against the cycle's number; this graph was called the kurtosis change curve. The curve in Figure 3-9 illustrate an example of the kurtosis change curve.

   

The local maxima and minima values in the kurtosis change curve (peaks and pits) were located and considered as critical points for segment separation. To avoid the tiny local peaks or pits, the kurtosis change curve was smoothed before the determination of the critical points2. The cycles at the critical points were figured out, and the attribute value (elevation or intensity) corresponding to these cycles were determined. The iterative single-segment separation process followed. An iterative process was conducted, where the points that had attribute values less than the value of the last critical point were separated in a segment (Figure 3-10a). Then the kurtosis change curve was redeveloped based on the remaining points, and another segment was excluded (Figure 3-10b). This iterative process continued until no peaks or pits existed3, as shown in Figures 3-10c, d, e.

 

The output of the previous steps was point clouds ASCII file contained (, , , , _) for each point, where the _ is the segment numbers that each point belongs to. Based on the complementary attribute value, each segment was treated as a separate point cloud file and was sub-segmented following the previous steps. Another field, then, was added to the point clouds data, containing the sub-segment number corresponding to each point.

3.4.2

Supervised Classification Stage

The second stage of the proposed classification approach was the supervised classification stage, where classes would be assigned to the produced sub-segments using one of the existing supervised

2

The find maxima and find minima functions, used to extract the local minima and maxima, work by comparing each point by its surrounding points, any tiny changes can be considered as local minimum or maximum.
3

The find maxima and find minima functions in the MATLAB code consider the first and last points of the curve as inflexion points; therefore, if the number of critical points is 2, it means that no more peaks or pits are found.

60

classification algorithms. In this stage each sub-segment produced in the first stage would be treated as a single pixel or a single object that had to be assigned to one of the distinguished land cover classes. Figure 3-12 illustrates the general steps of the supervised classification stage. The detailed steps are described below.

Read the point cloud data

Sort the points descending based on the considered attribute value

Determine the minimum number of points

Calculate the kurtosis value of the points

Remove the highest point

N

Number of points < min number Y Plot the Kurtosis Change Curve

First point = separating point

Determine the Peaks and Pits of the change curve (Critical Points Determination)

Separating value corresponding to the last Peak

Number of critical points <3

N

Y

End

Figure  3-11: Workflow of Unsupervised Segmentation of the Point Cloud LiDAR Data
61

1. Identification of Classes and Selection of Training Sites

2. Calculation of the classifier required parameters.

3. Determination of the center of each subsegment

6. Accuracy assessment

5. Assign the class value to each subsegment

4. Applying the classification algorithm

Figure  3-12: Steps of Supervised Classification Process for the Produced Sub-Segments

1) The classification process started by distinguishing the classes of the study area and selecting the training sites; the training sites can be selected from the same study area or a similar one. The selection of the training sites depended on the distinguished classes and the variation of the values in each class. 2) Based on the selected training sites data, the required parameters were calculated. For example for the Maximum Likelihood classification algorithm, mean and covariance of each training site based on the considered attribute values were calculated using the formulas in Equations (3-13) and (3-14): 3)  =
1     =1  1   (  =1 

(3-13) -  ) ( -  ) (3-14)

4)  =

Depending on the number of attribute values considered in the classification, the dimensions of the mean vector and covariance matrix would be defined. When there were only two attribute values  (elevation and intensity), the mean would be a vector with two elements, [  ], and the covariance   2 matrix would be a matrix with dimension 2x2, [    
2 

 ]. Where, 2

mean of the elevation values; mean of the intensity values; variance in elevation values; variance in intensity values; covariance of elevation and intensity values.

2 

62

5) The mean value of each sub-segment was calculated based on the formula (3-13). The vector with the mean values of each sub-segment was considered as the (spectral) center of the sub segment, [  ].  6) Using the classification algorithm, each sub-segment would be assigned to the appropriate land cover class. 7) The last step was the evaluation of the classification results, where the well-distributed predefined reference points were used with the confusion matrix approach to calculate the overall accuracy of the classification results. 3.4.3 Proposed Classification Approach for Large Areas

The proposed classification approach may take a long processing time because the segmentation approach depended on an iterative algorithm. Consequently, large areas will be time consuming. To accelerate the processing time for large datasets, two alternatives were proposed: either eliminating the iterative process, or reducing the size of the data. For eliminating the iterative process a MultipleSegments Separation method was proposed for the unsupervised segmentation stage. This method relied on the kurtosis change curve to cluster the data as the iterative single-segment separation method did. Yet, instead of selecting the last critical point as a separator, all critical points (peaks and pits) of the kurtosis change curve were considered for separating various segments at the same time. Figure 3-13 depicts an example of using the multiple-segments separation method in segmenting LiDAR point cloud data, where each segment had attribute values that were ranged between the corresponding values of each two sequential critical points.

The other way to reduce the processing time for large datasets was to reduce the size of the data by partitioning large areas into small sub-areas, in such a way that each sub-area would have a manageable (workable) number of points to be handled quickly. This way reduced the time, even if the iterative single-segment separation method was used. That is because iterations, then, would have a much less number of points, which required less number of cycles to determine the kurtosis change curves. For partitioning the large areas, the number of points that was workable based on the specifications of the computer in use had to be defined at the beginning. Having the coordinates and the number of LiDAR point data, the extent of the area in consideration can be detected, and the point density can be calculated. Based on the point density and the selected number of points for each sub-area, a

63

preliminary dimension of each sub-area can be estimated. Knowing the extent of the area and the dimensions of each sub-area, the number of rows and columns containing the sub-areas can be determined. Subsequently, the final coordinates of the sub-areas can be determined. By partitioning any large area into smaller sub-areas, the segmentation process will be applied to each of the sub-areas, and the final classification results will be collected at the end.

Segment 2

Segment 4

Segment 1

Segment 5

Segment 3

Segment 6

Figure  3-13: Example of Segmentation Using the Multiple-Segments Separation Method

For evaluating the different classification resulted by applying the methodology, several comparisons were performed as follows: 1. Comparing results of different layer combinations for the pixel-based classification logic to classification results from aerial imagery, and verifying the findings in all study areas. 2. Comparing results of pixel-based and object-based classification logics for all study areas. 3. For point-based classification, results accuracy of the gap-filling methods were compared (Nearest Neighbour and Iterative Majority Moving Window). 4. Point-based classification results were compared to the equivalent cases of the multi-layer classified images (pixel-based classification). 5. Comparing results of the different combining methods of the overlapping areas. 6. Comparing the kurtosis change curve approach to the pixel-based classification results. 7. Comparing the accelerated process results to the normal approach in the proposed statistical analysis classification approach.

64

4. Results and Analysis: Single Wavelength LiDAR Data
4.1 Overview
This chapter describes the study areas, the dataset, the experimental work, and the results and analysis. The experimental work was divided into three sections as described in Chapter 3. In the first section, LiDAR point data were converted into raster image data, and then classified using pixel-based and object-based classification logics. The supervised image classification technique, the Maximum Likelihood algorithm, was used to represent the pixel-based classification logic. LiDAR intensity and elevation data were used in the classification process. Then, the Normalized DSM (NDSM), the slope of the elevation layers, and the texture of the intensity were used as auxiliary layers that were derived from the LiDAR elevation and intensity data. An ortho-rectified aerial image that was acquired during the LiDAR data acquisition mission was included in the classification process for results verification and evaluation. The object-based classification logic was applied to the raster image data using the developed decision tree with certain criteria that were set, based on the characteristics of the study area, as described in Section 3.2.2. In the second part of the experimental work, LiDAR point cloud data were used directly in the classification process instead of converting the point cloud into raster images. The Maximum Likelihood classifier was applied to the LiDAR point cloud data. Then, two approaches were followed to fill the gaps between the LiDAR footprints and generate classified image data--nearest neighbour and majority moving window approaches. Details of these approaches are described below. The Combined Multiple Classified Datasets (CMCD) approach was developed and used to combine the classification results of the multiple data-strips and to decide upon the final classification of the study area (Section 3.3.2). The CMCD approach depended on the a posteriori probabilities of each class of the classification results of two different data-strips. The distance between grid points and the original LiDAR points of each strip was used as another weighting factor in the CMCD. A combination of data of the two data-strips and classification of all the data were included also in Part-2, Section 3.3. The point-based classification logic was used again to classify the LiDAR point cloud data based on the statistical analysis segmentation technique. The new approach used the concept of the kurtosis change curve algorithm to unsupervised segmenting of the data into segments based on several attribute values (intensity and elevation in this case). Then the produced segments were classified into the distinguished classes using the Maximum Likelihood classifier.
65

4.2 Study Area and Dataset
4.2.1 Location of the Study Area

The study area is located in Burnaby, British Columbia, Canada. It covers the area surrounding the British Columbia Institute of Technology (BCIT). The coordinates of the scanned area are as shown in Figure 4-1. This area contains a variety of land cover types including buildings, parking areas, trees, roads, and open spaces with and without grassy coverage.

Figure  4-1: Study Area (Clipped from Google Map and Google Earth)

4.2.2

Dataset

A Leica ALS50 sensor, operating in 1.064 m wavelength, 0.33 mrad beam divergence and 83 kHz pulse repetition frequency, was used to acquire the LiDAR data. The acquisition mission was conducted on July 17, 2009 at local time 14:55. The area covered by LiDAR data is 1 x 2 km. The LiDAR data were captured from six different flight lines forming six different data-strips as shown in Figure 4-2. Two long distance data-strips, Strip1 and Strip2, were in the north-south/south-north directions, captured from a flying

66

height of about 1150 m. The other four data-strips, Strips 3-6, were short distance strips in the eastwest/west-east directions, captured from a flying altitude of about 540 m (Habib et al., 2011).

Strip1 Strip2 Strip3 Strip4 Strip5 Strip6 Flight Line 1 Flight Line 2 Flight Line 3 Flight Line 4 Flight Line 5 Flight Line 6

Figure  4-2: Flight Directions and Data-Strips of the LiDAR Acquisition Mission
Modified from Habib et al. (2011)

The dataset was acquired as part of the GEOIDE collaborative research project4. The data consisted of 3D point cloud data in ASCII format together with the GPS trajectory data. The data file stored the x, y, and z coordinates, the linearized intensity value in 8 bit5, and the time tag of each pulse of the point cloud. The trajectory data stored the time and the x, y, and z coordinates of the LiDAR sensor at the
4

The GEOIDE collaborative research project was cooperation between University of Calgary and Ryerson University. The project was led by Drs. Ayman Habib, Derek Litchi, the University of Calgary (U of C), and Ahmed Shaker, Ryerson University. 5 Notated in this research as original intensity data 67

moments of LiDAR data collected. The point density of the data was 4-5 points/m2. Aerial images were captured during the same flight mission of the LiDAR data acquisition. The aerial images were geometrically corrected and ortho-rectified. The provided ortho-rectified aerial imagery had a 0.5 m spatial resolution and consisted of three bands (blue, green and red). 4.2.3 Study Areas

Three different study areas were clipped out of the acquired area to conduct the land cover classification and to verify the outcomes. The selected areas were located in different strips--Strip3, Strip4 and Strip6. The locations of the selected study areas relative to the strips are shown in Figure 4-3 (b). An area of around 500 m x 400 m was clipped out of Strip4 and defined as Area1 (which contained around 650,000 points), as shown in Figure 4-3 (c-1). This area was selected because it contained a variety of land cover features including buildings, parking areas, trees, open spaces covered by bare soil, and grassy areas. The west side of Area1 was a forest area. For outcome verification, Area2 was clipped out of Strip3, where the same land cover types as Area1 were contained. Area2 was around 600 m x 350 m and contained around 940,000 points (See Figure 4-3 (c-2)). This area is an urban area with a forest at the southwest corner. Another smaller area around 360 m x 85 m contained around 150,000 points, Area3, was clipped out of Strip4 but located in the overlapped area with Strip6 (the same area contained around 120,000 points in Strip6, Figure 4-3 (c-3)), to verify the results and apply the Combined Multiple Classified Datasets (CMCD) approach. Area3 had complex land cover types relative to its size, where different types of roof surfaces and different ground elevations were found within a small area. Although there were no forest areas within Area3, the study area contained a number of scattered trees.

68

4.3 Data Preparation

(c-2)

(a) (b) Area2

(c-1)

(c-3)

Area3

Figure  4-3: Study Areas

Area1

(a) The location of the study areas within the whole captured area, (b) The location of the study areas relative to the Strips, (c-1, 2, and 3) the details of the study areas Area1, 2, and 3 respectively

4.3.1

Geometric Calibration

The LiDAR point cloud data were geometrically calibrated using 37 ground points collected by GPS as described in Habib et al. (2011). The accuracy of the geometric calibration is summarized in Table 4-1. Based on the checkpoints, the Root Mean Square Errors (RMSE) in ,  and  directions were 6, 7, and 17 cm, respectively. The total RMSE was equal to 19 cm.

69

Table  4-1: RMSE analysis
(Habib et al., 2011)

Before Calibration Mean X (m) Mean Y (m) Mean Z (m)  (m)  (m)  (m)  (m)  (m)  (m)  (m) -0.03 -0.18 0.15 0.11 0.15 0.17 0.11 0.23 0.23 0.34

After Calibration -0.01 -0.01 0.02 0.06 0.07 0.17 0.06 0.07 0.17 0.19

4.3.2

Radiometric Correction

The intensity data were radiometrically corrected to determine the spectral reflectance of the illuminated objects on the ground by eliminating the effects of the systems characteristics, the object geometry, and the atmospheric attenuation. The radar range equation was used to correct the intensity values of the intensity data, where LiDAR intensity values, range values, atmospheric attenuations, and incidence angles of the laser pulses were used as described in Shaker et al. (2011). As a step for improving the homogeneity of the intensity data, a histogram matching approach was applied to the overlapped area between Strip4 and Strip6 (Area3). The geometrically calibrated and radiometrically corrected data were used in the classification process to achieve the objectives of this research. LiDAR point cloud data covering the three areas were represented in Figures 4-4 and 4-5, for the intensity and elevation values, respectively. It can be noted, by observing these figures, that the study areas have gaps between the LiDAR footprints in some parts of the roads and some buildings. In Area3, where overlapped data are available, it can be seen that data of Strip6 have larger gaps between LiDAR footprints than the Strip4 data. By investigating the DSM of the three study areas, a hilly trend in the terrain can be noticed, where the south-western corners have higher elevation values than the northeastern corners.

70

Area1

174

Area2

22 17 11 6 0

Area3 (Strip4)

(Strip6)

Figure  4-4: Point Cloud Data (Intensity Values) of the Three Study Areas for the Single Wavelength LiDAR Data

71

Area1

85

75

65

60

55

50

45

Area2

40 38

33 30 28 25 23 20

0

Area3 (Strip4)

(Strip6)

Figure  4-5: Point Cloud Data (Elevation Values) of the Three Study Areas for the Single Wavelength LiDAR Data

72

4.4 Experimental Work and Results
4.4.1 Part-1: Classification of LiDAR Data Converted into Raster Image Format

The image classification processes were performed on LiDAR point data that were converted into a raster (image) by interpolating the intensity and elevation attribute values. Then two classification logics were conducted--pixel-based and object-based classification logics. The results of the two classification logics were compared to each other at the end of the first part. In this part, the classification approaches were applied to the study area Area1, and then to verify the findings, the same approaches were applied to the other two study areas, Area2 and Area3. 4.4.1.1 Pixel-Based Classification

The pixel-based classification workflow started by data preparation step, as illustrated in Figure 3-3 in the previous chapter. The LAS file was converted into ASCII file using the LASTools software. Then three ASCII files were extracted representing: a) intensity data (, , ), b) elevation points for the entire study area (, , ) and c) elevation points for the terrain areas (, , ). The terrain points were collected manually out of the roads, grass and soil areas. The three files (intensity and two elevation data files) were resampled into three different images using the Kriging interpolation technique to convert the point data files into raster grid (image) and to fill the gaps between the LiDAR footprints. Intensity and Digital Surface Model (DSM) images were produced out of the intensity and elevation attribute values of all points. The points collected out of the terrain areas were interpolated, and a third image containing the Digital Terrain Model (DTM) was generated. Based on the average density of the LiDAR point cloud data, a pixel size of 0.2 m was selected for the production of the three images. The intensity image, the DSM, and the DTM of Area1 are illustrated in Figure 4-6 a, b, and c, respectively.

73

(a)

Intensity

(b)

DSM

(c)

DTM

Figure  4-6: Interpolated Data of "Area1"
a) Intensity image, b) the DSM, and c) the DTM

74

Other layers were extracted from the elevation and intensity data such as Normalized DSM (NDSM), texture of the intensity, and slopes of the elevation surfaces (DSM and NDSM). The ERDAS Imagine software package was used to produce the texture of intensity and the slopes of the elevation models. Figure 4-7 illustrates the additional extracted layers for Area1. By observing the extracted layers it can be noticed that:

a.

NDSM

b.

Texture of Intensity

c.

Slope of DSM

d.

Slope of NDSM

Figure  4-7: Extracted Layers for "Area1"
a) Normalized DSM, b) Texture of the Intensity, c) Slope of DSM, and D) Slope of NDSM

1- The NDSM is similar to the DSM; however, the terrain (roads) in the south-western corner has the same gray level as other terrain areas. 2- The texture of intensity is bright at the trees areas and the grassy areas. 3- The slope of the DSM and the NDSM are almost the same. Both of them have very bright values at the trees areas, and at the borders of the buildings.
75

Most of the buildings at the study area have flat roofs; therefore, the slope within the buildings is a dark value. The inclined roofs buildings have a dark gray value (brighter than the flat-roof buildings). The second step of the workflow was combining layers. Several combinations of layers were executed, where two or more layers were combined into one multi-layer image. The cases of single or of multilayers that were investigated are: 1) Aerial Imagery (For comparison) 2) DSM 3) Original Intensity 4) Radiometric corrected intensity 5) Intensity and DSM 6) Intensity and NDSM 7) Intensity, DSM, and Texture 8) Intensity, NDSM, and Texture 9) Intensity, DSM, Texture, and DSM Slope 10) Intensity, NDSM, Texture, and NDSM Slop

The Maximum Likelihood supervised image classification technique was used as a pixel-based classifier to investigate the possibility of producing land cover classes from LiDAR intensity and elevation data independently of any other external data sources. Five different classes were identified: buildings, trees, roads, bare soil, and grass. The classification results of Area1 to the five distinguished classes for the mentioned cases are illustrated in Figure 4-8.

Study Area1

Buildings Grass Roads Soil Trees

Aerial Imagery

DSM

Figure 4-8 a: Pixel-Based Classification Results for "Area1"

76

Original Intensity

Intensity

Intensity and DSM

Intensity and NDSM

Intensity, DSM, and Texture

Intensity, NDSM and Texture

Figure 4-8 b: Pixel-Based Classification Results for "Area1"

77

Intensity, DSM, Texture and DSM Slope

Intensity, NDSM, Texture and NDSM Slope

Figure 4-8 c Figure  4-8: Pixel-Based Classification Results for "Area1"

The classification results for these cases were assessed using the confusion matrix approach based on 978 reference points, which were well-distributed over the study area, as shown in Figure 4-9. The reference points were randomly selected out of the original point cloud data to avoid the effect of the interpolation on the accuracy of the ground validation. The ground validation information was collected from the ortho-rectified aerial imagery that was captured at the same LiDAR mission. Table 4-2 shows the number of reference points in each class.

Figure  4-9: The Distribution of the Reference/Check Points in "Area1"

78

Table  4-2: The Number of Reference Points in each Land Cover Class in "Area1"

Class Buildings Grass Roads Soil Trees

Number of Reference Points 213 79 250 143 293

The accuracies achieved from the classification results of the different layer combinations were compared to each other to conclude the best combination of layers, or to determine which layer had a significant influence on the classification results. Table 4-3 illustrates the accuracy assessment of the classification results for Area1 with the 10 mentioned cases. In this study area, the characteristics of the bare soil areas and other areas that were covered by grass were close to each other, and it was difficult to differentiate between them. Therefore, distinguishing four classes only--buildings, grass and soil, roads, and trees--were investigated and assessed using the same 1000 reference points. Table 4-3 includes the accuracy assessment of the classification results for Area1 for the distinguished five and four classes. The confusion matrices for the mentioned cases and the Kappa statistics calculations for each class are illustrated in Appendix A.
Table  4-3: Accuracy Assessment of Classification Results for "Area1"

Case No. 1 2 3 4 5 6 7 8 9 10

Bands Combination Aerial Imagery DSM Original Intensity Intensity6 Intensity, DSM Intensity, NDSM Intensity, DSM, Texture Intensity, NDSM, Texture Intensity, DSM, Texture, Slope Intensity NDSM, Texture, Slope

Overall Accuracy 4 Classes 5 Classes 62% 58% 43% 41% 39% 32% 44% 36% 55% 53% 72% 66% 58% 56% 77% 71% 60% 58% 73% 71%

6

Intensity term denotes the radiometric corrected intensity values

79

By analyzing the results, it can be observed that:  The radiometric corrected intensity values produced more accurate classification results than the original intensity values (around 4% increase in the overall accuracy). That is because the radiometric corrected intensity values considered the variation of the scanned angles that lead to produce more homogeneous values within each class, which represents the object reflectance more accurately. In Figure 4-8 it can be noticed that the trees class is classified correctly when the radiometric corrected intensity was used. That is can be proofed by comparing the Kappa statistics value of the Trees class in both cases (original intensity: 0.22 and radiometric corrected intensity: 0.76)  Using the Normalized DSM instead of the DSM greatly improved the classification results (1317% improvement). By observing the elevation values of the study area it was found that it is a hilly area; and some roads in the hilly area (at the southwest corner) have high elevation values that are similar to the elevation of the buildings in the lower elevated areas (at the middle). Therefore, the absolute heights of the objects (NDSM) differentiated the raised buildings from the roads.  Adding the texture of the intensity data to the intensity and the elevation layers improved the classification results. Due to the homogeneity of the classes after applying the radiometric correction, the texture of the intensity, which represents the variation within the class, did not add more information to the classification process, and hence no major improvements were found. However, some areas that were classified incorrectly as buildings were classified correctly as trees when the texture of intensity was included.  Including the slope of the elevation layers had no effect on the classification results. Most of the buildings have flat roofs and no difference between roofs and roads could be detected, which is opposite of the sloped roof buildings, where the slope will add value. However, the slope image can be used for detecting the object boundaries that can be used for further improvement of the classification results.  The results of the combined LiDAR layers (intensity and DSM) were less accurate than the classification results of the aerial imagery (by around 5%). However, using the NDSM and intensity layers produced more accurate classification results than the aerial imagery (by around 8%). There was greater improvement when the texture of intensity was added to the intensity and the NDSM (around 5%).

80

To verify the previous observations, land cover classification of LiDAR data for another study area "Area2" was investigated following the same classification procedure of Area1. The raster grids, the intensity image, the DSM, and the DTM of Area2 are illustrated in Figure 4-10 a, b, and c, respectively. The extracted layers out of the LiDAR elevation and intensity data are illustrated in Figure 4-11.

(a)

Intensity

(b)

DSM

(c)

DTM

Figure  4-10: Interpolated Data of "Area2"
a) Intensity image, b) DSM, and c) DTM

81

a. NDSM

b. Texture of Intensity

c. Slope of DSM

d. Slope of NDSM

Figure  4-11: Extracted Layers for "Area2"
a) Normalized DSM, b) Texture of the Intensity, c) Slope of DSM, and D) Slope of NDSM

The Maximum Likelihood classification technique was conducted on the different combined layers considering the same five distinguished land cover classes as in Area1. The classification results of Area2 for the mentioned cases are illustrated in Figure 4-12. For evaluating the classification results of Area2, 854 reference points were randomly collected. The ground validation of the reference points was collected from the ortho-rectified aerial imagery. Figure 413 illustrates the distribution of the reference/check points of the study area, and Table 4-4 shows the number of reference points in each of the five land cover classes. The reference points were used to create the confusion matrix for accuracy assessment of the classification results. Table 4-5 shows the overall accuracy of the classification results of Area2. The confusion matrices for the mentioned cases and the Kappa statistics calculations for each class are illustrated in Appendix A.

82

Study Area2

Buildings Grass Roads Soil Trees

Aerial Imagery

DSM

Original Intensity

Intensity

Intensity and DSM

Intensity and NDSM

Intensity, DSM and Texture

Intensity, NDSM and Texture

Figure 4-12a: Pixel-Based Classification Results for "Area2"
83

Intensity, DSM, Texture and DSM Slope

Intensity, NDSM, Texture and NDSM Slope

Figure 4-12b Figure  4-12: Pixel-Based Classification Results for "Area2"

Figure  4-13: The Distribution of the Reference Points in "Area2" Table  4-4: The Number of Reference Points in each Land Cover Class in "Area2"

Class Buildings Grass Roads Soil Trees

Number of Reference Points 237 49 394 74 100

84

Table  4-5: Accuracy Assessment of Classification Results for "Area2"

Case 1 2 3 4 5 6 7 8 9 10

Band Combination Aerial Imagery DSM Original Intensity Intensity Intensity, DSM Intensity, NDSM Intensity, DSM, Texture Intensity, NDSM, Texture Intensity, DSM, Texture, Slope Intensity NDSM, Texture, Slope

(4 Classes) 52% 45% 53% 49% 55% 79% 55% 77% 56% 77%

(5 Classes) 49% 45% 50% 45% 54% 77% 54% 75% 55% 74%

By analyzing the results, it can be observed that similar to the first area "Area1":  Using the Normalized DSM instead of the DSM greatly improved the classification results, and in this area the improvement was 23%. Similar to Area1; the NDSM represented the absolute height of objects which facilitate distinguishing roads from buildings.  The radiometric corrected intensity values produced less accurate classification results than the original intensity values (around 5% decrease in the overall accuracy), however, by visually inspecting the results, the radiometric corrected intensity correctly classified the trees in the southwest corner (Kappa statistics for the Tree class is 0.22 for the classification of the original intensity, and 0.76 for the classification results of the radiometric corrected intensity). The reduction of the classification results was because of the misclassification of the roads as buildings, where both roads and buildings have same reflectance values and both have smooth surfaces.  The results of the combined LiDAR layers (intensity and DSM) were more accurate than the classification results of the aerial imagery (by around 6%). That is because the roads and buildings in the aerial imagery have the same reflectance, which led to misclassification of roads as buildings.   Adding the texture of the intensity data to the intensity and the elevation layers did not improve the classification results. Including the slope of the elevation layers had almost no effect on the classification results.

85

From the previous observations, it can be concluded that:   Both the radiometrically corrected intensity and elevation data had main roles in the classification, Using the normalized elevation data instead of the elevation data improved the classification results. Therefore, for the classification of the third study area, Area3, the cases that did not improve the classification results were excluded. Figure 4-14 represents the interpolated data, intensity image, DSM, and DTM, of the study area "Area3." Following same procedure of the pixel-based classification workflow (Figure 3-3), the study area, Area3, was investigated. Figure 4-15 shows the Normalized DSM and the texture of intensity layers that were extracted from LiDAR elevation and intensity data and used as auxiliary layers in the classification process.

a ) Intensity

b) DSM

c) DTM

Figure  4-14: Interpolated Data of "Area3"
a) Intensity image, b) the DSM, and c) the DTM

86

a) NDSM

b) Texture of Intensity

Figure  4-15: Extracted Layers for "Area3"
a) Normalized DSM, b) Texture of the Intensity

The classification results of the five distinguished land cover classes are illustrated in Figure 4-16. For the evaluation of the classification results, 500 reference points were randomly well-distributed over the area and the ground validation was collected from the ortho-rectified aerial imagery; the distribution of the reference points is illustrated in Figure 4-17, and the number of points in each class is listed in Table 4-6. The confusion matrix approach was used for assessing the classification results, and the accuracy of each case is summarized in Table 4-7. The confusion matrices for the mentioned cases and the Kappa statistics calculations for each class are illustrated in Appendix A.

87

Aerial Imagery

DSM

Original Intensity

Intensity

Intensity and DSM

Buildings Grass Roads Soil Trees

Intensity and NDSM

Intensity, DSM and Texture

Intensity, NDSM and Texture

Figure  4-16: Pixel-Based Classification Results for "Area3"
88

Figure  4-17: The Distribution of the Reference Points on, "Area3" Table  4-6: The Number of Reference Points in each Land Cover Class in "Area3"

Class Buildings Grass Roads Soil Trees

Number of Reference Points 77 50 191 80 102

Table  4-7: Accuracy Assessment of Pixel-Based Classification Results for "Area3"

Case 1 2 3 4 5 6 7 8

Band Combination Aerial Imagery DSM Original Intensity Intensity Intensity, DSM Intensity, NDSM Intensity, DSM, Texture Intensity, NDSM, Texture

4 Classes 60% 36% 50% 51% 46% 58% 51% 61%

5 Classes 59% 30% 49% 49% 44% 58% 49% 61%

By studying the classification results for the third study area, it can be noticed that the results are similar to the results of Area1:   Using the Normalized DSM instead of the DSM improved the classification results (12%­14% improvement). The radiometric correction of the intensity had no effect on the classification results. However, the produced classes from the radiometric corrected intensity were more homogeneous than the classes produced from the original intensity.  Adding the texture of intensity data to the intensity and the elevation layers did slightly improve the classification results (3-5% improvement).

89

From the previous observations, it can be verified that the conclusions derived from the previous study areas were verified in the third one, which had more complex land cover features:     Producing a land cover image out of the LiDAR data independent of external auxiliary data is comparable to the produced land cover image out of the aerial imagery. Both the radiometric corrected intensity and elevation data had main roles in the land cover classification of LiDAR data. Using the normalized elevation data instead of the elevation data improved the classification results. The texture of the intensity layer had a slight improvement on the quantitative classification accuracy (overall accuracy). 4.4.1.2 Object-Based Classification

As illustrated in Figure 4-18, the data preparation step started by extracting intensity and elevation point data, interpolating and converting the point data into raster images. The point data were already interpolated and converted into raster grids in the pixel-based classification (Section 4.4.1.1). Therefore, there was no need to repeat this step of data preparation. Figures 4-6, 4-10, and 4-14 represent the resampling and interpolation results of the LiDAR data resulting in the intensity images, the DSMs and the DTMs of the three study areas. The normalized Digital Surface Models (NDSMs) that were generated in the pixel-based classification step by subtracting the DTM from the DSM were also used in the objectbased classification decision tree (Figures 4-7 a, 4-11 a, and 4-15a for the three study areas).

90

Data Preparation

Level-1 Classification
Homogeneous Surface Interpolating intensity values Interpolating the entire points (DSM) Non-Terrain Segmentation of Intensity Image Area > 100 m2 Heterogeneou s Surfaces Area < 100 m2

Level-2 Classification
Homogeneous Non-Terrain [Buildings] Heterogeneous Non-Terrain [Trees]

Intensity Points (x, y, I) LiDAR Point Cloud Data (x, y, z, I) Elevation Points (x, y, z)

Homogeneous Terrain [Roads]

Generation of NDSM

NDSM > 0

Terrain Interpolation of Terrain Points (DTM) NDSM = 0

Heterogeneous Terrain [Grass/Soil]

Figure  4-18: Workflow of the Object-Base Classification (Developed Desicion Tree)

Additionally, the intensity images were segmented into homogenous regions where the neighbouring pixels that fulfill the specified homogeneity criterion were gathered to form a segment. The homogeneity criterion was specified based on the variation of the intensity values within the neighbouring pixels. Several variation quantities were tried, and visually assessed based on the knowledge that the variance in the intensity values within the vegetation areas (grass or trees) is large enough to produce small segments, and that the variation in the intensity values within the manmade areas (roads and buildings) is small and will lead to large patches. Therefore, the intensity values were observed, resulting in a variation of 3 in the intensity values that may be found in the manmade areas, and more than that within the natural areas (trees and grassy areas). Thus, the homogeneity criterion within the segments was set to a variation of 3 in the intensity values of the neighbouring pixels. Figure 4-19 illustrates the segmentations of the three study areas based on the variation of the intensity values.

91

Study Area1

Study Area2

Study Area3

Figure  4-19: Segmentation Results of the Intensity Layer

The minimum area of the homogenous surfaces (minA) was defined as any region with an area greater than 100 square meters, and it was considered as a homogenous surface. This criterion was established based on the common minimum size of buildings in the urban areas in Canada (El-Ashmawy et al., 2011). The second criterion in level 1 was the minimum object heights (MinHob) to separate the terrain from the non-terrain surfaces. Zero value was chosen to separate the terrain as the DTM was generated

92

out of the points on the roads, grass, and soil areas, which leads to NDSM values equal to zero at these regions. In the Level-2 Classification step, the four Level-1 clusters were intersected to form the four Level-2 classes. Based on the characteristics of the four classes, the distinguished classes of the study areas were assigned to the four Level-2 classes as follows: Class 1: The homogenous intensity surfaces that are elevated above the terrain: buildings. Class 2: The heterogeneous intensity surfaces that are elevated above the terrain: trees. Class 3: The homogenous intensity surfaces that are attached to the terrain: roads. Class 4: The heterogeneous intensity surfaces that are attached to the terrain: grass/soil. By following the procedure of the object based classification, the land cover classification of the three study areas was conducted and the classification results are shown in Figure 4-20. The distinguished five land cover classes were reduced to four classes, where the grass and bare soil classes were combined into one class. That is because of the difficulty in differentiating the open spaces that were bare soil or covered by grass, and no certain criterion could be defined to differentiate between them. Same reference points that were used in the pixel-based classification logic were used to assess the accuracy of the classification results. Figures 4-10, 4-15, and 4-19 show the distribution of the reference points over the three study areas. Tables 4-2, 4-4, and 4-6 list the number of reference points on each class in the three study areas. Table 4-8 summarizes the overall accuracy of the object-based classification results for the three areas. The confusion matrices for the three study areas and the Kappa statistics calculations for each class are illustrated in Appendix A.

93

Study Area1

Study Area2

Buildings Grass Roads Trees

Study Area3

Figure  4-20: Object-Based Classification Results for the three Study Areas Table  4-8: Accuracy Assessment of Object-Based Classification Results for the Three Study Areas

Area Area1 Area2 Area3

Overall Accuracy 68% 60% 61%

94

To compare the results of the pixel-based and the object classification, the object-based classification results were compared to Case 5 of the multiple-layer combinations with pixel-based classification. That is because Case 5 had the same input data as the decision tree, intensity and DSM. From the results, it can be noticed that the object-based classification produced more accurate results by 13%, 5%, and 16% for the three study areas. However, in the decision tree the intensity image was segmented into homogeneous regions, which represented the texture of the intensity, and the terrain was separated into terrain and objects, which represented the NDSM. These layers were used as auxiliary layers in the pixel-based classification Case 8 (intensity, NDSM, and texture of intensity) and can be considered the comparable case to the objectbased classification. The classification results of Case 8 were more accurate than the object-based classification results by 9%, and 17% in the first two study areas and no difference in accuracies in the third area. These reductions in accuracies were a consequence of the misclassification of some grassy areas, which were classified as roads in the object-based classification. To overcome this reduction, the decision tree needs to be modified to include the intensity values as another input, which may improve the classification results. Table 4-9 illustrates the comparison between accuracies of the pixel-based and the object-based classification results. Figures 4-21, 4-22, and 4-23 illustrate the comparison between pixel-based and object-based classification results for Area1, Area2, and Area3 respectively.
Table  4-9: Comparison Between Pixel-based and Object-Based Classification Results (4 classes)

Pixel-Based Case Area1 Area2 Area3 55% 55% 46% Case 5 (Intensity, DSM) Case 8 (Intensity, NDSM, texture) 77% 77% 61% Object-Based 68% 60% 61%

95

a

55%

b

77%

Buildings Grass Roads Trees

c

68%

Figure  4-21: Comparing Pixel-based and Object-based Classification Results for "Area1"
a) Pixel-based (Intensity and DSM), b) Pixel-based (Intensity, NDSM, and Texture), and c) Object-based

96

a

55%

b

77%

Buildings Grass Roads Trees

c

60%

Figure  4-22: Comparing Pixel-based and Object-based Classification Results for "Area2"
a) Pixel-based (Intensity and DSM), b) Pixel-based (Intensity, NDSM, and Texture), and c) Object-based

97

a

46%

b

61%

Buildings Grass Roads Trees

c

61%

Figure  4-23: Comparing Pixel-based and Object-based Classification Results for "Area3"
a) Pixel-based (Intensity and DSM), b) Pixel-based (Intensity, NDSM, and Texture), and c) Object-based

4.4.2

Part-2: Point-Based Classification of LiDAR Data Using (Maximum Likelihood Classifier)

This section of the research introduces a classification approach that relies on the point-based classification logic using a supervised classifier. In this part, the classification was conducted on the original LiDAR points without resampling the points into a grid space (image), in order to avoid any loss of details associated with resampling points into 2D grids, as described in Section 3.3. After classifying the LiDAR point data, to produce a full populated land cover area, the results were resampled into a predefined grid space, and gaps between LiDAR footprints were filled using two different proposed approaches (described in Subsection 3.3.2.2). By studying the outcomes of the first part, it can be noted that the classification results of each LiDAR attribute values (elevation and intensity) separately produce lower accuracy than the classification results when both attribute values were included. Thus, there was no need to conduct the classification separately on any of the two attribute values. Hence, the introduced classification approach was conducted on both attribute values together (   ) of the LiDAR point cloud data. Furthermore, to get the benefit of all acquired data, all data of any area that was acquired several times with different acquisition characteristics (acquired from different flight lines) have to be included in the classification process. Thus, the classification of LiDAR point cloud data in this part included classification of single data-strip and multiple data-strips. The multiple data-strips were the data of an overlapped area of adjacent strips, which were collected from different flight trajectories. To conduct the point classification of LiDAR data for single and multiple data-strips, MATLAB code was developed. The

98

developed code implemented one of the existing classification algorithms (Maximum Likelihood classifier was selected for this work), to be applied to both attribute values of the LiDAR point data. The study area "Area3" was selected to conduct this work as it is located in the overlapped area of Strip4 and Strip6. Moreover, it is a small area with a variety of land cover types and with different elevations. The workflow of the point data classification approach was described in Section 3.3. As shown in Figure 3-5, this approach is applicable for both single and multiple data-strips. The introduced point classification approach consists of three stages: data preparation, classification, and evaluation. The first and last stages are the same for single and multiple data-strips. In the classification stage, there were two sub-stages: Stage 2A, which is applicable to single and multiple data-strips, and Stage 2B, which is applicable to multiple data-strips only. In the data preparation stage, the same training sites that used in the first part were used here. A grid space was created to represent the whole area; this grid was similar to the one created in the first part. Nevertheless, the elements of the new created grid were points instead of pixels of the previous raster grid. The distance between the grid points was 0.2 m, which was equivalent to the pixel size of the raster grid (Part-1). The last step in the data preparation stage was the collection of ground validation of the reference points. The same reference points that were used to evaluate the classification results of the Pixel and object-based classification were used here. Yet another set of reference points were randomly distributed over the whole grid space, and their ground validation was collected from the ortho-rectified aerial imagery to assess the final land cover classification image. The details of the classification procedure for both single and multiple data-strips of this area are as follows: 4.4.2.1 Classification of Single Data-Strip

The classification of a single data-strip contained the classification of the original point data and assigning the appropriate land cover classes to the grid points. The developed MATLAB code was applied to the point data based on the attribute values in consideration to classify the points to the class with maximum probability (Maximum Likelihood classifier). The classified points, then, were resampled to the grid points, and additional MATLAB code was developed to assign the grid points to the appropriate land cover classes. The results of the classification of the original points of each of the data-strips (Strip4 and Strip6) separately based on elevation and intensity attribute values are illustrated in Figure 4-24.

99

Strip4

Elevation and Intensity Values Strip6

Buildings Grass Roads Soil Trees

Elevation and Intensity Values
Figure  4-24: Classification Results of the Original Points using the Point-Based Classification Logic with ML Classifier

For the evaluation of the point data classification, the defined 500 reference points were used with the confusion matrix approach. The overall accuracies of the classification results of Strip4 and Strip6 are 56% and 51%, respectively. By analyzing the classification results, it can be noticed that there is dissimilarity in the classification results of the different data-strips covering the same area. Finding a difference is expected, because each strip was acquired from a different flight trajectory with various acquisition characteristics and different spatial distributions. The procedure of assigning classes to the grid points was described in Subsection 3.3.2.2. Figure 3-6 demonstrates the process of assigning classes to the resampled points. The difference between the Nearest Neighbour and the Iterative Majority Moving Window methods is illustrated in Figure 3-7. The results of the classified grid points are illustrated in Figures 4-25 and 4-26 for data of Strip4 and Strip6, respectively. For evaluating the final classified grid space, the defined set of reference points on the grid space was used. Table 4-11 represents the accuracy assessment of the classified grid points for both data-strips. It can be noticed that the accuracies of the classification results obtained by either methods are the same. The confusion matrices with the Kappa statistics values for each class are illustrated in Appendix B.

100

a

b

Buildings Grass Roads Soil Trees

Figure  4-25: Classification Results of Grid Points For Strip4
a) Nearest Neighbour, and b) Iterative Majority Moving Window Method

a

b

Buildings Grass Roads Soil Trees

Figure  4-26: Classification Results of Grid Points for Strip6
a) Nearest Neighbour, and b) Iterative Majority Moving Window Method

Table  4-10: Accuracy Assessment of the Classified Grid Space for Both Data-Strips with NN and IM Filling Gaps Approaches.

Case Nearest Neighbour Iterative Majority Moving Window 4.4.2.2 Classification of Multiple Data-Strip

Strip4 58% 58%

Strip6 52% 52%

For land cover classification, to take advantage of acquiring an area from different flight lines, the multiple data-strips of this area should be considered in the classification. Each data-strip most likely covers the acquired area with different footprints; therefore, when data of all data-strips are used, smaller gaps appear, and more information can be obtained. The simplest way to use both data-strips is
101

merging the data-strips and classifying the entire data of the two data-strips together. Thus, the data of the two strips, Strip4 and Strip6, were merged into one file and the developed MATLAB code for point classification using the Maximum Likelihood algorithm was applied to the entire data (around 270,000 points). The classification results of the point data and of the points of the grid spaces are illustrated in Figure 4-27. Around 1000 reference points were used to assess the classification results of the merged data-strips. The confusion matrices with the Kappa statistics values for each class are illustrated in Appendix B.

49%

Classification Results of Original Merged Point Data
Buildings Grass Roads Soil Trees

50%

Classified Grid Points: Filling Gaps using the Nearest Neighbour Method

50%

Classified Grid Points: Filling Gaps using the Iterative Majority Moving Window Method Figure  4-27: Classification Results of the Merged Data of the two data-strips

Opposite of what was expected, the classification results of the combined data are less accurate than each of the separate data-strips. However, by visual inspection, some parts of the study area were improved after combining the data. Figure 4-28 shows two examples of the improvements, where the buildings had more regular shapes. To improve the classification accuracy, other methods for combining the data were investigated. As mentioned in the radiometric correction Subsection (4.3.2), a histogram matching approach was applied to the data of the overlapped areas to further improve the surface reflectance. The resulted data of the histogram matching were a combination of the two data-strips after adjusting the intensity

102

values. This correction is called an intensity normalization process. These radiometric corrected data were classified to check the effect of the normalized intensity on the classification results. The classification results of the combined data after intensity normalization are illustrated in Figure 4-29. The grid data were assigned the appropriate classes, as described earlier. The same reference points were used to assess the classification results of the combined normalized data. By observing the results, a slight improvement can be seen of the classification results after applying the intensity normalization; higher overall accuracy and more homogeneous classes were obtained.

Classified Grid Points of Strip4

Classified Grid Points of Strip6

Buildings Grass Roads Soil Trees

Classified Grid Points of the Merged Data (Strip4 & Strip6) Figure  4-28: Examples of the Classification Improvement after Merging the Data-Strips

Another method to get the benefit of the multiple data-strips is merging the classification results of the data-strips. After that, resampling the points into the grid space and assigning grid points to the appropriate classes were completed. Assigning classes to the grid points was done following only the Iterative Majority Moving Window method (IM), as described earlier with the single data-strip, since there was no difference between the two filling gaps methods. The classification results of the two combined classified data-strips before and after filling the gaps are shown in Figure 4-30. From the classification results, it can be seen that combining the classification results improved the obtained

103

accuracy of the classified points; however, the classification of the entire area is almost the same. The confusion matrices with the Kappa statistics values for each class are illustrated in Appendix B. 50%

Classification Results of Normalized Intensity Original Point Data 51%
Buildings Grass Roads Soil Trees

Classified Grid Points: Filling Gaps using the Nearest Neighbour Method 51%

Classified Grid Points: Filling Gaps using the Iterative Majority Moving Window Method
Figure  4-29: Classification Results of the Normalized Intensity Combined Data

54%

Combination of the Classification Results of Both Data-Strips 50%
Buildings Grass Roads Soil Trees

Classified Grid Points: Filling Gaps using the Iterative Majority Moving Window Method
Figure  4-30: Classification Results of the Merged Classified Data
104

Therefore, the Combined Multiple Classified Datasets (CMCD) technique was introduced in Subsection 3.3.2.5, (Stage 2A in Figure 3-5). As mentioned earlier, the decision of the final classification results was built on the classification results of both data-strips. The a posteriori probability of each class in each data-strip was calculated based on the values of its normalized confusion matrix. Hence, the confusion matrices were normalized. The confusion matrices and the normalized confusion matrices of the two data-strips are as shown in Figure 4-31. Confusion Matrix
Build Build 42 2 22 2 9 Build Build 45 0 26 1 6 Grass 2 36 4 2 6 Grass 1 35 5 2 5 Road 11 8 124 9 39 Road 42 29 86 7 21 Soil 2 10 20 34 14 Soil 6 17 19 39 4 Tree 19 6 24 9 44 Tree 18 9 17 10 50 Build Grass Road Soil Tree Build Grass Road Soil Tree

Normalized Confusion Matrix
Build 0.6341 0.0289 0.1782 0.0477 0.1111 Build 0.5761 0.0000 0.2856 0.0271 0.1111 Grass 0.0429 0.7382 0.0460 0.0678 0.1052 Grass 0.0200 0.6641 0.0860 0.0850 0.1449 Road 0.0838 0.0583 0.5066 0.1083 0.2430 Road 0.2228 0.1457 0.3915 0.0787 0.1612 Soil 0.0229 0.1093 0.1226 0.6142 0.1309 Soil 0.0473 0.1269 0.1285 0.6517 0.0456 Tree 0.2164 0.0653 0.1466 0.1619 0.4098 Tree 0.1337 0.0633 0.1083 0.1575 0.5372

Strip4

Grass Road Soil Tree

Strip6

Grass Road Soil Tree

Figure  4-31: Confusion Matrices and Normalized Confusion Matrices of both Data-Strips

The other factor that affected the decision of the final classification was the Inverse Distance Weighting factor (IDW). To calculate the IDW, the distances between each of the grid points and the nearest original points of the two data-strips, were determined. The proximity function (near) in ArcGIS was used. The IDW for each point was calculated using the formula in Equation (3-1). Based on the calculated factors, a posteriori probability and the IDW, the belief of each grid point belonging to each class was calculated using Equation (3-9). After that, each grid point was assigned to the class that has maximum belief. MATLAB code was developed to perform the CMCD technique. The classification results of the CMCD technique are shown in Figure 4-32. Using the same reference points with the confusion matrix approach, the classification results were assessed. The obtained overall accuracy of the classification results of the CMCD technique was 56%.

105

Buildings Grass Roads Soil Trees

Figure  4-32: Classification Results of the Two Data-Strips Using CMCD Technique

By observing the classification results of the overlapped areas, as listed in Table 4-11, it can be noticed that:
Table  4-11: Accuracy Assessment of the Combined Data-Strips by Various Approaches

Case Merged Points of Two Data-Strips Merged Points of Two Data-Strips with Normalized Intensity Merged Classified Points of Two Data-Strips Combined Classified Data using CMCD 

Overall Accuracy 50% 51% 50% 56%

Classifying the LiDAR data after normalizing the intensity values (applying histogram matching) slightly improved the classification results of the overlapped areas. That is because the histogram matching further improved the surface reflectance and the intensity values. By improving the intensity values, more accurate classification results can be obtained.



Combining the classified data by following the CMCD approach improves the classification results of the overlapped area by 6%. The CMCD approach considers the a posteriori probabilities of each class and the distance between the original point cloud data and the grid points.

By comparing the results of the point-based classification logic and the pixel-based classification logic, it can be seen that the point-based classification logic is more accurate than the pixel-based logic by around 12%. That is because when the point data are classified without resampling into a 2D grid, no losses occur in the original attribute values. Figure 4-33 illustrates the difference in the classification results between the point-based and the pixel-based logics.

106

44%

Pixel-based Classification results of intensity and DSM 56%

Buildings Grass Roads Soil Trees

Point-based Classification results of intensity and DSM
Figure  4-33: Comparison between Pixel-based and Point-based Classification

4.4.3

Part-3: Classification of LiDAR Data in Point Format Using Kurtosis Change Curve

The third part of the experimental work proposed a new approach for land cover classification of LiDAR point cloud data. This approach uses the point-based classification logic as well, but is built on the statistical analysis segmentation technique. The proposed classification approach depends on the kurtosis values of the elevation and intensity attribute values of LiDAR point data. To investigate the new classification approach the study area, Area3 was selected. This area was the smallest of the three study areas; however, it was complex enough to drive conclusions about the new approach. The study area, "Area3", contains a variety of land cover types with various elevations. The workflow of this approach, as illustrated in Figure 3-8, consists of two stages: an unsupervised segmentation stage based on statistical analysis of the attribute data of LiDAR point cloud data, and a supervised classification stage using an existing classifier. MATLAB code was developed and applied to the selected study area. The details of this experimental work are described below. The modified kurtosis change curve segmentation algorithm was applied to the LiDAR point cloud data, first based on the elevation values of the entire dataset. Afterwards, it was re-applied to the produced segments to cluster each segment into sub-segments based on their intensity values. To segment the data based on the elevation values, the point cloud data were sorted and the kurtosis values for the elevation attribute values were calculated. By removing the point with the highest elevation, the first cycle was completed. The kurtosis value of the remaining points was calculated, and then the highest point was removed (cycle 2). This process continued until the number of points reached the minimum number of points. To calculate the minimum number of points, the formula in Equation (3-12) was used,
107

with marginal error of 5% for both elevation and intensity values. These values were selected after observing the characteristics of the study area. After calculating the kurtosis values for all cycles, the kurtosis change curve was plotted and the peaks and pits of the curve were determined. The last peak/pit was selected as a separation point. The cycle number corresponding to this point was identified. The elevation value at this cycle number was used as a separator. The points with an elevation value less than the separator were clustered in a unique segment, segment1. After separating this segment, the same steps were repeated iteratively on the remaining points to cluster all points into segments. The output of this process was an additional field in the data file containing the segment number. When this process was applied to the elevation values of the entire data, 34 segments were produced. Figure 4-34a illustrates the segmented point data based on the elevation values.

a)

b)
Figure  4-34: Iterative Single-Segment Separation Method

Thereafter, the point data were sorted based on the segments numbers, and the Iterative SingleSegment Separation process was applied to the intensity values of each produced segment. That was to cluster the produced segments into sub-segments based on the intensity values. A new field then was added to the data file containing the intensity segments numbers. Another field containing the subsegments numbers, which were the combination of the elevation and intensity segments, was added. When this process was performed on the 34 produced elevation segments, a total of 100 sub-segments were formed (Figure 4-34 b).

108

The second stage, as illustrated in Figure 3-12, was a supervised classification stage, where the appropriate land cover classes were assigned to the produced sub-segments using one of the existing supervised classification techniques. In this research, the Maximum Likelihood supervised classification technique was conducted to assign the produced sub-segments into the appropriate classes of the five distinguished land cover types. Since the Maximum Likelihood Classifier is parametric, the training sites had to be tested to ensure that they are normally distributed. However, the selected training sites were the same as those used in the previous parts; therefore, no additional tests had to be done. The subsegments, produced in the first stage, were treated as single pixels or single objects that had to be assigned to one of the distinguished classes. Using the "spectral" center vector of the considered subsegment, which was the vector of means of the elevation and intensity values, the probabilities of the center belonging to the five distinguished classes were calculated. The entire points of this sub-segment were, then, assigned to the class with the maximum probability. Figure 4-35 illustrates the classification results of the study area using the proposed approach.

a)

Buildings Grass Roads Soil Trees

b)
Figure  4-35: Classification Results Using the Proposed Classification Approach Based on Statistical Analysis Technique
a) Original Points, b) Grid Points

For assessing the classification results, same 500 reference points that were used in the previous research (Part-1, and Part-2) were used here. A confusion matrix was formed, and the overall accuracy was calculated. The overall accuracy of the classification results using the proposed classification approach was 53%. The confusion matrices with the Kappa statistics values for each class are illustrated in Appendix C.

109

By comparing the classification results of the pixel-based classification logic and the point-based proposed classification approach, it can be noted that the proposed approach (after filling the gaps using the IM moving window method) is more accurate than the pixel-based logic by around 9%. That is because the proposed approach dealt with the attribute values of the individual original points without resampling into 2D grid. However, by comparing the classification results of the point-based logic (Part2) and the proposed approach (Part-3), the proposed approach is less accurate than the point-based classification by around 4%. Therefore, further investigation is required to improve the proposed classification technique to include the spatial location of the data in the segmentation process. Figure 436 illustrates the difference in the classification results between the three implemented classification approaches (three parts of the methodology). 44%

Pixel-based Classification results of intensity and DSM

58%

Buildings Grass Roads Soil Trees

Point-based Classification results of intensity and DSM

54%

Results of the Proposed Classification Approach Based on the Statistical Analysis
Figure  4-36: Comparison between the Three Implemented Classification Approaches

4.4.3.1

The Proposed Classification Approach for Large Areas

By following the proposed ways to accelerate the processing time of the statistical analysis proposed approach, the Multiple-Segments Separation methods were applied to the data based on the elevation values at the beginning, and then based on the intensity values. Eight segments were produced when the algorithm was applied to the data based on the elevation values, (Figure 4-37a), and total of 111 sub-segments were produced when the algorithm was re-applied based on the intensity values, (Figure
110

4-37b). After applying the Maximum Likelihood supervised classification algorithm on the 111 segments, the classification results were presented, as illustrated in Figure 4-38. The processed time reduced to one sixth of the process time when the Iterative Single-Segment separation process was used.

a)

b)
Figure  4-37: Segments of the Multiple-Segments Separation Method
a) Segments of the elevation data, b) sub-segments of intensity data

a)

Buildings Grass Roads Soil Trees

b)
Figure  4-38: Classification Results of the Proposed Statistical Analysis Approach Using the Multiple-Segments Separation Process
a) Original Points, b) Grid Points

For assessing the classification results, the same 500 reference points were used to form the confusion matrix. The overall accuracy of the classification results was 36%. That means that the MultipleSegments Separation process did not lead to acceptable results because of the very low achieved classification results. The other introduced method for accelerating the results was reducing the size of the data. Therefore, MATLAB code was developed to divide the entire data into a number of partitions based on a selected
111

number of points within each partition. Selected sizes of 20,000 and 10,000 points per sub-area were checked using the Iterative Single-Segment Separation and the Multiple-Segment Separation methods. The results of the classification for the 20,000 points size are illustrated in Figures 4-39 and 4-40, and the 10,000 points are illustrated in Figures 4-41, and 4-42. The accuracies of the classification results are listed in Table 4-12.

a)

Buildings Grass Roads Soil Trees

b)
Figure  4-39: Classification Results of the Proposed Classification Approach for the Partitioned Areas (20,000 points) Using Iterative Single-Segment Separation Process
a) Original Points, b) Grid Points

a)

Buildings Grass Roads Soil Trees

b)
Figure  4-40: Classification Results Using the Proposed Classification Approach for the Partitioned Areas (20,000 points) Using Multi-Segment Separation Process
a) Original Points, b) Grid Points

112

a)

Buildings Grass Roads Soil Trees

b)
Figure  4-41: Classification Results Using the Proposed Classification Approach for the Partitioned Areas (10,000 points) Using Iterative Single-Segment Separation Process
a) Original Points, b) Grid Points

a)

Buildings Grass Roads Soil Trees

b)
Figure  4-42: Classification Results Using the Proposed Classification Approach for the Partitioned Areas (10,000 points) Using Multi-Segment Separation Process
a) Original Points, b) Grid Points

Table  4-12: Accuracy Assessment of the Partetioned Classified Data

Overall Accuracy Size of Area Iterative Single-Segment Separation Process Entire Area 20,000 points 10,000 points 53% 56% 56% Multi-Segment Separation Process 36% 49% 56%

113

From the results it can be seen that the classification using the Multi-Segments Separation process is less accurate than the Iterative Single-Segment Separation process. The difference between the two methods is reduced by reducing the size of the partitioned areas. By observing the figures it can be seen that the results of the Multi-Segments Separation process is more homogeneous than the Iterative Single-Segment Separation process.

114

5. Results and Analysis: Multi-Wavelength (Multi-Spectral) LiDAR Data
5.1 Overview
Optech, one of the superior LiDAR sensor developers, has constructed the first multi-spectral airborne LiDAR sensor. The new LiDAR sensor, called "Optech Titan", includes three active laser beams operating at wavelengths of 0.532 m, 1.064 m, and 1.550 m. By combining the multi-spectral LiDAR data collected by the three wavelengths, more reliable and accurate information can be extracted, compared to the single wavelength LiDAR sensors. This chapter describes the use of multi-spectral LiDAR data for land cover classification using methodologies described in Chapter 3. The investigated dataset in this chapter is a multi-spectral LiDAR data acquired by Optech Inc. The following sections describe the dataset, the study area, and the experimental work. The chapter also includes an in-depth discussion of the results and the use of multi-spectral LiDAR data in land cover classification.

5.2 The Dataset and Study Area
The Titan sensor has a 300 kHz effective sampling rate for each channel and a combined ground sampling rate approaching 1 MHz. The Titan system operates in 0.35 mrad beam divergence for Channels 1 and 2, and approximately 0.7 mrad for Channel 3. LiDAR data was acquired from flying height of around 1000 m that creates a strip swath width of about 800 m. The data were collected in a strip length of 4 km by 800 m (Figure 5-1 b). The scanned area, during the data collection mission, is a flat urban area south of Oshawa Municipal Airport of the city of Oshawa, Ontario, Canada. Figure 5-1a illustrates the captured area as it appears in a Google map. This area contains a variety of land cover types including: buildings with different roof materials, roads and drive ways, scattered trees and areas covered by dense trees, and open spaces with and without grassy coverage. The provided data were delivered in LAS format files for the three channels (separate file for each channel). The data include the GPS trajectory data with x, y, and z coordinates, the linearized intensity values in 8 bit, the time tag of each signal, the number of returns associated with each signal (up to four returns), and the return number for each point. The provided data had point density of about 3.5 points/m2.

115

(a)

(b)

(c)

Figure  5-1: Location of the Captured Data and the Study Area

A small area of around 120 x 120 m (Figure 5-1 c) was clipped out of the LiDAR data strip to conduct the experimental work. The clipped area contained around 50,000 points in each channel and was used to apply the developed classification approaches that were investigated with the single wavelength LiDAR data. The selected area is an urban flat area that contained various land cover types, as explained
116

before. No aerial images were provided with the data, therefore, high resolution ortho-images were downloaded from USGS site to be used as a reference. Channel 1
114.953 115

(1.550 m)

Channel 2 (1.064 m)
59

52 48 45 42 39 37 35 33 31 29 27 24 21 19 16 13 11 8

Channel 3 (0.532 m)

5 4.78971

Figure  5-2: Point Cloud Data (Intensity Values) of the Three Channels for the Multi-Spectral LiDAR Data

5.3 Data Preparation
The provided data were geometrically corrected; therefore, no geometric correction was required. To make the dataset available for conducting the land cover information extraction, the LiDAR point cloud
117

data were first corrected from the radiometric errors following the same procedure that was applied to the previous single wavelength LiDAR dataset. The radar range equation was used, where the LiDAR intensity values, the range values, the atmospheric attenuation, and the incidence angle of the laser pulses were used as described in Shaker et al. (2011). The intensity and elevation values of the point cloud data for the three channels are illustrated in Figures 5-2 and 5-3, respectively. Channel 1

Channel 2

Channel 3

92.0 91.0 90.0 89.0 88.0 87.0 86.0 85.0 84.0 83.0 82.0 81.0 80.0 79.0 78.0 77.0 76.0 75.0 74.0 73.0 72.0

Figure  5-3: Point Cloud Data (Elevation Values) of the Three Channels for the Multi-Spectral LiDAR Data

118

5.4 Classification Results and Analysis
5.4.1 Part-1: Classification of LiDAR Data in Raster Format

In Part-1 of the methodology, the LiDAR data were converted into images and either a pixel-based supervised classifier was used, or the pixels were converted into objects and classified using a developed decision tree classifier. The Kriginig interpolation technique was used to resample and interpolate the LiDAR data for each channel into two images--the intensity image based on the intensity values  , and DSM based on the elevation values  of the first return points. 5.4.1.1 Pixel-Based Classification

The pixel-based classification workflow started with a data preparation step, where the point cloud data were converted into raster format for each of the considered attribute values. The three LAS files were separated into ground and non-ground points using the LASTools software, and converted into ASCII files. The provided data had smaller gaps between LiDAR footprints than the single-wavelength LiDAR dataset. When the Kriging interpolation technique was used, the small gaps were filled with data. For each channel, intensity image, DSM of the first returns, and DTM of the ground points were generated. Figure 5-4 illustrates the three images (intensity, DSM, and DTM) for each channel. For layer combination, several combinations of layers were conducted where two or more layers were combined into one multi-layer raster image. The investigated images with single or multi-layer combinations were: 1. Intensity images (for each channel separately). 2. DSM (for each channel separately). 3. Intensity and DSM (for each channel separately). 4. Intensity and NDSM (for each channel separately). 5. Intensity of the three channels in three-layer image. 6. Intensity and DSM of the three channels in six-layer image. 7. Intensity of the three channels and DSM of one channel in four-layer image. Based on the findings of the classification results of the single-wavelength LiDAR dataset (described in Chapter 4) it was noticed that including the Normalized DSM (NDSM) instead of the DSM in the classification improved the classification results, while including the texture of the intensity did not.
119

Therefore, the Normalized DSM (NDSM) layers of the three channels were extracted out of the elevation data by subtracting the DTM from the DSM. Figure 5-5 illustrates the extracted NDSM for each channel.

Intensity C1

DSM

DTM

C2

C3

Figure  5-4: Intensity images, DSM, and DTM of the Three Channels

120

Channel1

Channel2

Channel3

Figure  5-5: Extracted Normalized DSM layers of the Three Channels

The Minimum Distance to Means image classification algorithm was applied to the different layer combinations. That is because the selected training areas for supervised classification did not follow normal distribution; hence the Maximum Likelihood classifier would not be applicable. The classification results of the study area based on the mentioned cases are illustrated below. There were four distinguished classes in the study area: buildings, trees, roads, open spaces covered or not by grass. Figure 5-6 presents the classification results of the intensity images and the DSM of each channel. Figure 5-7 demonstrates the classification results of the multi-layer images intensity and DSM, and the intensity and NDSM images of each channel. Figure 5-8 shows the classification results for the multi-Layer images as mentioned in Cases 5, 6 and 7.

121

Intensity Images C1

DSM

C2

Buildings Grass/Soil Roads Trees

C3

Figure  5-6: Pixel-Based Classification Results for Intensity images and DSM Separately

122

Intensity and DSM C1

Intensity and NDSM

C2

Buildings Grass/ Soil Roads Trees

C3

Figure  5-7: Pixel-Based Classification Results for Intensity and DSM images, and Intensity and NDSM images

123

a) Intensity Channels

of

3

b) Intensity and DSM of the 3 Channels Buildings Grass /Soil Roads Trees

c) Intensity of the 3 Channels and 1 DSM layer.

Figure  5-8: Pixel-Based Classification Results for the Multi-Layer images a) three-layer image (3 intensity layers), six-layer image (3 intensity and 3 DSM layers), and four-layer image (3 intensity and
1 DSM layers)

The classification results for all cases were assessed using the confusion matrix approach. Welldistributed reference areas were selected out of the study area, and these areas are represented by around 57,000 grid points. These points were used to evaluate the classification results. The ground validation information was collected from the downloaded aerial images. Figure 5-9 shows the distribution of the reference areas/points over the study area. Table 5-1 shows the number of reference points in each class. Appendix D illustrates the confusion matrices and the Kappa statistics for all cases. The accuracy assessment of the mentioned classification cases is illustrated in Table 5-2.
124

Figure  5-9: The Distribution of the Reference Points within the Study Area Table  5-1: The Number of Reference Points in each Distinguished Class within the Study Area for Multi-Spectral LiDAR data

Class Buildings Grass/Soil Roads Trees Total

Number of Areas 8 5 3 3 19

Number of Reference Points 10382 18233 23014 5298 56927

By observing the classification accuracies, it can be noted that: 1. Using the intensity images only or DSM in land cover classification produced low accurate results (overall accuracy less than 50%). However, combining the intensity values and the DSM improves the classification results by around 11­30%, 20­49%, and 17­32% for the three channels. 2. Using the normalized DSM instead of the DSM in the classification did not improve the classification accuracy. That is probably because the study area is a flat area, so no advanced information is added by using the NDSM. 3. Combining the intensity images of the three channels produces more accurate results than each channel separately (around 18­30% improvement). By combining the three channels, more information was included in the classification, which enriched the classification results.

125

Table  5-2: Classification Accuracy for the Pixel-Based Classification Logic for the Multi-Spectral LiDAR data

Case Intensity images of Channel 1 Intensity images of Channel 2 Intensity images of Channel 3 DSM of Channel 1 DSM of Channel 2 DSM of Channel 3 Intensity and DSM of Channel 1 Intensity and DSM of Channel 2 Intensity and DSM of Channel 3 Intensity and NDSM of Channel 1 Intensity and NDSM of Channel 2 Intensity and NDSM of Channel 3 Intensity of the three channels in one three-layer image Intensity and DSM of the three channels in one six-layer image Intensity of three channels and DSM of one channel in one four-layer image

Overall Accuracy % 64 73 61 45 44 46 75 93 78 76 93 81 91 94 93

4. Adding the DSM layer to the intensity of the three channels slightly improved the classification results (2­3% improvement). This means that the intensity of the three channels had a more important role than the DSM. 5. The classification results of Channel 2 (either intensity, or intensity and elevation) are more accurate than any of the other channels. That is probably because Channel 2 is operated with 1.064 m wavelength, which has large separability between the land cover classes of urban areas. 6. Adding three layers of the DSM (layer from each channel) did not improve the results greatly (1% improvement). Therefore, adding any of the DSM layers would lead to similar results. 5.4.1.2 Object-Based Classification

The object-based classification workflow started with the data preparation step. The point cloud data were separated into intensity and elevation files, and then interpolated and converted into raster format. The intensity data file (, , ), the elevation data file (, , ) for the first return, and the elevation data file for the ground points (, , ) were separated. The terrain points were determined
126

by using the ground module in the LASTools software. To convert the point data into raster format and to fill the gaps between the LiDAR footprints, Kriging interpolation technique was used (Figure 5-4). The procedure of the object based classification, illustrated in Figure 3-4, was followed in order to classify the LiDAR data into the distinguished four land cover classes: buildings, grass/soil, roads and trees. During the data preparation step, the intensity data were segmented into homogenous regions and the elevation data were used to determine the object heights. For intensity segmentation, the neighbouring pixels that fulfill a specified homogeneity criterion were gathered to form a segment. Based on the variation of the intensity values within each of the manmade classes, homogeneity criteria were specified. The specified criterion for homogeneity was that the Euclidean spectral distances between the neighbouring pixels had to be less than 3.5. Figure 5-10 illustrates the segmentation results of the intensity image (of the three channels together) based on the specified criterion. To determine the heights of objects, the normalized Digital Surface Model (NDSM) was generated by subtracting the DTM from the DSM, shown in Figure 5-5.

Figure  5-10: Segmentation Results of the Intensity Layer

By following the decision tree that is illustrated in Figure 3-4, with selecting the MinA equal to 50 m2, the MinA was used to separate the homogeneous and heterogeneous areas. This value was specified by observing the sizes of buildings within the study area. The ground was separated out of the whole area using the ground module of LASTools Software. The four Level-1 clusters were then intersected to separate the four distinguished land cover classes. The classification results are represented in Figure 511.

127

Buildings Grass /Soil Roads Trees

Figure  5-11: Classification Results Using Object-Based Classifier

To assess the accuracy of the classification results, the same reference areas, which were used in the pixel-based classification logic, were used for more reliability in comparing the classification results of the two logics. The accuracy of the object-based classification results was 62%. This classification result is comparable to the classification result of Case 6 in the pixel-based classification (classification of the intensity and DSM of the three channels in one six-layer image), where the pixel-based classification accuracy was 94%. 5.4.2 Part-2: Point-Based Classification of LiDAR Data Using Existing Classification Algorithm

The second part is a classification of LiDAR point cloud data using point-based classification logic with one of the existing classifiers. In this part, the classification was conducted on the original LiDAR points without resampling the points to a grid space. Afterwards, to produce a land cover image, the classified data were resampled into a predefined grid space and gaps between LiDAR footprints in the entire area were filled using the proposed approach. The developed MATLAB code based on the Minimum Distance to Means algorithm was used to apply the point-based classification logic to the study area. The workflow of the point data classification approach, as described in Section 3.3, was followed. The introduced point classification approach consists of three stages: data preparation, classification, and evaluation. In the data preparation stage, the distinguished land cover classes within the study area were defined. For a more reliable comparison between the pixel-based and point-based classification logics, the Minimum Distance to Means classifier was applied to the point data. The same training sites that were used in the first part were used here to train the classifier. Consequently, a grid space was created to represent the whole area. The space between the grid points was 0.2 m, which was
128

equivalent to the pixel size of the raster grid (in Part-1). The last step in the data preparation stage was the collection of ground validation of the reference points. The same reference areas that were used to evaluate the classification results of the Pixel and object-based classification were used here. The proposed approach for point data classification that is described in Section 3.3 was applied. The developed MATLAB code was applied to the point data based on the attribute values in consideration to classify the points to the most appropriate class. The classification procedure was conducted for each channel based on the attribute values (intensity and elevation) separately and together. After that, the classification results of each channel were combined by gathering the classified points into one file and resampling them to the grid points and filling the gaps. The proposed combination method, CMCD, was followed as well to produce the final classified grid points based on the classification of each channel and the a posteriori probabilities of the classification results. The classified points, then, were resampled to the grid points, and additional MATLAB code was developed to assign the grid points to the appropriate land cover classes using the CMCD technique. The results of the classification are illustrated in Figures 5-12 to 5-14. These figures illustrate the classification results of the original points (first column) and the full grid points (second column) of each channel separately based on intensity values (Figure 5-12), elevation values (Figure 5-13), elevation, and intensity values (Figure 5-14). Since the point cloud data of the three channels do not coincide, as shown in Figure 5-15, combining the data of the three channels to act as three different groups of attribute values (intensity and elevation) is not possible. Therefore, combining the classification results is required to get benefit of the advantage of the multi-spectral LiDAR data. To combine the classification results, the two approaches that were followed for the multiple data-strips in the previous chapter were applied to these data channels.

129

Classification of the Original Points
C1

Classified Grid Points after filling the Gaps

C2

Buildings Grass /Soil Roads Trees

C3

Figure  5-12: Point-Based Classification Results for the Three Channels Based on Intensity Data

130

Classification of the Original Points
C1

Classified Grid Points after filling the Gaps

C2

Buildings Grass /Soil Roads Trees

C3

Figure  5-13: Point-Based Classification Results for the Three Channels Based on Elevation Data

131

Classification of the Original Points
C1

Classified Grid Points after filling the Gaps

C2

Buildings Grass /Soil Roads Trees

C3

Figure  5-14: Point-Based Classification Results for the Three Channels Based on Intensity and Elevation Data

132

Channel1 Channel2 Channel3

Figure  5-15: Points of the 3 Channels
 Points of Channel 1,  Points of Channel 2,  Points of Channel 3

Classification of the Original Points Merging C1, C2, and C3 with Intensity and Elevation values

Classified Grid Points after filling the Gaps

Merging C1, and C2 with Intensity and Elevation values

Buildings Grass /Soil Roads Trees

Figure  5-16: Point-Based Classification Results for the Combined Points of the Classified Channels Based on Intensity and Elevation Data

133

The first approach that was followed is to merge the classified points, resample the points into the predefined grid space, and fill the gaps. The second approach is to resample the classified points into the grid space, fill the gaps, and then combine the classified grid points using the CMCD technique. The results of the first approach are illustrated in Figure 5-16, where the results of the three channels were merged and the results of merging the two NIR channels (Channel 1 and Channel 2). Figure 5-17 illustrates the results of the CMCD for the intensity values and for the intensity and elevation values together.
Combining C1, C2, and C3 with Intensity

values using CMCD

Combining C1, C2, and C3 with Intensity and Elevation values using CMCD

Buildings Grass /Soil Roads Trees

Figure  5-17: Point-Based Classification Results for the Combined Points of the Classified Channels Using CMCD Combination Approach

To evaluate the classification results, the reference areas/points that were used to evaluate the pixel-based and object-based classification results were used with the confusion matrix approach. The accuracy of the classification results is listed in Table 5-3.

134

Table  5-3: Classification Accuracy for the Point-Based Classification Logic for the Multi-Spectral LiDAR data

Case

Intensity attribute values of Channel 1 Intensity attribute values of Channel 2 Intensity attribute values of Channel 3 Elevation attribute values of Channel 1 Elevation attribute values of Channel 2 Elevation attribute values of Channel 3 Intensity and Elevation attribute values of Channel 1 Intensity and Elevation attribute values of Channel 2 Intensity and Elevation attribute values of Channel 3

Overall Accuracy % Original Grid Points Points 55 57 65 47 36 37 38 68 81 59 70 48 42 43 45 72 86 65 79 80 73 88

Intensity and Elevation attribute values of the three channels (Merged 72 Classified Points) Intensity and Elevation attribute values of the two NIR channels (Merged 75 Classified Points) Intensity attribute values of the Three Channels combined by CMCD Intensity and Elevation attribute values of the Three Channels combined by CMCD By observing the accuracy of the classification results, it can be seen that:

1. The classification accuracy of all the DSM of each of the three channels is lower than 50%, and the accuracy of the intensity is higher than 50%. However, both attribute values, intensity and DSM, have a central role in the classification. Therefore, when the intensity and DSM were considered in the classification, the overall accuracy improved by 13­32%, 16­44%, and 12­21% for the three channels. 2. The CMCD combination approach produced more accurate results than merging the classified points of the two NIR channels or the three channels by 8 and 9%, respectively. That is because the CMCD includes the a posteriori probabilities of the classified data as well. 3. Combining the classification results of the elevation and intensity values of the three channels produced more accurate results than combining the classification results of the intensity values only (12% improvement).

135

By comparing the classification results of the pixel-based and point-based classification logics, it can be noticed that: 1. The classification accuracies of the pixel-based logic are more accurate than the point-based logic for the separate and combined attribute values for each channel (1­13% improvement). That is because in the pixel-based logic, the terrain was separated before applying the classification algorithm, which might lead to accurate results in the classification. 2. The classification accuracy of the combined intensity of the three channels using the pixel-based classification is more accurate than the CMCD of the classified points of the three channels by 18%. By combining the raster images into one multiple layer image, each pixel gets an attribute value from each layer. On the other hand, the point data could not be combined because the points of the different channels did not coincide. 5.4.3 Part-3: Classification of LiDAR Point Data Format with Kurtosis Change Curve Classifier

The third part of the work is applying the new proposed approach based on a statistical analysis segmentation technique on the multi-spectral LiDAR point cloud data. The proposed classification approach depended on the kurtosis values of the elevation and intensity attribute values of LiDAR point data independently of the spatial locations of the points. The work procedure, as illustrated in Figure 3.8, consisted of two stages: unsupervised segmentation stage based on statistical analysis segmentation technique, and supervised classification stage by applying one of the existing supervised classification algorithms on the produced segments. The following subsections describe the details of the experimental work. 5.4.3.1 Unsupervised Segmentation Stage

To divide the LiDAR data into segments, the Iterative Single-Segment Separation process described in Subsection 3.4.1, was used. The last inflexion point of the kurtosis change curve of the selected attribute value was considered as separator to segment the points with attribute values less than the value corresponding to the inflexion point into a unique segment. This process continued in an iterative process until all points were segmented. The segmentation results of the Iterative Single-Segment Separation process based on elevation values then intensity values are illustrated in Figure 5-18.

136

Iterative Single-Segment Separation process Segmentation Based on Elevation Values Segmentation Based on Elevation Then Intensity Values C1

C2

C3

Figure  5-18: Segmentation Results of the Three Channels Using Iterative Single-Segment Separation Process Based on Elevation then Intensity Values

137

5.4.3.2

Supervised Classification Stage

The second stage is assigning class values to the sub-segments using one of the existing supervised classification algorithms. In this stage each sub-segment produced in the first stage was treated as a pixel or a single object that had to be assigned to one of the distinguished classes, as illustrated in Figure 3-12. The Minimum Distance to Means classification algorithm was used to classify the segments. The classification results of each channel, for original points and grid points, are illustrated in Figure 5-19. Original Points C1 Grid Points

C2

Buildings Grass/Soil Roads Trees

C3

Figure 5-19a: Classification Results of the Produced Segments by Iterative Single-Segment Separation Process Using the Minimum Distance to Means Classifier
138

C1,2,3

C1,2

Buildings Grass/Soil Roads Trees

C1,2,3 Using CMCD

Figure 5-19b Figure  5-19: Classification Results of the Produced Segments by Iterative Single-Segment Separation Process Using the Minimum Distance to Means Classifier

To evaluate the classification results, the reference areas/points that were used to evaluate the pixelbased and object-based classification results were used with the confusion matrix approach. The accuracy of the classification results is listed in Table 5-4.

139

Table  5-4: Accuracy of Classification Results for the LiDAR Point Cloud Using the Iterative Single-Segment Separation process and the Minimum Distance to Means Classification Algorithm

Case

Intensity and Elevation attribute values of Channel 1 Intensity and Elevation attribute values of Channel 2 Intensity and Elevation attribute values of Channel 3

Overall Accuracy % Original Grid Points Points 70 72 72 62 76 63 76

Intensity and Elevation attribute values of the three channels (Merged 68 Classified Points) Intensity and Elevation attribute values of the two NIR channels (Merged 71 Classified Points) Intensity and Elevation attribute values of the Three Channels combined by CMCD

76

87

The same observations of the point-based classification logic can be noted with the results of the proposed approach. The CMCD approach produced more accurate results than each channel separately, and more than by merging the classified points. It can be also noted that the classification accuracy is comparable to the point-based classification. To accelerate the classification process, the Multiple-Segment Separation process, described in Subsection 3.4.3, was followed. Instead of using the last inflexion point of the kurtosis change curve in the segmentation stage, all inflexion points were used to divide the LiDAR point data into segments. As a result, the segmentation of the point data based on the elevation and the intensity for the three channels are illustrated in Figure 5-20. The second stage is assigning class values to the sub-segments using the Minimum Distance to Means classification algorithm. The classification results of each channel, for original points and grid points, are illustrated in Figure 5-21. The classification results of the combined data and the combined classified data are illustrated in Figure 5-21 as well.

140

Segmentation Based on Elevation Values

Segmentation Based on Elevation Then Intensity Values

C1

C2

C3

Figure  5-20: Segmentation Results of the Three Channels Using Multiple-Segments Separation Process Based on Elevation then Intensity Values

141

Original Points C1

Grid Points

C2

Buildings Grass /Soil Roads Trees

C3

Figure 5-21a: Classification Results of the Produced Segments by Multiple-Segments Separation Process Using the Minimum Distance to Means Classifier

142

C1,2,3

C1,2

Buildings Grass /Soil Roads Trees

C1,2,3 Using CMCD

Figure 5-21b Figure  5-21: Classification Results of the Produced Segments by Multiple-Segments Separation Process Using the Minimum Distance to Means Classifier

Table 5-5 summarizes the accuracy of the classification results; the confusion matrix approach was formed based on the same reference areas that were used before. It can be noticed that the accelerating method not only produced the classification results more quickly, but it also produced results with the same accuracy as the Iterative Single-Segment separation process.

143

Table  5-5: Accuracy of Classification Results for the LiDAR Point Cloud Using the Multiple-Segments Separation process and the Minimum Distance to Means Classification Algorithm

Case

Intensity and Elevation attribute values of Channel 1 Intensity and Elevation attribute values of Channel 2 Intensity and Elevation attribute values of Channel 3 Intensity and Elevation attribute values of the three channels (Combined Classified Points) Intensity and Elevation attribute values of the two NIR channels (Combined Classified Points) Intensity and Elevation attribute values of the Three Channels combined by CMCD

Overall Accuracy % Original Grid Points Points 62 64 77 57 66 83 58 72

70

74

89

5.4.4

Classification of LiDAR Point Data Using Rule-Based Method with a Developed Decision Tree

By observing the data, some criteria can be specified to separate the point data into the distinguished classes. Not only can the elevation and intensity values be used, but other rules can also be set based on other attribute values such as number of returns, point density, difference in elevation and intensity within certain small area, and standard deviation of the elevation and intensity values. Therefore, the required attributes to be extracted were determined within an area of 1 m2 and assigned to each point of the original point cloud data. The extracted attribute values were: 1) point density, 2) maximum number of returns, 3) difference in elevation between maximum and minimum values, 4) difference in intensity between maximum and minimum values, 5) standard deviation of the elevation values, 6) standard deviation of the intensity values, and 7) ground point detected by LASTools. The developed decision tree is illustrated in Figure 5-22.

144

Point Cloud Data

Ground Points

Non-Ground Points

Intensity  IRoads

Intensity > IRoads [Grass/Soil]

Z>H [Tree]

ZH

Stdv_I  IRoads

Stdv_I > IRoads [Grass/Soil]

Point Density D

Point Density >D [Tree]

Stdv_Z  ZRoads [Roads]

Stdv_Z > ZRoads [Grass/Soil]

Z_Diff > HBuild [Tree]

Z_Diff  HBuild

Stdv_Z  ZBuild

Stdv_Z > ZBuild [Tree]

H : Highest Elevation of Buildings D : Average Point Density over the entire dataset H-Build : Maximum Building Height -ZBuild : Standard deviation of Building's points elevation I-Build : Highest Intensity of the Buildings Dif-I : Maximum variation of intensity within Buildings -IBuild : Standard deviation of Building's points intensity I-Roads : Maximum intensity values of Roads' points -IRoads : Standard deviation of Roads' points intensity -ZRoads : Standard deviation of Roads' points elevation

Intensity > IBuild [Tree]

Intensity  IBuild

I_Diff  Dif-I

I_Diff > Dif-I [Tree]

Stdv_I > IBuild [Tree]

Stdv_I  IBuild [Build]

Figure  5-22: Rule-Based Classifier

By following the developed decision tree, four distinguished land cover classes could be separated for each channel. The classification results of the three channels are illustrated in Figure 5-23.

145

Original Points C1

Grid Points

C2

Buildings Grass /Soil Roads Trees

C3

Figure 5-23a: Classification Results Using the Rule-Based Classification

146

C1,2,3

C1,2

Buildings Grass /Soil Roads Trees

C1,2,3 Using CMCD

Figure 5-23b Figure  5-23: Classification Results Using the Rule-Based Classification

Table 5-6 summarizes the accuracy of the classification results of the Rule-Based classifier. The confusion matrix approach was formed based on the reference areas.

147

Table  5-6: Accuracy of Classification Results for the LiDAR Point Cloud Using the Rule-Based Classifier

Case

Intensity and Elevation attribute values of Channel 1 Rule-Based Intensity and Elevation attribute values of Channel 2 Rule-Based Intensity and Elevation attribute values of Channel 3 Rule-Based Intensity and Elevation attribute values of the three classified channels With Rule-Based (Combined Classified Points) Intensity and Elevation attribute values of the two NIR classified channels With Rule-Based (Combined Classified Points) Intensity and Elevation attribute values of the Three Channels combined by CMCD

Overall Accuracy for Classification of Original Points Grid Points 79 80 89 56 75 90 55 78

84

86

91

By observing the results of the different classification approaches, it can be noted that: 1. Considering the elevation values in addition to the intensity values in the classification improved the classification results. 2. Classification results of each channel were less accurate than the classification results of the entire dataset (three channels together). 3. Combining the classification results using the CMCD technique was more accurate than combining the classified points of the different channels by merging the points into one grid space without considering the a posteriori probabilities of each channel. 4. The classification results of the pixel-based classification logic were more accurate than the point-based logic. That is because when the data were converted into images with multiplelayers of the different channels, each pixel got multiple attribute values regardless of whether the original points of the different channels coincided or not. 5. The Decision Rules classification technique produced almost the same results as the point-based classification logic. Nevertheless, more attention is required in analysing the data for more specific criteria.

148

6. Summary, Conclusions and Future Work
This chapter summarizes the entirety of the work carried throughout this research. It begins with a summary of the work, handling and analysis of the results, the derived conclusions, and closes with future work requiring further attention.

6.1 Summary
6.1.1 Research Purpose

The main goal of this research was to maximize the benefit of the LiDAR data independently of any other sources of data for land cover information extraction. To achieve this goal, specific objectives were set. These objectives can be summarized as follows: · Studying the potential use of LiDAR data for land cover information extraction, and studying the effect of including other auxiliary layers extracted from LiDAR data in the classification process. · Developing a new point-based classification approach to classify LiDAR point cloud data without losing details of the 3D points. · Developing a Combined Multiple Classified Datasets (CMCD) approach for land cover classification of overlapped data-strips. · Developing a new classification approach for land cover classification of LiDAR point cloud data based on Kurtosis Change Curve algorithm. 6.1.2 Research Methodology and Experimental Work

To fulfill the research objectives, a methodology consisting of three parts was followed and applied to LiDAR data of urban areas. The selected urban areas contained different land cover types with different elevations. The first part of the methodology investigated the image classification techniques. Hence, point cloud LiDAR data were converted into intensity and DSM images. Other auxiliary layers were extracted from the intensity and DSM images to be included in the classification process. Several multilayers images were produced by combining two or more layers together into one image. The selected supervised classification algorithm was applied to the various images that were produced. Additionally, the produced LiDAR images were clustered into objects and classified based on certain criteria. The criteria used for classification were specified based on the characteristics of the data in the study areas. Results of the pixel-based and object-based classification logics were evaluated using randomly distributed reference points as part of the confusion matrix approach.
149

The second part of the methodology introduced a method to extract land cover information from LiDAR data without converting the irregular LiDAR points into a 2D grid to avoid losing any important details. Afterwards, to obtain a classified area, the classified points were resampled into grid space and the grid points were assigned to the appropriate classes. Gaps between LiDAR footprints were filled using two different approaches: Nearest Neighbour and Iterative Majority Moving Window. Furthermore, to achieve maximal benefit of data that were collected more than once from different trajectories, combining data of overlapped areas were investigated. Data of two overlapping adjacent strips were examined, and combined using different techniques: pre- and post-classification. Points of the two datastrips were combined pre-classification by merging the two data-strips into one file with and without normalizing the intensity. While post-classification, the classified points were either merged into one file or combined using the proposed Combined Multiple Classified Datasets (CMCD) technique. The CMCD technique utilized the concept of combining classifiers that was introduced in the pattern recognition field to benefit from multiple groups of data for an area (multiple data-strips). This was achieved by combining information obtained from each group of data based on the a posteriori probabilities of classified data. In the third part of the methodology, an innovative classification approach was suggested for LiDAR point data. The proposed approach classified the entire data into the distinguished classes altogether. It relied on the statistical analysis segmentation technique, where a proposed Iterative Single-Segment Separation method was introduced to cluster the data into segments regardless of the spatial location of the points. Afterwards, a supervised classification algorithm was applied to the produced segments to assign them into the appropriate classes. For the supervised classification, the sub-segments acted as separate objects or pixels, with the attribute values for this object represented by the attribute values of the spectral centre point7. However, these segments were not necessary attached spatially. The third part ended with introducing two different methods to accelerate the process; one eliminated the iterative process (Multi-Segments Separation process), and another reduced the size of the data by partitioning the study area into sub-areas, each containing a manageable number of points. The three parts of the methodology were applied to two different datasets. The first dataset was singlewavelength LiDAR data covering three urban study areas in British Columbia, while the second dataset was multi-spectral LiDAR data covering an urban area in Oshawa, Ontario.

7

The spectral centre point means the centre point of the segment on a scatter plot.

150

6.2 Results Discussion and Conclusions
6.2.1 Results Discussion

By converting the point cloud data of the single wavelength LiDAR dataset into raster images and applying the Maximum Likelihood algorithm, the following results were realized:  In most cases, the radiometric-corrected intensity values produce more accurate classification results compared to the original intensity values. This occurs due to the radiometric correction process correcting the surface reflectance from the geometric and atmospheric attenuations.  Both the radiometric-corrected intensity and elevation data had main roles in the land cover classification. By classifying any of them separately, the accuracy obtained was less than 50%. The overall accuracy reached 66% for the five distinguished classes when intensity and elevation values were included.  The classification results based on the LiDAR elevation and intensity attribute values are comparable to the classification results of the aerial imageries. The classification results of the aerial imagery were less than 60%. By observing LiDAR data, more information layers can be generated out of the intensity and elevation values, which can be used as auxiliary layers in the classification process. The normalized digital surface model (NDSM), the texture of the intensity, and the slope of the elevation surfaces are among the information layers that can be generated. By including these layers in the classification, and comparing the classification results, the following conclusions can be derived:    Using the normalized DSM (NDSM) instead of the DSM improved the accuracy of the classification results. This improvement ranged between 13-23%. Including the texture of the intensity layer slightly improved the classification results. However, classification results of the trees were improved. Although including slopes of the elevation layers did not improve the classification results, the slope of DSM images detect the borders of the buildings. By applying the object-based classification logic using the developed decision tree, which used homogeneous segments of the intensity and the NDSM as inputs, and comparing the results to the equivalent classified multi-layer image, the following conclusions can be derived:

151

 

The developed decision tree did not include criteria for separating open spaces covered by grass from those covered by bare soil. The produced classification results have a lower accuracy than those obtained by the pixelbased classification logic for Intensity, NDSM, and texture of intensity multi-layer images. Nevertheless, by visual inspection of the results, the land cover classes obtained by the objectbased classification had smoother surfaces and produced more homogeneous areas.

To preserve details of information associated with the 3D point data, converting the 3D point data into 2D grid points should be avoided. Therefore, extracting land cover information from LiDAR data with point-based classification logic was introduced. Since the LiDAR footprints are neither regular nor cover the whole area (gaps appeared between the footprints) to produce full populated land cover information, a procedure for converting the point data into regular grid after extracting the required land cover information was proposed. Two approaches were followed to assign classes to the grid points; Nearest Neighbour approach, and Iterative Majority Moving Window approach. For both approaches, the original points were resampled to the grid space and the grid points were assigned to the most-frequent class of the resampled points it coincided with. The rest of the grid points that did not coincide with any resampled points remained unclassified. Following that, one of the two gaps-filling approaches can be followed:   In the Nearest Neighbour approach, the remaining grid points were assigned the class of the nearest classified point before resampling. In the Iterative Majority Moving Window approach, a 3x3 moving window (kernel) passed through the unclassified grid points and assigned at the point at the centre of the window (kernel) to the majority class of the 8 neighbouring points. This process was repeated until all unclassified grid points were assigned to one of the distinguished land cover classes. By evaluating the final classification results and comparing the results to the pixel-based classification results, it can be concluded that:  The point-based classification logic produced more accurate results than the pixel-based logic (14% improvement for either Nearest Neighbour, or the Iterative Majority Moving Window approaches).

152



Both gap-filling methods (Nearest Neighbour, and the Iterative Majority Moving Windows) produced similar classification results. These methods are applicable to fill the gaps, however, some areas with fewer resampled points require more attention for more accurate results.

Acquiring data from different flight trajectories should be considered for land cover classification. To do so, data of different strips covering the same area were combined. There were two ways to combine the data and get benefit of each collected point. The first method is merging the data into one file before classifying the entire dataset. The second method is classifying each data-strip then combining the classified data through merging the classified points into one file, or combining the data using the proposed Combined Multiple Classified Datasets (CMCD) approach. Therefore, information of the two data-strips covering the same area was merged and classified using the Maximum Likelihood classifier. The merged data were radiometrically corrected with and without applying histogram matching. On the other hand, both data-strips were classified using the Maximum Likelihood classifier, then the classified data-strips were merged and evaluated, or combined using the CMCD approach. By applying these methods, the following was observed:   Merging several data-strips filled parts of the gaps between the LiDAR footprints. Although applying the Maximum Likelihood classifier on the combined data-strips reduced the accuracy of the classification results, some gaps that were filled with points were classified in a more appropriate way than their classification results before merging the data.    Carrying out histogram-matching on the data of the two strips increased the homogeneity between LiDAR points, and slightly improved the classification results (1% improvement). Merging the point data after classification improved the classification results by 4%. Applying the CMCD approach achieved more improvement in the classification accuracy (3-4% improvement). From literature, it was found that the statistical analysis segmentation technique was introduced as a robust technique for separating terrain points of LiDAR data, as well as detecting features. Based on the previously introduced statistical analysis segmentation techniques such as Skewness Balancing and Change curve algorithms, an innovative classification approach was proposed and investigated. The proposed approach consists of two stages; a segmentation stage based on the statistical analysis technique, followed by labeling the segments to the appropriate land cover classes using a supervised classification technique. A statistical analysis segmentation algorithm was proposed. The algorithm relies
153

on the concept of the kurtosis change curve algorithm, yet a number of segments were produced altogether without prior knowledge of the data. The proposed algorithm works successively on all attribute values considered for classification. By applying this classification approach, it was realized that:  The achieved accuracy of the proposed classification approach was 9% more accurate than the classification accuracy of the pixel-based classifier for the intensity and the DSM. When land cover classification is required for large area, it is expected that the statistical analysis classification technique will consume a longer time. To reduce the consumed time, accelerating the classification process is required. To achieve this, two different ways were investigated: eliminating the iterative process, or reducing the data size.  For eliminating the iterative process, a different segmentation algorithm was proposed. Though this algorithm was also based on the kurtosis change curve, it separates the entire data into a number of segments at once. This can be done through detecting the critical points of the kurtosis change curve and clustering the entire data at the attribute values corresponding to the detected critical points. Each cluster contained data with attribute values ranging between the values corresponding to each two consequent points. By doing so, it was noted that the consumed time was reduced to one sixth of the time consumed by the Iterative Single-Segment Separation process. However, a lower accuracy of the classification results was noted. For reducing data size, partitioning the area into smaller ones containing a manageable number of points was introduced. A user can specify the number of points that is manageable based on the computer specification. It was noted that partitioning the area reduced the processing time dramatically. The same level of accuracy was achieved when there were less than 20,000 points. When the described methodology was applied to Multi-Spectral LiDAR data, which consists of three channels, the following was observed:   Considering the elevation values in addition to the intensity values in the classification improved the classification results. Classification results of each channel were less accurate than classification results of the entire dataset (three channels together).
154



Combining classification results using the CMCD technique were more accurate than combining classified points of the different channels by merging the points into one grid space without considering the a posteriori probabilities of each channel.



Classification results of the pixel-based classification logic were more accurate than the pointbased logic. This occurred while data was being converted into images with multiple layers of the different channels, since each pixel got multiple attribute values irrespective of whether or not the original points of the different channels coincided.



The Decision rules classification technique produced similar results as the point-based classification logic. Nevertheless, more attention is required when analysing the data for more specific criteria.

6.2.2

Conclusions:

1. Producing land cover images out of the LiDAR data independent of external auxiliary data is comparable to the produced land cover images from the aerial imagery. 2. Using normalized elevation data instead of elevation data improved the classification results. 3. The texture of the intensity layer had a slight improvement on the quantitative classification accuracy (overall accuracy), and improved the classification of some classes. 4. The developed decision tree requires more attention to include other criteria for more distinguished classes with more accurate results. 5. Following the point-based classification logic and filling the gaps between the LiDAR footprints by any of the proposed approaches is more appropriate (accurate) for land cover classification of single wavelength LiDAR data in comparison to the pixel-based logic. 6. Classifying LiDAR data after normalizing the intensity values (applying histogram-matching) slightly improved the classification results of the overlapped areas. This occurs as histogrammatching further improved the surface reflectance and the intensity values. By improving the intensity values, more accurate classification results can be obtained. 7. Combining the classified data by merging the two classified data-strips achieved more accurate results than merging the two data-strips before classifying the entire dataset. By classifying each data-strip separately, more homogeneous classes can be achieved than those of merged data. 8. Combining classified data by following the CMCD approach improves the classification results of the overlapped area by 8%. The CMCD approach considers the a posteriori probabilities of each class and the distance between the original point cloud data and the grid points.
155

9. The proposed classification approach (Statistical analysis segmentation and supervised classification) can be used to produce land cover images as it produces more accurate results than the pixel-based classification. 10. For the multi-spectral LiDAR data, classification results of the pixel-based classification logic were more accurate than the point-based logic. This occurred as the data was being converted into images with multiple-layers of the different channels, allowing each pixel to get multiple attribute values irrespective of whether or not the original points of the different channels coincided

6.3 Future Work
     Consider the spatial location in the segmentation process. Apply different classification algorithms on the same data, and combine classification results. Specify further criteria for the decision tree to achieve more classes with higher accuracy. Include the slope of the DSM to detect borders of the buildings in the classification. Study techniques for improving the homogeneity of the classification results (eliminating the small areas).

156

Appendices
These appendices include all confusion matrices for all cases of the classifications results for the three study areas of the single wavelength LiDAR data (Appendices A, B, and C) and the study area of the multi-spectral LiDAR data (Appendix D). Each confusion matrix consists of either 5 rows by 5 columns for the cases of 5-classes classification, or 4 rows by 4 columns for the cases of 4-classes classification. Where, T = Trees Class, B = Building Class, G = Grass Class, R = Roads Class, S = Soil Class, and G/S = Grass/Soil Class. The numbers in the diagonal elements represent the number of reference points that were correctly classified. While the numbers off-diagonals are the number of points that were misclassified in other classes. In other words, for a column  and row  , the element  contains the number of reference points in class  that were incorrectly assigned to the class  . Two accuracy assessment values were calculated for each confusion matrix; overall accuracy and overall Kappa statistics. The overall accuracy represents the percentage of points that were correctly classified, while the overall Kappa ( ) measures the agreement between model predictions and reality. The overall accuracy can be measured as:   =   =  =
 =1  N

× 100 × 100

(A-1)

 -  1-chance agreement

(A-2) (A-3)

 2 ( =1  /)-( =1(+  + ) /  )  2  1--( =1(+  + ) /  )

× 100

Where,   + +  number of classes (number of rows in the confusion matrix) number of reference points in row  and column  (diagonal elements). total number of reference points in row  . total number of reference points in column  . total number of reference points;

157

Appendix A:

Confusion Matrices for the Image Classification Results of the Single Wavelength LiDAR Data

A.1 A.1.1

Confusion Matrices for Study area "Area1" Confusion Matrices of Pixel-based Classification Results 5 Classes
T B T 151 4 B 0 115 G 20 0 R 11 90 S 36 4 Overall Accuracy = Overall Kappa = T B T 199 59 B 78 106 G 7 39 R 8 9 S 1 0 Overall Accuracy = Overall Kappa = T B T 48 11 B 144 146 G 60 12 R 4 31 S 37 13 Overall Accuracy = Overall Kappa = T B T 105 12 B 56 127 G 17 13 R 72 54 S 43 7 Overall Accuracy = Overall Kappa = G 36 1 10 1 30 58% 0.46 G 28 7 5 30 9 41% 0.22 G 5 14 10 12 38 32% 0.14 G 3 12 8 22 34 36% 0.18 R 7 96 0 120 9 S 4 0 6 11 111 Kappa 0.66 0.39 0.21 0.34 0.51

Case 1) Aerial Imagery

4 Classes
T B T 151 4 B 0 115 G/S 56 4 R 11 90 Overall Accuracy = Overall Kappa = G/S 40 1 157 12 62% 0.50 R 7 96 9 120 Kappa 0.66 0.39 0.60 0.34

R 94 19 6 74 57

S 73 17 7 25 21

2) DSM

Kappa 0.20 0.32 0.00 0.34 0.11

T B T 199 59 B 78 106 G/S 8 39 R 8 9 Overall Accuracy = Overall Kappa =

G/S 101 24 42 55 43% 0.23

R 94 19 63 74

Kappa 0.20 0.32 0.06 0.34

R 11 177 10 49 3

S 30 25 32 0 56

3) Original Intensity

Kappa 0.22 0.09 0.00 0.34 0.27

T B T 48 11 B 144 146 G/S 97 25 R 4 31 Overall Accuracy = Overall Kappa =

G/S 35 39 136 12 39% 0.20

R 11 177 13 49

Kappa 0.22 0.09 0.36 0.34

R 5 172 8 58 7

S 1 18 42 29 53

4) Intensity

Kappa 0.76 0.14 0.01 -0.01 0.26

T B T 105 12 B 56 127 G/S 60 20 R 72 54 Overall Accuracy = Overall Kappa =

G/S 4 30 137 51 44% 0.26

R 5 172 15 58

Kappa 0.76 0.14 0.47 -0.01

158

Case

5 Classes
T B T 229 132 B 44 60 G 0 0 R 20 21 S 0 0 Overall Accuracy = Overall Kappa = T B T 188 65 B 67 137 G 11 6 R 11 3 S 16 2 Overall Accuracy = Overall Kappa = T B T 256 132 B 17 60 G 0 0 R 20 21 S 0 0 Overall Accuracy = Overall Kappa = T B T 249 79 B 13 125 G 4 5 R 22 3 S 5 1 Overall Accuracy = Overall Kappa = T B T 278 134 B 2 63 G 0 0 R 13 16 S 0 0 Overall Accuracy = Overall Kappa = G 20 5 5 38 11 53% 0.36 G 8 2 8 7 54 66% 0.56 G 25 0 5 36 13 56% 0.40 G 15 0 5 9 50 71% 0.62 G 20 0 5 47 7 58% 0.43 R 33 12 1 203 1 S 38 19 7 55 24 Kappa 0.30 0.27 0.33 0.47 0.61

4 Classes
T B T 229 132 B 44 60 G/S 0 0 R 20 21 Overall Accuracy = Overall Kappa = G/S R 58 33 24 12 47 2 93 203 55% 0.39 Kappa 0.30 0.27 0.95 0.47

5) Intensity, DSM

R 20 7 0 202 21

S 3 3 7 20 110

6) Intensity, NDSM

Kappa 0.52 0.53 0.18 0.77 0.46

T B T 188 65 B 67 137 G/S 27 8 R 11 3 Overall Accuracy = Overall Kappa =

G/S R 11 20 5 7 179 21 27 202 72% 0.63

Kappa 0.52 0.53 0.69 0.77

7) Intensity, DSM, Texture

R 36 9 1 204 0

S 37 20 7 58 21

Kappa 0.32 0.45 0.33 0.47 0.55

T B T 256 132 B 17 60 G/S 0 0 R 20 21 Overall Accuracy = Overall Kappa =

G/S R 62 36 20 9 46 1 94 204 58% 0.42

Kappa 0.32 0.45 0.97 0.47

8) Intensity, NDSM, Texture

R 23 7 0 214 6

S 7 1 7 23 105

Kappa 0.53 0.82 0.17 0.72 0.57

T B T 249 79 B 13 125 G/S 9 6 R 22 3 Overall Accuracy = Overall Kappa =

G/S R 22 23 1 7 167 6 32 214 77% 0.69

Kappa 0.53 0.82 0.86 0.72

9) Intensity, DSM, Texture, Slope

R 35 10 0 205 0

S 36 19 7 61 20

Kappa 0.36 0.58 0.37 0.46 0.70

T B T 278 134 B 2 63 G/S 0 0 R 13 16 Overall Accuracy = Overall Kappa =

G/S R 56 35 19 10 39 0 108 205 60% 0.45

Kappa 0.36 0.58 1.00 0.46

159

Case

5 Classes
T B T 273 87 B 2 122 G 3 3 R 12 1 S 3 0 Overall Accuracy = Overall Kappa = G 21 0 10 24 24 71% 0.61 R 37 7 0 201 5 S 17 12 0 27 87 Kappa 0.47 0.81 0.59 0.68 0.69

4 Classes
T B T 273 87 B 2 122 G/S 6 3 R 12 1 Overall Accuracy = Overall Kappa = G/S R 38 37 12 7 121 5 51 201 73% 0.64 Kappa 0.47 0.81 0.87 0.68

10) Intensity NDSM, Texture, Slope

A.1.2

Confusion Matrices of Object-based Classification Results
G/S 22 29 93 78 67% 0.56 R 7 8 78 157 Kappa 0.68 0.72 0.28 0.52

T B T 256 44 B 6 153 G/S 29 9 R 2 7 Overall Accuracy = Overall Kappa =

A.2 A.2.1

Confusion Matrices for Study area "Area2" Confusion Matrices of Pixel-based Classification Results 5 Classes
T B T 82 4 B 4 129 G 0 0 R 4 103 S 3 1 Overall Accuracy = Overall Kappa = T B T 60 98 B 21 119 G 0 0 R 5 1 S 14 19 Overall Accuracy = Overall Kappa = G 21 3 8 0 17 49% 0.28 G 11 4 6 22 6 44% 0.28 R 4 233 1 152 2 S 12 11 5 2 44 Kappa 0.66 0.39 0.21 0.34 0.51

Case 1) Aerial Imagery

4 Classes
T B T 82 4 B 4 129 G/S 3 1 R 4 103 Overall Accuracy = Overall Kappa = G/S 33 14 74 2 52% 0.31 R 4 233 3 152 Kappa 0.63 0.08 0.90 0.22

2) DSM

R 128 16 36 169 45

S 25 9 1 13 26

Kappa 0.20 0.32 0.00 0.34 0.11

T B T 60 98 B 21 119 G/S 14 19 R 5 1 Overall Accuracy = Overall Kappa =

G/S 36 13 39 35 45% 0.28

R 128 16 81 169

Kappa 0.08 0.59 0.13 0.64

160

Case

5 Classes
T B T 1 6 B 4 38 G 27 20 R 53 170 S 15 3 Overall Accuracy = Overall Kappa = T B T 39 9 B 1 79 G 16 20 R 23 125 S 21 4 Overall Accuracy = Overall Kappa = T B T 65 81 B 21 153 G 0 0 R 7 2 S 7 1 Overall Accuracy = Overall Kappa = T B T 62 44 B 31 190 G 0 0 R 3 2 S 4 1 Overall Accuracy = Overall Kappa = T B T 74 81 B 17 154 G 0 0 R 6 1 S 3 1 Overall Accuracy = Overall Kappa = G 2 0 24 5 18 50% 0.24 G 8 1 12 1 27 45% 0.23 G 14 3 9 17 6 54% 0.36 G 13 5 10 2 19 77% 0.67 G 16 1 8 19 5 54% 0.37 R 3 44 25 308 14 S 0 2 9 6 57 Kappa 0.22 0.09 0.00 0.34 0.27

4 Classes
T B T 1 6 B 4 38 G/S 42 23 R 53 170 Overall Accuracy = Overall Kappa = G/S 2 2 108 11 53% 0.27 R 3 44 39 308 Kappa -0.04 0.21 0.43 0.20

3) Original Intensity

R 5 144 28 201 16

S 5 2 7 3 57

4) Intensity

Kappa 0.76 0.14 0.01 -0.01 0.26

T B T 39 9 B 1 79 G/S 37 24 R 23 125 Overall Accuracy = Overall Kappa =

G/S 13 3 103 4 49% 0.27

R 5 144 44 201

Kappa 0.54 0.10 0.41 0.20

R 23 150 7 213 1

S 30 10 6 8 20

5) Intensity, DSM

Kappa 0.30 0.27 0.33 0.47 0.61

T B T 65 81 B 21 153 G/S 7 1 R 7 2 Overall Accuracy = Overall Kappa =

G/S R 44 23 13 150 41 8 25 213 55% 0.38

Kappa 0.21 0.24 0.67 0.74

R 9 10 18 339 18

S 10 1 5 5 53

6) Intensity, NDSM

Kappa 0.52 0.53 0.18 0.77 0.46

T B T 62 44 B 31 190 G/S 4 1 R 3 2 Overall Accuracy = Overall Kappa =

G/S R 23 9 6 10 87 36 7 339 79% 0.70

Kappa 0.38 0.73 0.63 0.94

7) Intensity, DSM, Texture

R 24 151 10 207 2

S 32 10 6 8 18

Kappa 0.32 0.45 0.33 0.47 0.55

T B T 74 81 B 17 154 G/S 3 1 R 6 1 Overall Accuracy = Overall Kappa =

G/S R 48 24 11 151 37 12 27 207 55% 0.38

Kappa 0.24 0.26 0.65 0.74

161

Case

5 Classes
T B T 73 41 B 23 194 G 0 0 R 1 2 S 3 0 Overall Accuracy = Overall Kappa = T B T 78 76 B 16 159 G 0 0 R 3 1 S 3 1 Overall Accuracy = Overall Kappa = T B T 78 47 B 17 189 G 1 0 R 1 0 S 3 1 Overall Accuracy = Overall Kappa = G 16 2 14 2 15 75% 0.65 G 16 3 8 17 5 55% 0.38 G 18 1 13 3 14 74% 0.65 R 16 10 17 308 43 S 10 2 7 5 50 Kappa 0.53 0.82 0.17 0.72 0.57

4 Classes
T B T 73 41 B 23 194 G/S 3 0 R 1 2 Overall Accuracy = Overall Kappa = G/S R 26 16 4 10 86 60 7 308 77% 0.68 Kappa 0.40 0.78 0.51 0.94

8) Intensity, NDSM, Texture

9) Intensity, DSM, Texture, Slope

R 26 153 8 206 1

S 33 9 6 8 18

Kappa 0.36 0.58 0.37 0.46 0.70

T B T 78 76 B 16 159 G/S 3 1 R 3 1 Overall Accuracy = Overall Kappa =

G/S R 49 26 12 153 37 9 25 206 56% 0.39

Kappa 0.25 0.26 0.70 0.77

10) Intensity NDSM, Texture, Slope

R 20 14 10 305 45

S 9 1 8 5 51

Kappa 0.47 0.81 0.59 0.68 0.69

T B T 78 47 B 17 189 G/S 4 1 R 1 0 Overall Accuracy = Overall Kappa =

G/S R 27 20 2 14 86 55 8 305 77% 0.68

Kappa 0.38 0.79 0.52 0.95

A.2.2

Confusion Matrices of Object-based Classification Results
G/S 9 4 46 64 65% 0.49 R 6 11 85 292 Kappa 0.63 0.86 0.15 0.45

T B T 71 20 B 1 148 G/S 22 15 R 6 54 Overall Accuracy = Overall Kappa =

162

A.3 A.3.1

Confusion Matrices for Study area "Area3" Confusion Matrices of Pixel-based Classification Results 5 Classes
B G B 80 0 G 0 13 R 17 0 S 1 0 T 2 8 Overall Accuracy = Overall Kappa = B G B 22 8 G 15 27 R 9 4 S 14 3 T 17 8 Overall Accuracy = Overall Kappa = B G B 36 2 G 0 31 R 29 8 S 0 6 T 12 3 Overall Accuracy = Overall Kappa = B G R 86 4 73 6 12 59% 0.47 R 48 73 30 22 18 30% 0.15 R 67 2 72 14 36 49% 0.34 R 81 2 52 14 42 49% 0.36 S 2 2 1 62 13 S 1 1 7 56 15 S 8 4 9 57 12 T 11 18 4 2 67 Kappa 0.29 0.30 0.54 0.83 0.58

Case 1) Aerial Imagery

4 Classes
B G/S B 80 0 G/S 0 13 R 19 8 T 1 0 Overall Accuracy = Overall Kappa = R 97 22 156 8 62% 0.42 T 8 4 21 57 Kappa 0.29 0.30 0.45 0.83

S 13 25 19 16 7

T 5 13 15 12 57

2) DSM

Kappa 0.09 0.08 0.00 0.09 0.41

B G/S B 22 8 G/S 15 27 R 26 12 T 14 3 Overall Accuracy = Overall Kappa =

R 53 86 120 34 37% 0.10

T 13 25 26 16

Kappa 0.09 0.08 0.16 0.09

T 14 1 22 17 48

3) Original Intensity

Kappa 0.17 0.87 0.22 0.53 0.27

B G/S B 36 2 G/S 0 31 R 41 11 T 0 6 Overall Accuracy = Overall Kappa =

R 81 3 178 31 60% 0.37

T 1 1 22 56

Kappa 0.17 0.87 0.29 0.53

T 32 1 2 17 49

4) Intensity

B 42 2 G 1 39 R 19 0 S 0 6 T 15 3 Overall Accuracy = Overall Kappa =

Kappa 0.13 0.85 0.52 0.55 0.25

G/S B 42 2 G/S 1 39 R 34 3 T 0 6 Overall Accuracy = Overall Kappa =

B

R 113 3 145 31 58% 0.38

T 2 2 14 62

Kappa 0.13 0.85 0.37 0.55

163

Case

5 Classes
B G B 53 0 G 0 17 R 19 0 S 0 0 T 28 4 Overall Accuracy = Overall Kappa = B G B 78 1 G 0 11 R 10 7 S 1 0 T 10 2 Overall Accuracy = Overall Kappa = B G B 64 0 G 0 17 R 10 1 S 0 0 T 26 3 Overall Accuracy = Overall Kappa = R 96 6 56 1 24 44% 0.26 R 42 6 127 1 7 58% 0.43 R 85 4 69 0 25 49% 0.34 S 6 10 35 27 13 S 14 0 48 26 3 S 7 10 33 28 13 T 14 1 19 7 64 Kappa 0.14 0.48 0.11 0.73 0.34

4 Classes
B G/S B 53 0 G/S 0 17 R 47 4 T 0 0 Overall Accuracy = Overall Kappa = R T 110 7 7 10 163 46 8 28 52% 0.22 Kappa 0.14 0.48 0.12 0.73

5) Intensity, DSM

T 30 3 16 7 49

6) Intensity, NDSM

Kappa 0.34 0.53 0.38 0.68 0.61

B G/S B 78 1 G/S 0 11 R 20 9 T 1 0 Overall Accuracy = Overall Kappa =

R 72 9 199 8 63% 0.38

T 14 0 51 26

Kappa 0.34 0.53 0.32 0.69

T 11 3 14 7 70

7) Intensity, DSM, Texture

Kappa 0.23 0.48 0.26 0.75 0.38

B G/S B 64 0 G/S 0 17 R 36 4 T 0 0 Overall Accuracy = Overall Kappa =

R 96 7 178 7 57% 0.30

T 6 10 48 27

Kappa 0.23 0.48 0.22 0.75

Case

5 Classes
B G B 77 1 G 0 10 R 7 8 S 0 0 T 15 2 Overall Accuracy = Overall Kappa = R 45 2 127 1 8 61% 0.47 S 11 0 43 31 6 T 22 2 16 5 60 Kappa 0.37 0.70 0.42 0.80 0.57

4 Classes
B G/S B 77 1 G/S 0 10 R 22 10 T 0 0 Overall Accuracy = Overall Kappa = R 67 4 211 6 66% 0.42 T 11 0 49 31 Kappa 0.37 0.70 0.34 0.80

8) Intensity, NDSM, Texture

A.3.2

Confusion Matrices of Object-based Classification Results
B R 10 26 116 13 61% 0.46 T 14 12 2 55 Kappa 0.53 0.27 0.47 0.52

G/S B 65 15 G/S 2 31 R 13 43 T 11 11 Overall Accuracy = Overall Kappa =

164

Appendix B:

Confusion Matrices for the Point Classification Results of the Single Wavelength LiDAR Data

B.1

Confusion Matrices of Classification Results of Single Data-Strip Strip Strip4
B G B 42 2 G 2 36 R 11 8 S 2 10 T 19 6 Overall Accuracy = Overall Kappa = B G B 52 2 G 2 45 R 27 3 S 2 3 T 12 6 Overall Accuracy = Overall Kappa = B G B 42 2 G 1 38 R 22 3 S 3 2 T 9 5 Overall Accuracy = Overall Kappa = R 22 4 124 20 24 56% 0.42 R 11 11 144 10 45 58% 0.44 R 11 9 122 7 41 58% 0.45 S 2 8 18 40 12 S 2 2 9 34 9 T 9 6 39 14 44 Kappa 0.46 0.68 0.43 0.35 0.27 B G B 45 0 G 1 35 R 42 29 S 6 17 T 18 9 Overall Accuracy = Overall Kappa = B G B 46 1 G 0 36 R 24 4 S 1 3 T 6 4 Overall Accuracy = Overall Kappa = B G B 46 1 G 0 36 R 24 4 S 1 3 T 6 4 Overall Accuracy = Overall Kappa =

Strip6
R 26 5 86 19 17 51% 0.37 R 42 28 88 7 20 52% 0.38 R 42 27 88 8 20 52% 0.38 S 6 17 20 38 4 S 1 2 7 39 10 T 6 5 21 4 50 Kappa 0.45 0.67 0.23 0.39 0.37

1) Original Points

S 3 12 21 48 16

T 21 7 30 11 57

2) Grid Points (NN)

Kappa 0.51 0.54 0.43 0.58 0.27

S 6 17 20 38 4

T 19 9 17 9 50

Kappa 0.29 0.34 0.32 0.58 0.49

T 19 6 19 9 48

3) Grid Points (IM)

Kappa 0.47 0.57 0.46 0.59 0.27

T 19 9 17 9 50

Kappa 0.29 0.34 0.32 0.57 0.49

B.2 B.2.1

Confusion Matrices of Classification Results of Multiple Data-Strips Confusion Matrices of Classification Results for Merged Original Data-strips
B G B 92 1 G 3 73 R 34 10 S 9 6 T 17 8 Overall Accuracy = Overall Kappa = R 83 51 158 25 59 49% 0.34 S 10 27 34 74 20 T 42 20 35 20 89 Kappa 0.29 0.36 0.33 0.46 0.32

1) Original Points

165

2) Grid Points (NN)

B G B 91 1 G 3 76 R 35 11 S 10 6 T 15 8 Overall Accuracy = Overall Kappa = B G B 91 1 G 3 76 R 35 11 S 10 6 T 15 8 Overall Accuracy = Overall Kappa =

R 76 47 158 21 60 50% 0.36 R 76 47 158 21 60 50% 0.36

S 12 29 31 80 20

T 42 20 40 22 102

Kappa 0.30 0.37 0.34 0.49 0.35

S 12 29 31 80 20

T 42 20 40 22 102

3) Grid Points (IM)

Kappa 0.30 0.37 0.34 0.49 0.35

B.2.2 Confusion Matrices of Classification Results for Merged Data-strips after Intensity Normalization 1) Original Points
B G R 83 43 171 20 59 50% 0.36 R 78 37 172 19 56 51% 0.37 R 80 37 171 16 57 51% 0.37 S 11 29 36 78 18 S 12 30 36 77 17 S 10 31 32 72 20 T 41 18 38 17 92 Kappa 0.29 0.38 0.33 0.51 0.35

B 91 1 G 3 75 R 42 10 S 7 6 T 12 6 Overall Accuracy = Overall Kappa = B G

T 42 20 41 21 102

2) Grid Points (NN)

B 88 1 G 5 78 R 41 12 S 7 7 T 13 4 Overall Accuracy = Overall Kappa = B G B 88 1 G 5 79 R 42 12 S 7 6 T 12 4 Overall Accuracy = Overall Kappa =

Kappa 0.29 0.40 0.33 0.50 0.40

T 40 20 41 22 102

3) Grid Points (IM)

Kappa 0.29 0.40 0.33 0.52 0.39

166

B.2.3

Confusion Matrices of Classification Results for Merged Classified Data-strips
B G B 87 3 G 2 71 R 48 9 S 3 4 T 15 11 Overall Accuracy = Overall Kappa = B G B 50 0 G 1 14 R 32 1 S 1 0 T 16 6 Overall Accuracy = Overall Kappa = R 53 37 210 16 60 54% 0.39 R 16 18 108 5 36 50% 0.34 S 1 19 28 32 11 S 8 27 39 73 18 T 37 15 41 19 94 Kappa 0.36 0.41 0.37 0.56 0.34

1) Original Points

T 9 10 26 13 47

2) Grid Points (IM)

Kappa 0.62 0.21 0.46 0.59 0.34

B.2.4

Confusion Matrices of Classification Results for Combined Classified Data-strips using CMCD
B G R 15 8 122 7 39 56% 0.42 S 2 11 18 38 11 T 24 8 16 12 42 Kappa 0.46 0.54 0.60 0.57 0.33

Grid Points

B 43 2 G 1 36 R 22 3 S 3 3 T 8 6 Overall Accuracy = Overall Kappa =

167

Appendix C:

Confusion Matrices for the Point Classification Results of the Single Wavelength LiDAR Data Using Kurtosis Change Curve

C.1

Confusion Matrices of Classification Results for Iterative Single Segment Separation Method
B G B 38 1 G 2 23 R 20 3 S 4 9 T 13 13 Overall Accuracy = Overall Kappa = B G B 37 2 G 1 21 R 24 5 S 2 8 T 13 13 Overall Accuracy = Overall Kappa = R 9 6 114 21 41 53% 0.37 R 8 5 120 16 42 54% 0.38 S 2 2 19 41 16 S 2 3 20 40 15 T 16 6 15 18 47 Kappa 0.54 0.55 0.58 0.39 0.29

1) Original Points

T 13 4 18 18 49

2) Grid Points (IM)

Kappa 0.56 0.62 0.56 0.44 0.30

C.2

Confusion Matrices of Classification Results for Multiple Segments Separation Method
B G B 13 0 G 2 35 R 12 2 S 1 2 T 49 10 Overall Accuracy = Overall Kappa = B G B 12 0 G 1 37 R 14 1 S 1 2 T 49 10 Overall Accuracy = Overall Kappa = R 31 9 51 2 98 35% 0.16 R 32 10 51 2 96 36% 0.18 S 3 13 22 19 23 S 3 11 25 17 24 T 19 9 14 3 57 Kappa 0.13 0.51 0.37 0.65 0.15

1) Original Points

T 20 8 11 2 61

2) Grid Points (IM)

Kappa 0.11 0.51 0.40 0.71 0.17

168

Appendix D:

Confusion Matrices for the Classification Results of the MultiSpectral LiDAR Data

D.1

Confusion Matrices of Image Classification Results of Multi-Spectral LiDAR Data

D.1.1. Confusion Matrices of Pixel-Based Classification Results (1) Intensity Data
T B T 7702 147 B 0 12127 G/S 23 4977 R 2657 982 Overall Accuracy = Overall Kappa = T B T 10017 16 B 0 16031 G/S 305 856 R 60 1330 Overall Accuracy = Overall Kappa = T B T 2996 3693 B 631 10765 G/S 0 2011 R 6755 1764 Overall Accuracy = Overall Kappa = T B T 8185 0 B 0 15729 G/S 397 182 R 1800 2322 Overall Accuracy = Overall Kappa = G/S 0 4007 13172 5835 64% 0.50 G/S 455 0 13713 8846 73% 0.63 G/S 485 4752 17658 119 61% 0.47 G/S 0 0 22776 238 91% 0.87 R 1216 295 561 3226 Kappa 0.82 0.61 0.50 0.18

1) Channel 1

2) Channel 2

R 573 211 2540 1974

Kappa 0.88 0.98 0.64 0.08

3) Channel 3

R 1498 178 39 3583

Kappa 0.20 0.50 0.83 0.22

4) Channel 1,2 &3

R 93 281 81 4843

Kappa 0.99 0.97 0.95 0.48

169

(2)

DSM Data
T B T 7362 572 B 2496 10648 G/S 1 6699 R 523 314 Overall Accuracy = Overall Kappa = T B T 6613 565 B 2524 10435 G/S 1 6865 R 1244 368 Overall Accuracy = Overall Kappa = T B T 6882 221 B 2416 11020 G/S 0 6971 R 1084 21 Overall Accuracy = Overall Kappa = G/S 15 18595 4404 0 45% 0.21 G/S 6 18273 4735 0 44% 0.21 G/S 9 18157 4848 0 46% 0.23 R 1731 494 116 2957 Kappa 0.71 0.01 -0.02 0.76

1) Channel 1

2) Channel 2

R 1297 402 99 3500

Kappa 0.73 0.01 0.00 0.65

3) Channel 3

R 1135 507 177 3479

Kappa 0.80 0.03 0.00 0.73

(3)

Intensity and DSM Data
T B T 8064 143 B 0 12372 G/S 51 4923 R 2267 795 Overall Accuracy = Overall Kappa = T B T 10046 26 B 0 16240 G/S 104 946 R 232 1021 Overall Accuracy = Overall Kappa = G/S 0 4009 18818 187 75% 0.64 G/S 16 0 22991 7 93% 0.90 R 1005 314 355 3624 Kappa 0.85 0.62 0.63 0.48

1) Channel 1

2) Channel 2

R 759 172 601 3766

Kappa 0.91 0.98 0.89 0.72

170

3) Channel 3

T B T 9961 1240 B 1 15621 G/S 0 1348 R 420 24 Overall Accuracy = Overall Kappa = T B T 10131 48 B 0 16138 G/S 16 484 R 235 1563 Overall Accuracy = Overall Kappa = T B T 10106 29 B 0 16003 G/S 21 337 R 255 1864 Overall Accuracy = Overall Kappa =

G/S 73 7529 15407 5 78% 0.69 G/S 2 0 22960 52 94% 0.91 G/S 137 0 22806 71 93% 0.90

R 1321 438 39 3500

Kappa 0.74 0.50 0.86 0.87

4) Channel 1,2 &3 (Intensity of Channel 1, 2 & 3 and DSM of Channel 1, 2 &3)

R 472 165 381 4280

Kappa 0.94 0.99 0.94 0.67

5) Channel 1,2 &3 (Intensity of Channel 1, 2 & 3 and DSM of Channel 2 )

R 660 172 336 4130

Kappa 0.91 0.98 0.95 0.62

(4)

Intensity and NDSM Data
T B T 8939 187 B 0 12127 G/S 41 5287 R 1402 632 Overall Accuracy = Overall Kappa = T B T 10083 28 B 0 16275 G/S 93 1209 R 206 721 Overall Accuracy = Overall Kappa = T B T 10379 1069 B 3 15139 G/S 0 2011 R 0 14 Overall Accuracy = Overall Kappa = G/S 63 4007 18899 45 76% 0.65 G/S 0 0 22989 25 93% 0.89 G/S 68 5288 17658 0 81% 0.73 R 1348 295 538 3117 Kappa 0.81 0.61 0.60 0.56

1) Channel 1

2) Channel 2

R 784 196 853 3465

Kappa 0.91 0.98 0.86 0.76

3) Channel 3

R 1692 553 39 3014

Kappa 0.74 0.59 0.83 0.99

171

D.1.2. Classification of Multi-Spectral LiDAR Data Using Object-Based Image Classification
T B T 10167 663 B 9 3502 G/S 71 13018 R 135 1050 Overall Accuracy = Overall Kappa = G/S 19 2355 20599 41 62% 0.43 R 4176 231 88 803 Kappa 0.60 0.37 0.35 0.33

D.2 D.2.1

Confusion Matrices of Point Classification Results of Multi-Spectral LiDAR Data Intensity Data
T B T 761 85 B 3 1384 G/S 26 705 R 468 119 Overall Accuracy = Overall Kappa = T B T 1032 94 B 2 1780 G/S 224 182 R 10 353 Overall Accuracy = Overall Kappa = T B T 441 441 B 130 1342 G/S 5 221 R 692 235 Overall Accuracy = Overall Kappa = G/S 18 353 1581 919 55% 0.39 G/S 569 107 1897 325 65% 0.51 G/S 177 1514 1003 42 47% 0.27 R 312 71 156 268 Kappa 0.64 0.75 0.62 0.14 T B T 6437 745 B 39 10999 G/S 190 5599 R 3716 890 Overall Accuracy = Overall Kappa = T B T 8720 510 B 9 14183 G/S 1596 1129 R 57 2411 Overall Accuracy = Overall Kappa = T B T 3435 3277 B 964 10871 G/S 32 1611 R 5951 2474 Overall Accuracy = Overall Kappa = T B 9173 673 6 12814 373 777 830 3969 Overall Accuracy = Overall Kappa = G/S 195 2393 13416 7010 57% 0.41 G/S 4299 636 16061 2018 70% 0.58 G/S 1276 12126 9174 438 48% 0.30 G/S 1727 1038 16929 3320 73% 0.63 R 1894 480 1057 1867 Kappa 0.63 0.69 0.43 0.05

1) Channel 1

2) Channel 2

R 322 75 384 167

Kappa 0.50 0.90 0.69 0.18

R 1467 505 2365 961

Kappa 0.49 0.89 0.60 0.09

3) Channel 3

R 151 49 9 424

Kappa 0.35 0.42 0.80 0.30

R 990 327 68 3913

Kappa 0.24 0.19 0.74 0.24

4) Channels 1, 2 & 3 (CMCD)

R 1612 352 502 2832

Kappa 0.63 0.86 0.85 0.18

172

D.2.2

Elevation Data
T B T 761 45 B 328 965 G/S 0 858 R 169 15 Overall Accuracy = Overall Kappa = T B T 830 54 B 341 973 G/S 0 909 R 97 20 Overall Accuracy = Overall Kappa = T B T 836 17 B 331 955 G/S 1 890 R 100 16 Overall Accuracy = Overall Kappa = G/S 3 2319 549 0 36% 0.07 G/S 6 2295 597 0 37% 0.09 G/S 3 2168 565 0 38% 0.09 R 18 316 46 30 Kappa 0.92 0.22 0.34 0.13 T B T 6368 532 B 2715 10121 G/S 2 7204 R 1297 376 Overall Accuracy = Overall Kappa = T B T 6937 484 B 2755 9981 G/S 0 7379 R 690 389 Overall Accuracy = Overall Kappa = T B T 6909 177 B 2676 10466 G/S 17 7427 R 780 163 Overall Accuracy = Overall Kappa = G/S 31 18596 4387 0 42% 0.17 G/S 42 18253 4719 0 43% 0.19 G/S 37 18280 4697 0 45% 0.21 R 1347 399 530 3022 Kappa 0.72 0.00 -0.07 0.61

1) Channel 1

2) Channel 2

R 35 328 53 37

Kappa 0.89 0.22 0.35 0.23

R 1372 316 570 3040

Kappa 0.74 0.00 -0.05 0.71

3) Channel 3

R 1 322 37 1

Kappa 0.97 0.23 0.35 0.00

R 837 492 518 3451

Kappa 0.84 0.01 -0.06 0.76

173

D.2.3

Intensity and Elevation Data
T B T 761 47 B 3 1158 G/S 14 621 R 480 57 Overall Accuracy = Overall Kappa = T B T 1135 91 B 2 1912 G/S 54 193 R 77 213 Overall Accuracy = Overall Kappa = T B T 1111 112 B 27 1585 G/S 0 143 R 130 38 Overall Accuracy = Overall Kappa = T B T 3007 316 B 32 5201 G/S 68 1083 R 687 341 Overall Accuracy = Overall Kappa = T B T 1896 176 B 5 3325 G/S 68 905 R 557 296 Overall Accuracy = Overall Kappa = G/S 18 355 2433 65 68% 0.52 G/S 96 120 2649 33 81% 0.73 G/S 12 1745 971 8 59% 0.41 G/S 126 2220 6053 106 72% 0.60 G/S 114 475 5082 98 75% 0.65 R 38 255 91 26 Kappa 0.88 0.64 0.76 0.03 T B T 6426 739 B 39 11263 G/S 90 5595 R 3827 636 Overall Accuracy = Overall Kappa = T B T 9520 497 B 9 15199 G/S 325 1213 R 528 1324 Overall Accuracy = Overall Kappa = T B T 9121 1459 B 179 15049 G/S 0 1248 R 1082 477 Overall Accuracy = Overall Kappa = T B 8766 777 27 14270 113 2325 1476 861 Overall Accuracy = Overall Kappa = T B 8083 626 16 13534 179 3160 2104 913 Overall Accuracy = Overall Kappa = T B 9274 1071 10 15550 33 1272 1065 340 Overall Accuracy = Overall Kappa = G/S 193 2153 20253 415 72% 0.59 G/S 610 628 21604 172 86% 0.80 G/S 104 13938 8851 121 65% 0.51 G/S 191 3520 19146 157 79% 0.70 G/S 222 1380 21205 207 80% 0.71 G/S 128 1208 21657 21 88% 0.83 R 1687 479 225 2907 Kappa 0.65 0.72 0.62 0.31

1) Channel 1

2) Channel 2

R 356 70 133 389

Kappa 0.67 0.90 0.87 0.54

R 1603 458 656 2581

Kappa 0.73 0.90 0.85 0.52

3) Channel 3

R 28 291 35 7

Kappa 0.88 0.42 0.84 0.03

R 771 562 65 3900

Kappa 0.75 0.27 0.78 0.67

4) Channels 1, 2&3 (Merged Data)

R 771 226 182 1209

Kappa 0.69 0.63 0.79 0.49

R 1499 502 306 2991

Kappa 0.73 0.67 0.79 0.50

5) Channels 1 &2 (Merged Data)

R 647 145 173 790

Kappa 0.65 0.83 0.80 0.44

R 1678 461 382 2777

Kappa 0.71 0.82 0.75 0.41

6) Channels 1, 2&3 (CMCD)

R 970 501 188 3639

Kappa 0.77 0.85 0.89 0.69

174

D.3 D.3.1

Confusion Matrices for Multi-Spectral LiDAR Data Using Kurtosis Change Curve Intensity and Elevation Data Using Iterative Single Segment Separation Method
T B T 752 85 B 6 1387 G/S 20 759 R 480 62 Overall Accuracy = Overall Kappa = T B T 1027 93 B 0 1754 G/S 194 225 R 47 337 Overall Accuracy = Overall Kappa = T B T 1073 173 B 49 1961 G/S 1 85 R 145 20 Overall Accuracy = Overall Kappa = T B T 2852 351 B 55 5102 G/S 215 1069 R 672 419 Overall Accuracy = Overall Kappa = T B T 1779 178 B 6 3141 G/S 214 984 R 527 399 Overall Accuracy = Overall Kappa = G/S 11 335 2523 2 70% 0.57 G/S 572 107 2174 45 72% 0.61 G/S 19 1911 803 3 62% 0.46 G/S 602 2353 5500 50 68% 0.55 G/S 583 442 4697 47 71% 0.59 R 283 71 61 392 Kappa 0.66 0.76 0.74 0.41 T B T 6342 702 B 66 10986 G/S 133 6035 R 3841 510 Overall Accuracy = Overall Kappa = T B T 8474 556 B 5 14020 G/S 1554 1404 R 349 2253 Overall Accuracy = Overall Kappa = T B T 8904 1850 B 321 15568 G/S 3 589 R 1154 226 Overall Accuracy = Overall Kappa = T B T 8095 884 B 43 14012 G/S 626 2387 R 1618 950 Overall Accuracy = Overall Kappa = T B T 7386 611 B 26 12914 G/S 836 3572 R 2134 1136 Overall Accuracy = Overall Kappa = T B T 9639 971 B 152 15458 G/S 243 1234 R 348 570 Overall Accuracy = Overall Kappa = 175 G/S 111 2005 20881 17 72% 0.60 G/S 4076 600 18079 259 76% 0.66 G/S 206 15296 7475 37 63% 0.48 G/S 1124 3832 17986 72 76% 0.65 G/S 1390 1307 20229 88 76% 0.66 G/S 72 1218 21708 16 87% 0.81 R 1590 462 331 2915 Kappa 0.66 0.72 0.60 0.34

1) Channel 1

2) Channel 2

R 328 36 150 434

Kappa 0.50 0.92 0.78 0.49

R 1547 219 779 2753

Kappa 0.48 0.92 0.71 0.44

3) Channel 3

R 134 86 4 409

Kappa 0.76 0.47 0.89 0.71

R 908 592 23 3775

Kappa 0.69 0.25 0.87 0.70

4) Channels 1, 2 & 3 (Merged Data)

R 745 193 215 1235

Kappa 0.60 0.62 0.75 0.50

R 1476 388 427 3007

Kappa 0.63 0.66 0.73 0.48

5) Channels 1 &2 (Merged Data)

R 611 107 211 826

Kappa 0.54 0.84 0.74 0.44

R 1620 292 529 2857

Kappa 0.60 0.84 0.67 0.40

6) Channels 1, 2&3 (CMCD)

R 1848 518 234 2698

Kappa 0.72 0.84 0.88 0.72

D.3.2

Intensity and Elevation Data Using Multiple Segments Separation Method
T B T 771 82 B 4 728 G/S 17 1397 R 466 86 Overall Accuracy = Overall Kappa = T B T 1121 98 B 0 1995 G/S 65 191 R 82 125 Overall Accuracy = Overall Kappa = T B T 1095 154 B 10 810 G/S 44 1229 R 119 46 Overall Accuracy = Overall Kappa = T B T 2987 334 B 14 3533 G/S 126 2817 R 667 257 Overall Accuracy = Overall Kappa = T B T 1892 180 B 4 2723 G/S 82 1588 R 548 211 Overall Accuracy = Overall Kappa = G/S 18 225 2558 70 62% 0.45 G/S 479 129 2273 17 77% 0.69 G/S 12 1120 1594 10 57% 0.38 G/S 509 1474 6425 97 66% 0.51 G/S 497 354 4831 87 70% 0.57 R 256 56 82 413 Kappa 0.68 0.71 0.61 0.39 T B T 6530 643 B 31 5671 G/S 141 11229 R 3680 690 Overall Accuracy = Overall Kappa = T B T 9436 516 B 6 15760 G/S 395 1181 R 545 776 Overall Accuracy = Overall Kappa = T B T 9067 1655 B 65 6306 G/S 255 9806 R 995 466 Overall Accuracy = Overall Kappa = T B 8725 809 17 8886 200 7855 1440 683 Overall Accuracy = Overall Kappa = T B 8059 606 16 10027 223 6909 2084 691 Overall Accuracy = Overall Kappa = T B 9118 939 26 15571 49 1239 1189 484 Overall Accuracy = Overall Kappa = G/S 183 1286 21093 452 64% 0.47 G/S 3339 712 18872 91 83% 0.75 G/S 89 8963 13823 139 58% 0.40 G/S 800 2108 19956 150 72% 0.59 G/S 1146 970 20705 193 74% 0.62 G/S 186 824 21971 33 89% 0.84 R 1394 370 487 3047 Kappa 0.69 0.66 0.40 0.32

1) Channel 1

2) Channel 2

R 359 60 91 438

Kappa 0.53 0.91 0.86 0.66

R 1560 381 390 2967

Kappa 0.55 0.90 0.84 0.64

3) Channel 3

R 135 23 69 406

Kappa 0.78 0.39 0.52 0.70

R 865 107 462 3864

Kappa 0.73 0.13 0.27 0.68

4) Channels 1, 2 & 3 (Merged Data)

R 750 139 242 1257

Kappa 0.63 0.64 0.61 0.53

R 1389 207 525 3177

Kappa 0.69 0.69 0.50 0.54

5) Channels 1 &2 (Merged Data)

R 615 116 173 851

Kappa 0.58 0.84 0.69 0.49

R 1483 306 438 3071

Kappa 0.65 0.83 0.55 0.46

6) Channels 1, 2&3 (CMCD)

R 774 357 260 3907

Kappa 0.79 0.89 0.89 0.66

176

D.4 Confusion Matrices of Classification Results of Multi-Spectral LiDAR Data Using Rule-Based Classification 1) Channel 1
T B T 1121 0 B 30 1611 G/S 5 474 R 49 153 Overall Accuracy = Overall Kappa = T B T 1150 4 B 0 2104 G/S 0 37 R 118 264 Overall Accuracy = Overall Kappa = T B T 1163 2 B 64 167 G/S 4 568 R 105 108 Overall Accuracy = Overall Kappa = T B T 3434 2 B 98 3882 G/S 9 1304 R 171 399 Overall Accuracy = Overall Kappa = T B T 2271 0 B 34 3715 G/S 5 736 R 66 291 Overall Accuracy = Overall Kappa = G/S 0 468 2379 3 79% 0.70 G/S 0 262 2630 6 89% 0.84 G/S 0 1942 2159 43 56% 0.35 G/S 0 2447 7168 51 75% 0.64 G/S 0 505 5009 8 84% 0.77 R 137 184 13 602 Kappa 0.89 0.69 0.82 0.74 T B T 9193 260 B 3 13217 G/S 0 3388 R 1186 1368 Overall Accuracy = Overall Kappa = T B T 9415 31 B 2 16302 G/S 2 227 R 963 1673 Overall Accuracy = Overall Kappa = T B T 9557 514 B 45 1143 G/S 7 16125 R 773 451 Overall Accuracy = Overall Kappa = T B T 9466 230 B 13 11337 G/S 4 5275 R 899 1391 Overall Accuracy = Overall Kappa = T B T 9354 124 B 2 14890 G/S 2 1694 R 1024 1525 Overall Accuracy = Overall Kappa = T B T 9415 39 B 5 16948 G/S 2 227 R 960 1019 Overall Accuracy = Overall Kappa = G/S 46 3604 19224 140 80% 0.71 G/S 0 1742 21230 42 90% 0.86 G/S 55 5246 17680 33 55% 0.33 G/S 12 3254 19684 64 78% 0.69 G/S 13 2747 20176 78 86% 0.79 G/S 0 1750 21230 34 91% 0.87 R 429 883 37 3949 Kappa 0.91 0.63 0.75 0.55

2) Channel 2

R 17 138 5 788

Kappa 0.98 0.83 0.98 0.66

R 114 707 38 4439

Kappa 0.98 0.81 0.98 0.59

3) Channel 3

R 103 66 5 377

Kappa 0.92 0.06 0.77 0.59

R 1440 564 447 2847

Kappa 0.79 -0.23 0.19 0.66

4) Channels 1, 2 & 3 (Merged Data)

R 358 514 24 1767

Kappa 0.90 0.51 0.81 0.73

R 370 692 134 4102

Kappa 0.93 0.62 0.64 0.60

5) Channels 1 &2 (Merged Data)

R 255 448 19 1390

Kappa 0.89 0.77 0.85 0.78

R 212 729 32 4325

Kappa 0.96 0.72 0.87 0.58

6) Channels 1, 2&3 (CMCD)

R 107 827 38 4326

Kappa 0.98 0.81 0.98 0.65

177

References
Ackermann, F. (1996). Airborne laser scanning for elevation models. Geomatics Info Magazine, 10 (10), 24-25. Ackermann, F. (1999). Airborne laser scanning--present status and future expectations. ISPRS Journal of Photogrammetry and Remote Sensing, 54(2), 64-67. Antonarakis, A. S., Richards, K. S., & Brasington, J. (2008). Object-based land cover classification using airborne LiDAR. Remote Sensing of Environment,112(6), 2988-2998. Baatz, M., & Schäpe, A. (2000). Multiresolution segmentation: an optimization approach for high quality multi-scale image segmentation. Angewandte Geographische Informationsverarbeitung XII, 1223. Bakker, W., Janssen, L., Reeves, C., Grote, B., Pohl, C., Weir, M., Horn, J., Prakash, A., and Woldai, T. (2001). Principles of Remote Sensing. 2/e, ISBN 90-6164-199-3 ITC, Enschede, The Netherlands. Baltsavias, E. P. (1999). Airborne laser scanning: existing systems and firms and other resources. ISPRS Journal of Photogrammetry and Remote sensing, 54(2), 164-198. Baltsavias, E. P. (1999)a. Airborne laser scanning: basic relations and formulas. ISPRS Journal of Photogrammetry and Remote Sensing, 54(2), 199-214. Bao, Y., Cao, C., Chang, C., Li, X., Chen, E., & Li, Z. (2007, September). Segmentation to the point clouds of LiDAR data based on change of kurtosis. In International Symposium on Photoelectronic Detection and Imaging: Technology and Applications 2007 (pp. 66231N-66231N). International Society for Optics and Photonics. Bartels, M., & Wei, H. (2006). Segmentation of LIDAR data using measures of distribution. International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences, 36(7), 426-31. Bartels, M., & Wei, H. (2010). Threshold-free object and ground point separation in LIDAR data. Pattern recognition letters, 31(10), 1089-1099. Bartels, M., Wei, H., & Ferryman, J. (2006, November). Analysis of LIDAR data fused with co-registered bands. In Video and Signal Based Surveillance, 2006. AVSS'06. IEEE International Conference (pp. 60-60). IEEE. Bartels, M., Wei, H., & Mason, D. C. (2006, August). DTM generation from LIDAR data using skewness balancing. In Pattern Recognition, 2006. ICPR 2006. 18th International Conference on Pattern Recognition (Vol. 1, pp. 566-569). IEEE.
179

Blaschke, T. (2010). Object based image analysis for remote sensing. ISPRS journal of photogrammetry and remote sensing, 65(1), 2-16. Bottu, E. (1998). Laser Scanning Applied to Vegetation Height and Roughness Length Determination. Ecole Superieure des Geometres Topographes (ESGT), Conservatoire National des Arts et Matiers, Appendix C. Brennan, R., & Webster, T. L. (2006). Object-oriented land cover classification of LiDAR-derived surfaces. Canadian Journal of Remote Sensing, 32(2), 162-172. Charaniya, A. P., Manduchi, R., & Lodha, S. K. (2004, June). Supervised parametric classification of aerial LiDAR data. In Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshop, CVPRW'04. (pp. 25-32). IEEE. Chen, Y., Su, W., Li, J., & Sun, Z. (2009). Hierarchical object oriented classification using very high resolution imagery and LIDAR data over urban areas. Advances in Space Research, 43(7), 11011110. Chen, Z., & Gao, B. (2014). An object-based method for urban land cover classification using Airborne Lidar data. IEEE Journal Of Selected Topics in Applied Earth Observations and Remote Sensing (2014) Volume 7, Issue 10, 4243-4254 Comaniciu, D., & Meer, P. (2002). Mean shift: A robust approach toward feature space analysis. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 24(5), 603-619. Coren, F., & Sterzai, P. (2006). Radiometric correction in laser scanning. International Journal of Remote Sensing, 27(15), 3097-3104. Costantino, D., & Angelini, M. G. (2011). Features and ground automatic extraction from airborne LiDAR data. International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences, 38(5/W12). Crosilla, F., Macorig, D., Scaioni, M., Sebastianutti, I., & Visintini, D. (2013). LiDAR data filtering and classification by skewness and kurtosis iterative analysis of multiple point cloud data categories. Applied Geomatics, 5(3), 225-240. Crosilla, F., Macorig, D., Sebastianutti, I., & Visintini, D. (2011). Points classification by a sequential higher--order moments statistical analysis of LIDAR data, International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences, 38(5/W12). Duda, R. O., Hart, P. E., & Stork, D. G. (2012). Pattern classification. John Wiley & Sons.

180

Dunford, R., Michel, K., Gagnage, M., Piégay, H., & Trémelo, M. L. (2009). Potential and constraints of Unmanned Aerial Vehicle technology for the characterization of Mediterranean riparian forest. International Journal of Remote Sensing, 30(19), 4915-4935. El-Ashmawy, N., & Shaker, A. (2014). Raster vs. point cloud LiDAR data classification. ISPRS-International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences,40/7, 79-83 El-Ashmawy, N., Shaker, A., & Yan, W. (2011). Pixel vs object-based image classification techniques for LiDAR intensity data. ISPRS-International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences, 38/12, 43-48. Haala, N., & Brenner, C. (1999). Extraction of buildings and trees in urban environments. ISPRS Journal of Photogrammetry and Remote Sensing, 54(2), 130-137. Habib, A., Bang, K. I., Kersting, A. P., & Chow, J. (2010). Alternative methodologies for LiDAR system calibration. Remote Sensing, 2(3), 874-907. Habib, A. F., Kersting, A. P., Shaker, A., & Yan, W. Y. (2011). Geometric calibration and radiometric correction of LiDAR data and their impact on the quality of derived products. Sensors, 11(9), 9069-9097. Hansen, M. C., DeFries, R. S., Townshend, J. R., & Sohlberg, R. (2000). Global land cover classification at 1 km spatial resolution using a classification tree approach. International journal of Remote Sensing, 21(6-7), 1331-1364. Höfle, B., & Pfeifer, N. (2007). Correction of laser scanning intensity data: Data and model-driven approaches. ISPRS Journal of Photogrammetry and Remote Sensing, 62(6), 415-433. Hu, X., Tao, C. V., & Hu, Y. (2004). Automatic road extraction from dense urban area by integrated processing of high resolution imagery and LiDAR data. ISPRS-International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences. Istanbul, Turkey, 35, B3. Huang, M. Z. (2004). A Knowledge-Based Approach to Urban-feature Classification Using Aerial Imagery with Airborne LiDAR Data. China Taiwan, National Sun Yat-sen University. Hug, C., & Wehr, A. (1997). Detecting and identifying topographic objects in imaging laser altimeter data. International Archives of Photogrammetry and Remote Sensing, 32(3 SECT 4W2), 19-26. Hui, L., Di, L., Xianfeng, H., & Deren, L. (2008, July). Laser intensity used in classification of LiDAR point cloud data. In International Geoscience and Remote Sensing Symposium, 2008. IGARSS 2008. IEEE (Vol. 2, pp. II-1140). IEEE.

181

Huising, E. J., & Pereira, L. G. (1998). Errors and accuracy estimates of laser data acquired by various laser scanning systems for topographic applications. ISPRS Journal of Photogrammetry and Remote Sensing, 53(5), 245-261. Im, J., Jensen, J. R., & Tullis, J. A. (2008). Objectbased change detection using correlation image analysis and image segmentation. International Journal of Remote Sensing, 29(2), 399-423. Jelalian, A. V. (1992). Laser radar systems. Artech House. Jensen, J. R. (2005). Introductory digital image processing. 3/e, Pearson Prentice Hall, Upper Saddle River, New Jersey, ISBN 0-13-1453361-0: 526p. Jensen, J. R. (2007). Remote sensing of the environment: An earth resource perspective 2/e. Pearson Prentice Hall, Upper Saddle River, New Jersey, ISBN 0-13-188950-8: 592p. Kittler, J., Hatef, M., Duin, R. P., & Matas, J. (1998). On combining classifiers. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 20(3), 226-239. Kotrlik, J. W. K. J. W., & Higgins, C. C. H. C. C. (2001). Organizational research: Determining appropriate sample size in survey research appropriate sample size in survey research. Information Technology, Learning, and Performance Journal, 19(1), 43. Kraus, K., & Pfeifer, N. (1998). Determination of terrain models in wooded areas with airborne laser scanner data. ISPRS Journal of Photogrammetry and Remote Sensing, 53(4), 193-203. Kuncheva, L. I., Bezdek, J. C., & Duin, R. P. (2001). Decision templates for multiple classifier fusion: an experimental comparison. Pattern Recognition, 34(2), 299-314. Liu, Y., Li, Z., Hayward, R., Walker, R., & Jin, H. (2009, December). Classification of airborne LiDAR intensity data using statistical analysis and Hough transform with application to power line corridors. In Digital Image Computing: Techniques and Applications, 2009. DICTA'09. (pp. 462467). IEEE. Ma, R. (2005). DEM generation and building detection from LiDAR data. Photogrammetric Engineering & Remote Sensing, 71(7), 847-854. Mallet, C., & Bretar, F. (2009). Full-waveform topographic LiDAR: State-of-the-art. ISPRS Journal of Photogrammetry and Remote Sensing, 64(1), 1-16. Morgan, T.W., (2012). North Carolina Technical Specifications for LiDAR Base Mapping. Land Records Management Division North Carolina Department of the Secretary of State 2012.

182

Polikar, R. (2006). Ensemble based systems in decision making. Circuits and Systems Magazine, IEEE, 6(3), 21-45. Rees, W. G., Williams, M., & Vitebsky, P. (2003). Mapping land cover change in a reindeer herding area of the Russian Arctic using Landsat TM and ETM+ imagery and indigenous knowledge. Remote Sensing of Environment, 85(4), 441-452. Shaker, A., Yan, W. Y., & El-Ashmawy, N. (2011, August). The effects of laser reflection angle on radiometric correction of airborne LiDAR intensity data. ISPRS-International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences, 38/12, 213-217. Song, J. H., Han, S. H., Yu, K. Y., & Kim, Y. I. (2002). Assessing the possibility of land-cover classification using lidar intensity data. International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences, 34(3/B), 259-262. Stricker, M. A., & Orengo, M. (1995, March). Similarity of color images. In IS&T/SPIE's Symposium on Electronic Imaging: Science & Technology (pp. 381-392). International Society for Optics and Photonics. Wallace, A., Nichol, C., & Woodhouse, I. (2012). Recovery of forest canopy parameters by inversion of multispectral LiDAR data. Remote Sensing, 4(2), 509-531. Wehr, A., & Lohr, U. (1999). Airborne laser scanning--an introduction and overview. ISPRS Journal of Photogrammetry and Remote Sensing, 54(2), 68-82. Weng, Q. (Ed.). (2011). Advances in environmental remote sensing: sensors, algorithms, and applications. CRC Press. Woodhouse, I. H., Nichol, C., Sinclair, P., Jack, J., Morsdorf, F., Malthus, T. J., & Patenaude, G. (2011). A multispectral canopy LiDAR demonstrator project. Geoscience and Remote Sensing Letters, IEEE, 8(5), 839-843. Xu, L., Krzyzak, A., & Suen, C. Y. (1992). Methods of combining multiple classifiers and their applications to handwriting recognition. IEEE Transactions on Systems, Man and Cybernetics, vol. 22, no. 3, pp. 418­435. Yan, G., Mas, J. F., Maathuis, B. H. P., Xiangmin, Z., & Van Dijk, P. M. (2006). Comparison of pixelbased and objectoriented image classification approaches--a case study in a coal fire area, Wuda, Inner Mongolia, China. International Journal of Remote Sensing, 27(18), 4039-4055. Yan, W. Y., Shaker, A., & El-Ashmawy, N. (2015). Urban land cover classification using airborne LiDAR data. Remote Sensing of Environment, 158, 295-310

183

Yan, W. Y., Shaker, A., Habib, A., & Kersting, A. P. (2012). Improving classification accuracy of airborne LiDAR intensity data by geometric calibration and radiometric correction. ISPRS Journal of Photogrammetry and Remote Sensing, 67, 35-44. Yan, W. Y., & Shaker, A. (2011). The effects of combining classifiers with the same training statistics using Bayesian decision rules. International Journal of Remote Sensing, 32(13), 3729-3745. Yunfei, B., Guoping, L., Chunxiang, C., Xiaowen, L., Hao, Z., Qisheng, H., & Chaoyi, C. (2008). Classification of LIDAR point cloud and generation of DTM from LIDAR height and intensity data in forested area. International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences, 37(7), 313-318.

184

