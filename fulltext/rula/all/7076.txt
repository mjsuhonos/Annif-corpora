LAND/WATER DISCRIMINATION AND LAND COVER CLASSIFICATION USING MULTISPECTRAL AIRBORNE LIDAR DATA

by

Salem Wagih Salem Morsy B.Sc. (Hons), Cairo University, Egypt, 2008 M.Sc., Cairo University, Egypt, 2012

A dissertation presented to Ryerson University

in partial fulfillment of the requirements for the degree of Doctor of Philosophy in the program of Civil Engineering

Toronto, Ontario, Canada, 2017 © Salem Morsy, 2017

DECLARATION

I hereby declare that I am the sole author of this dissertation. This is a true copy of the dissertation, including any required final revisions, as accepted by my examiners.

I authorize Ryerson University to lend this dissertation to other institutions or individuals for the purpose of scholarly research.

I further authorize Ryerson University to reproduce this dissertation by photocopying or by other means, in total or in part, at the request of other institutions or individuals for the purpose of scholarly research.

I understand that my dissertation may be made electronically available to the public.

ii

LAND/WATER DISCRIMINATION AND LAND COVER CLASSIFICATION USING MULTISPECTRAL AIRBORNE LIDAR DATA

Doctor of Philosophy 2017 Salem Morsy Department of Civil Engineering Ryerson University

ABSTRACT

Multispectral airborne Light Detection And Ranging (LiDAR) systems are currently available. Optech Titan is an example of these systems, which acquires LiDAR point clouds at three independent wavelengths (1550, 1064 and 532 nm) from Earth's surface. This dissertation aims to use the radiometric information (i.e., intensity) of the Optech Titan LiDAR data along with the geometric information (e.g., height) for land/water discrimination in coastal zones and land cover classification of urban areas.

A set of point features based on elevation, intensity, and geometry was extracted and evaluated for land/water discrimination in coastal zones. In addition, an automated land/water discrimination approach based on seeded region growing algorithm was presented. Two data subsets were tested at Lake Ontario and Tobermory Harbour in Ontario, Canada. The elevationand geometry-based features achieved average overall accuracies of 72.8% - 83.3% and 69.9% 74.4%, respectively, while the intensity-based features achieved an average overall accuracy of

iii

59.0% - 63.4%. The region growing method achieved an average overall accuracy of more than 99%, and the automation of this method is restricted by having double returns from water bodies at the 532 nm wavelength.

A hierarchal point-based classification approach was presented for land cover classification of urban areas. The collected point clouds at the three wavelengths were first merged and three intensity values were estimated for each LiDAR point, followed by three-level classification approach. First, a ground filtering method was applied to separate non-ground from ground points. Second, three normalized difference vegetation indices (NDVIs) were computed, followed by NDVIs' histograms construction. A multivariate Gaussian decomposition (MVGD) was then used to divide those histograms into buildings or trees from non-ground and roads or grass from ground points. Third, classes such as power lines, swimming pools and different types of trees were labeled based on their spectral characteristics. Three data subsets were tested representing different complexity of urban areas in Oshawa, Ontario, Canada. It is shown that the presented approach has achieved an overall accuracy up to 93.0%, which increased to more than 99% by considering the spatial coherence of the LiDAR point clouds.

iv

ACKNOWLEDGMENTS

All praise and gratitude be to ALLAH the Most Merciful who has granted me the opportunity to study at Ryerson University and has given me the health, strength and patience to complete my graduate studies.

This research would not have been possible without the help and encouragement of several individuals and organizations. First of all, I convey my sincere thanks and gratitude to my supervisors, Dr. Ahmed Shaker and Prof. Ahmed El-Rabbany for their guidance and support. It would have been difficult to complete this research without our constructive discussions and their valuable comments. I enjoyed working with my supervisors and I wish to continue working with them in the future.

I am immensely grateful to the department of Civil Engineering for their continuous support and encouragement, especially to Prof. Michael Chapman, Prof. Songnian Li, Rachel Peluso and Mira Grkavac. I express my deep sense of gratitude to Dr. Wai Yeung Yan for his fruitful help and continuous support during my PhD studies. Sincere appreciation goes to all PhD colleagues at Geomatic Engineering: Dr. Mahmoud Abd Rabbou, Dr. Akram Afifi, Dr. Nagwa ElAshmawy, Dr. Hassan Ibrahim, Dr. Mohamed Abdel Azem, Kamil Faisal, Shahram Sattar, Moh Moh Lin Khin, Ashraf Elshorbagy, Abdelsatar Elmezayen and Yasmine Megahed, and my office mates: Dr. Rana Morsy and Mona El Mosallamy for their support, company and sharing scientific knowledge.

My PhD has been obtained and funded by Ontario Trillium Scholarship (OTS) and the Natural Sciences and Engineering Research Council (NSERC). Thanks to Government of Ontario for giving me this opportunity to do my PhD at Ryerson University. I deeply appreciate the financial support from Ryerson University which allowed me to attend and compete on best papers, oral presentations and poster presentations in international and national conferences. In addition, indirect financial support was provided through the student competitions organized by the Association of Ontario Land Surveyors (AOLS) and Canadian Symposium on Remote Sensing (CSRS). v

I also would like to acknowledge Teledyne Optech for providing the LiDAR data from the world's first multispectral airborne LiDAR system Optech Titan for this research. Particularly, I express my special thanks to Dr. Paul LaRocque, vice president of Teledyne Optech for business development, for his valuable comments and discussions during my PhD studies.

Most importantly, I am greatly indebted to my parents for their love, support and prayers that enabled me to achieve this goal. I am always thankful to them. Profound thanks go to my sisters for their support. Last but not least, I cannot find suitable words to express my acknowledgments to my wife for her love, encouragement, support and help at every moment of my PhD studies. All praise and gratitude be to ALLAH for blessing us with a daughter and a son. They fill our life with joy and fun. We love you so much.

vi

DEDICATION

To my Parents, my Sisters, my Wife, my Daughter & my Son

vii

TABLE OF CONTENTS ABSTRACT ................................................................................................................................... iii ACKNOWLEDGMENTS .............................................................................................................. v DEDICATION .............................................................................................................................. vii TABLE OF CONTENTS ............................................................................................................. viii LIST OF TABLES ......................................................................................................................... xi LIST OF FIGURES ...................................................................................................................... xii LIST OF APPENDICES ............................................................................................................... xv INTRODUCTION .......................................................................................................................... 1 1.1. 1.2. 1.3. Research Motivation ........................................................................................................ 1 Research Objectives ......................................................................................................... 4 Dissertation Structure ....................................................................................................... 5

LITERATURE REVIEW ............................................................................................................... 7 2.1. 2.2. 2.3. 2.4. Overview on Airborne LiDAR System ............................................................................ 7 Related Work to Land/Water Discrimination .................................................................. 9 Related Work to Land Cover Classification .................................................................. 12 Historical Development of Multispectral LiDAR Systems............................................ 18

STUDY AREA AND DATASETS .............................................................................................. 22 3.1. 3.2. 3.3. The Multispectral airborne LiDAR sensor Optech Titan ........................................... 22 Study Areas and Datasets for Land/Water Discrimination ............................................ 25 Study Area and Datasets for Land Cover Classification ................................................ 30

LAND/WATER DISCRIMINATION .......................................................................................... 36 4.1. Methodology .................................................................................................................. 36 Point Features Extraction ........................................................................................ 36 Land/Water Points Labeling Based on LiDAR Point Features .............................. 40 viii

4.1.1. 4.1.2.

4.1.3. 4.1.4. 4.2.

Seeded region growing for labeling water points ................................................... 43 Accuracy Assessment ............................................................................................. 45

Results and Analysis ...................................................................................................... 46 Land/Water Discrimination from Elevation-based Features .................................. 47 Land/water discrimination from intensity-based features....................................... 51 Land/Water Discrimination from Geometry-based Features .................................. 55 Land/Water Discrimination from Seeded Region Growing ................................... 58 Land/Water Boundary Delineation ......................................................................... 63 Limitations of Land/Water Discrimination ............................................................. 65

4.2.1. 4.2.2. 4.2.3. 4.2.4. 4.2.5. 4.2.6.

LAND COVER CLASSIFICATION ........................................................................................... 66 5.1. Methodology .................................................................................................................. 66 Overall Classification Scheme ................................................................................ 66 Multi-wavelength LiDAR Points Merging ............................................................. 66 Ground Filtering...................................................................................................... 70 NDVIs Computation and Histograms Construction ............................................... 72 Multivariate Gaussian Decomposition (MVGD) .................................................... 73 LiDAR Points Classification................................................................................... 76 Evaluation of Classification Results ....................................................................... 78

5.1.1. 5.1.2. 5.1.3. 5.1.4. 5.1.5. 5.1.6. 5.1.7. 5.2.

Results and Analysis ...................................................................................................... 79 The Impact of Spatial Coherence ............................................................................ 87 Comparison with Previous Studies ......................................................................... 91

5.2.1. 5.2.2.

SUMMARY, CONCLUSIONS AND RECOMMENDATIONS................................................. 93 6.1. Summary and Conclusions ............................................................................................. 93 Land/Water Discrimination .................................................................................... 93 Land Cover Classification....................................................................................... 96 ix

6.1.1. 6.1.2.

6.2.

Recommendations .......................................................................................................... 98

APPENDICES .............................................................................................................................. 99 REFERENCES ........................................................................................................................... 121

x

LIST OF TABLES

Table 3.1. Optech Titan sensor specifications .............................................................................. 22 Table 3.2. Data subsets specifications .......................................................................................... 26 Table 3.3. Data specifications for Area 1, Area 2 and Area 3 ...................................................... 31 Table 4.1. Recorded returns from different classes at infrared and green wavelengths ............... 40 Table 4.2. Number of reference points ......................................................................................... 45 Table 4.3. Confusion matrix elements .......................................................................................... 45 Table 4.4. Accuracy measures of labeled point for Area_LO based on elevation, Z and R. The best results are highlighted in bold. ........................................................................... 49 Table 4.5. Accuracy measures of labeled point for Area_TH based on elevation, Z and R. The best results are highlighted in bold. ........................................................................... 49 Table 4.6. Threshold values of Ithresh and ICOVthresh ..................................................................... 51 Table 4.7. Accuracy measures of labeled points for Area_LO based on intensity, NDWI, ICOV and ID. The best results are highlighted in bold. ....................................................... 54 Table 4.8. Accuracy measures of labeled points for Area_TH based on intensity, NDWI, ICOV and ID. The best results are highlighted in bold. ....................................................... 54 Table 4.9. Accuracy measures of labeled point for Area_LO based on PD and MR. The best results are highlighted in bold. ................................................................................... 57 Table 4.10. Accuracy measures of labeled point for Area_TH based on PD and MR. The best results are highlighted in bold. ................................................................................... 57 Table 4.11. Accuracy measures of labeled point for Area_LO based on seeded region growing. The best results are highlighted in bold. .................................................................... 61 Table 4.12. Accuracy measures of labeled point for Area_TH based on seeded region growing. The best results are highlighted in bold. .................................................................... 61 Table 4.13. RMSE (m) of land/water boundary ........................................................................... 64 Table 5.1. The number of components/optimization method/ of the fitted Gaussians and

number of clusters obtained from MVGD for the three Areas .................................. 82 Table 5.2. Confusion matrix of Area 1 ......................................................................................... 84 Table 5.3. Confusion matrix of Area 2 ......................................................................................... 84 Table 5.4. Confusion matrix of Area 3 ......................................................................................... 85 xi

LIST OF FIGURES

Figure 1.1. Example on vegetation covering building roofs and confusion between power lines and vegetation points ................................................................................................... 4 Figure 2.1. Airborne LiDAR system............................................................................................... 8 Figure 3.1. Spectral reflection curve of different terrain covers ................................................... 23 Figure 3.2. Study areas for land/water discrimination .................................................................. 26 Figure 3.3. LiDAR point clouds of Area_LO colorized by elevation: a, b and c; intensity: d, e and f; and number of returns g, h and i ...................................................................... 27 Figure 3.4. LiDAR point clouds of Area_LH colorized by elevation: a, b and c; intensity: d, e and f; and number of returns g, h and i ...................................................................... 28 Figure 3.5. Reference image for the land/water interface of Area_LO ........................................ 29 Figure 3.6. Reference image for the land/water interface of Area_LH ........................................ 29 Figure 3.7. Study area for land/water classification ..................................................................... 30 Figure 3.8. LiDAR point clouds of Area 1 colorized by elevation: a, b and c; and intensity: d, e and f............................................................................................................................ 32 Figure 3.9. LiDAR point clouds of Area 2 colorized by elevation: a, b and c; and intensity: d, e and f............................................................................................................................ 33 Figure 3.10. LiDAR point clouds of Area 3 colorized by elevation: a, b and c; and intensity: d, e and f............................................................................................................................ 33 Figure 3.11. Aerial image of Area 1 ............................................................................................. 34 Figure 3.12. Aerial image of Area 2 ............................................................................................. 34 Figure 3.13. Aerial image of Area 3 ............................................................................................. 35 Figure 4.1. Workflow of land/water discrimination based on extracted point features ................ 37 Figure 4.2. Workflow of land/water discrimination based on seeded region growing algorithm 44 Figure 4.3. Elevation histogram of the three channels for Area_LO ............................................ 47 Figure 4.4. Elevation histogram of the three channels for Area_TH ............................................ 47 Figure 4.5. Labeled LiDAR points of Area_LO based on: elevation using (a) C1; (b) C2; and (c) C3; Z using (d) C1 and C3; and (e) C2 and C3; and R using (f) C1 and C3; and (g) C2 and C3................................................................................................................... 48

xii

Figure 4.6. Labeled LiDAR points of Area_TH based on: elevation using (a) C1; (b) C2; and (c) C3; Z using (d) C1 and C3; and (e) C2 and C3; and R using (f) C1 and C3; and (g) C2 and C3................................................................................................................... 49 Figure 4.7. Labeled LiDAR points of Area_LO based on: intensity using (a) C1; (b) C2; and (c) C3; NDWI using (d) C3C2; (e) C3C1; and (f) C1C2; ICOV using (g) C1; (h) C2; and (i) C3; and ID using (j) C1; (k) C2; and (l) C3 .......................................................... 52 Figure 4.8. Labeled LiDAR points of Area_TH based on: intensity using (a) C1; (b) C2; and (c) C3; NDWI using (d) C3C2; (e) C3C1; and (f) C1C2; ICOV using (g) C1; (h) C2; and (i) C3; and ID using (j) C1; (k) C2; and (l) C3 ....................................... 53 Figure 4.9. Labeled LiDAR points of Area_LO based on: PD using (a) C1 and C3; and (b) C2 and C3; and MR using (c) C1 and C3; and (d) C2 and C3 ......................................... 56 Figure 4.10. Labeled LiDAR points of Area_TH based on: PD using (a) C1 and C3; and (b) C2 and C3; and MR using (c) C1 and C3; and (d) C2 and C3 ...................................... 57 Figure 4. 11. LiDAR points of Area_LO based on: (a) single returns in C1; (b) single returns in C2; (c) first of double returns in C3; (d) possible seed points using C1 and C3; (e) possible seed points using C2 and C3; (f) water seed points using C1 and C3; (g) water seed points using C2 and C3; (h) seeded region growing using C1 and C3; and (i) seeded region growing using C2 and C3 ..................................................... 59 Figure 4.12. LiDAR points of Area_TH based on: (a) single returns in C1; (b) single returns in C2; (c) first of double returns in C3; (d) possible seed points using C1 and C3; (e) possible seed points using C2 and C3; (f) water seed points using C1 and C3; (g) water seed points using C2 and C3; (h) seeded region growing using C1 and C3; and (i) seeded region growing using C2 and C3 ..................................................... 60 Figure 4.13. Average overall accuracies for Area_LO based on different point features and seeded region growing ............................................................................................. 62 Figure 4.14. Average overall accuracies for Area_TH based on different point features and seeded region growing ............................................................................................. 63 Figure 4.15. Land/water boundary delineation for Area_LO (left) and Area_TH (right) ............ 64 Figure 5.1. Classification workflow.............................................................................................. 67 Figure 5.2. 3D spatial joining between points from C2 and C3 ................................................... 68 Figure 5.3. Ground filtering workflow ......................................................................................... 71 xiii

Figure 5.4. Examples of NDVIs' histograms constructed from (left: non-ground points and right: ground points) ............................................................................................................ 73 Figure 5.5. Recorded points of two types of trees (upper) and power lines (lower) in C1, C2 and C3, respectively.......................................................................................................... 77 Figure 5.6. Classified area before (left) and after (right) the application of the 3D filter ............ 77 Figure 5.7. The power lines display in an aerial image and the LiDAR data ............................... 78 Figure 5.8. Gaussian decomposition of NDVIs' histograms of Area 1 (upper: non-ground and lower: ground) ............................................................................................................ 80 Figure 5.9. Gaussian decomposition of NDVIs' histograms of Area 2 (upper: non-ground and lower: ground) ............................................................................................................ 81 Figure 5.10. Gaussian decomposition of NDVIs' histograms Area 3 (upper: non-ground and lower: ground) ............................................................................................................ 81 Figure 5.11. Classified LiDAR points of Area 1 .......................................................................... 83 Figure 5.12. Classified LiDAR points of Area 2 .......................................................................... 83 Figure 5.13. Classified LiDAR points of Area 3 .......................................................................... 84 Figure 5.14. Classified LiDAR points of Area 1 after mode filter ............................................... 88 Figure 5.15. Classified LiDAR points of Area 2 after mode filter ............................................... 88 Figure 5.16. Classified LiDAR points of Area 3 after mode filter ............................................... 89 Figure 5.17. Producer's and user's accuracies for MVGD and MVGD_filtered of Area 1 ......... 89 Figure 5.18. Producer's and user's accuracies for MVGD and MVGD_filtered of Area 2 ......... 90 Figure 5.19. Producer's and user's accuracies for MVGD and MVGD_filtered of Area 3 ......... 91 Figure 5.20. Overall accuracies comparison ................................................................................. 92

xiv

LIST OF APPENDICES

Appendix A: Confusion Matrices for Land/Water Discrimination .............................................. 99 Appendix B: Confusion Matrices and Figures for Land Cover Classification ........................... 105

xv

CHAPTER 1

INTRODUCTION

1.1. Research Motivation

Airborne Light Detection And Ranging (LiDAR) systems are widely used in remote sensing applications. Airborne LiDAR systems typically operate at a monochromatic wavelength (e.g., 1064 nm), also known as topographic airborne LiDAR. These systems measure the range and the strength of the reflected energy (intensity) from different objects on Earth's surface. The LiDAR data, collected by a monochromatic wavelength airborne LiDAR system, have been used over the past two decades for land/water discrimination in coastal zones and land cover classification of urban areas.

Coastal zones are environmentally sensitive to natural events and human activities. Land/water interface at coastal zones is expected to change over time and therefore requires accurate detection and frequent monitoring for their sustainable management. Land/water discrimination is valuable for providing decision makers with such services as water area estimation (Ma et al., 2007; Du et al., 2012), flood monitoring (Schumann et al., 2011; Mueller et al., 2016), flood disaster assessment (Qi and Altinakar 2011; Stephensa et al., 2012; Kuenzer et al., 2013), hydrological regulation and erosion control (Wang et al., 2013), and water resources management (Giardino et al., 2010; Ding and Li 2011; Van Dijk and Renzullo 2011).

Coastal zone mapping can be achieved by collecting data from land cover objects, water surfaces and/or bottoms, and inter-tidal zones (i.e., land/water boundaries). Conventional surveying techniques are not efficient for mapping land/water boundaries due to their inherent high cost and low point density. Multibeam echo-sounding techniques need sufficient water depth to be used effectively in bathymetry, so they are not effective at delineating land/water boundaries.

1

Over the past years, remotely sensed satellite imagery has been employed for land/water discrimination using normalized difference water indices (NDWIs). The green wavelength with near infrared (McFeeters, 1996; Ji et al., 2009) or shortwave infrared wavelength (Xu, 2006; Lacaux et al., 2007; Ji et al., 2009) were employed to form NDWIs for discriminating water bodies from land objects. However, a constant threshold value of zero was set to discriminate water bodies (McFeeters, 1996; Xu, 2006). Also, in some cases, built-up areas were wrongly classified as water areas (McFeeters, 1996). Moreover, the manual adjustment of the threshold value achieved accurate result for different datasets (Lacaux et al., 2007).

LiDAR elevation and intensity data, collected using topographic airborne LiDAR systems, have been utilized for land/water discrimination (Brzank et al., 2008; Höfle et al., 2009; Schmidt et al., 2012; Smeeckaert et al., 2013). LiDAR point features were extracted to be used in the discrimination process such as point density (Brzank et al., 2008; Smeeckaert et al., 2013), roughness (Höfle et al., 2009), and intensity variation (Höfle et al., 2009). However, threshold values were manually selected to make those point features fit the tested data, which means the existing methods are ad hoc. The existing methods require prior knowledge of land/water interface to aid in the supervised discrimination process (Brzank et al., 2008; Schmidt et al., 2012; Smeeckaert et al., 2013). Other methods require auxiliary data such as sensor position during data acquisition, GPS timestamps and scan angles (Höfle et al., 2009), which are not always available from the data supplier. These auxiliary data were used to model the data gap in water areas, and hence used for discriminating water from land areas (Höfle et al., 2009). Nevertheless, some methods could not discriminate water surface from land objects such as asphalt surfaces because they exhibited similar characteristics including intensity range and smooth surface (Höfle et al., 2009).

During the last 20 years, airborne LiDAR data have been used in urban areas classification (Yan et al., 2015). Numerous studies have focused on extracting one urban object type such as separation of ground from non-ground points in order to generate digital terrain models (DTMs) (Bartels et al. 2006), building roofs detection and extraction (Rottensteiner and Briese, 2002; Forlani et al., 2006; Huang et al., 2013), road extraction (Samadzadegan et al., 2009) and curbstones mapping (Zhou and Vosselman, 2012). The elementary objects of urban areas have 2

then been classified, and such objects include buildings, vegetation and ground (Samadzadegan et al., 2010; Mallet et al. 2011; Niemeyer et al., 2011). The ground is then separated into natural ground (e.g., grass) and artificial ground (e.g., roads) (Chehata et al., 2009).

Previous studies mostly converted the 3D LiDAR points into 2D raster images, combined LiDAR data with multispectral satellite/aerial imagery and applied image classification methods (Huang et al. 2008; Chen et al. 2009; Charaniya et al. 2004; Hartfield et al. 2011). These images vary according to the selected pixel size and the interpolation methods used to fill the gaps between pixels. In addition, the occurrence of mixed pixels is a fundamental problem in those studies.

In 3D point classification of the airborne LiDAR data, multi-class labeling has become an essential topic for 3D city modeling, change detection, map updating, disaster evaluation and emergency purposes. However, most recent studies focus on the geometric characteristics described by the LiDAR data (Mallet et al., 2011; Xu et al., 2014; Blomley et al., 2016; Vosselman et al., 2017). Studies rarely reported using the intensity LiDAR data along with the geometric features extracted from the LiDAR data for the purpose of urban areas classification.

The classification of airborne LiDAR point clouds of urban areas is still a problem due to the relatively low point density, irregular point distribution, data gaps caused by occlusion and the complex urban areas. Assigning each 3D point to an object class is a basic step in LiDAR processing for land cover classification. The classification of airborne LiDAR point clouds in urban areas have become challenging due to 1) the availability of dense point clouds requiring automatic, efficient, and low computational cost processing algorithms, 2) objects that are hardly detectable in urban areas such as swimming pools and power lines due to the lack of data collected for such objects; this problem is caused either by occlusion or no points being recorded at certain wavelengths, and 3) complex urban areas such as vegetation covering building roofs and power lines that can be confused from elevated objects (mostly vegetation) (see Figure 1.1).

3

Building Roofs Trees

Trees

Building Roofs

Power Lines

Trees

Figure 1.1. Example on vegetation covering building roofs and confusion between power lines and vegetation points

With the evolution of LiDAR technology, multispectral airborne LiDAR systems are currently available either in the form of separate multi-sensor or true multispectral systems. These systems acquire LiDAR point clouds at different wavelengths. Therefore, the point density has been increased and a diversity of spectral information about land objects and water bodies has become available. Thus, a complete scene classification could be achieved using LiDAR data only without the use of image data providing spectral information.

1.2. Research Objectives

The ultimate goal of this dissertation is to present approaches for land/water discrimination in coastal zones and land cover classification of urban areas from the multispectral airborne LiDAR data. The potential use of multispectral airborne LiDAR data in these two applications is demonstrated as this dissertation maximizes their use by considering both spectral information (i.e., intensity) and geometric information collected from Earth's surface. The main objectives of this research are as following:

4

A. To develop an automated land/water discrimination approach for coastal zones. To meet this objective, a set of LiDAR point features is extracted and evaluated for land/water discrimination. New definitions of existing LiDAR point features (e.g., point density) are explained by combining multispectral LiDAR data. LiDAR point features are defined, extracted and evaluated for the first time from the multispectral LiDAR data, including the multiple returns and the normalized difference water index (NDWI). The automatic clustering method is used to define threshold values for separating water from land using each LiDAR point feature. Also, the water points are discriminated from land points by means of a seeded region algorithm with a fully automated seed points selection process.

B. To develop an automated land cover classification approach for urban areas. To accomplish this task, point merging of multispectral LiDAR data is first described. The normalized difference vegetation indices (NDVIs) are produced for the first time using the multispectral LiDAR data. Two different data clustering methods are presented, assessed and compared with previous studies for classifying terrain into the four elementary classes of buildings, trees, roads and grass. The diversity of spectral information is used to extract more urban classes. Also, new steps are proposed for assessing classification results of 3D LiDAR points from aerial images.

1.3. Dissertation Structure

The dissertation is organized as follows: Chapter 2 presents an overview of the airborne LiDAR system and a literature review of developments in land/water discrimination in coastal zones. In addition, the related work to land cover classification of urban areas is explained using either only airborne LiDAR data or LiDAR data combined with aerial/satellite imagery. Additionly, the historical development of multispectral LiDAR systems is provided. Chapter 3 presents the study area for the both applications as well as the following: multispectral LiDAR data acquisition missions, data subsets used in this research ­ including their specifications and display, and the reference data used for results evaluation. Chapter 4 explains the land/water discrimination approach including the LiDAR point features extraction, the seeded region growing algorithm, and the accuracy assessment method. Experimental results are illustrated and 5

analyzed to assess the proposed approach. Chapter 5 explains the multi-class point-based land cover classification approach. It includes the description of a ground filtering method, NDVIs calculation, NDVIs histogram construction and the application of a multivariate Gaussian decomposition. The accuracy assessment method is then presented. The results are presented and discussed for different data subsets of an urban area. Comparisons with previous attempts of land cover classification and with another method for clustering the multispectral LiDAR data are also presented. Chapter 6 summarizes the presented approaches and the findings of this research for the two applications and provides recommendations for future work.

6

CHAPTER 2

LITERATURE REVIEW

This chapter presents an overview of the airborne LiDAR system, the related work to land/water discrimination from the airborne LiDAR data, and the previous work related to land cover classification using LiDAR data only or LiDAR data combined with aerial/satellite imagery. In addition, the historical development of the multispectral LiDAR system, including laboratory prototypes, terrestrial laser scanning platforms, and attempts to develop airborne LiDAR systems is presented.

2.1. Overview on Airborne LiDAR System

The airborne LiDAR system combines a laser sensor, receiver system, Global Positioning System (GPS) receiver and inertial measurement unit (IMU) (see Figure 2.1) (Baltsavias, 1999). The laser produces optical pulses that are transmitted, reflected off Earth's surface and returned to the receiver. The pulses travel at the speed of light (3 x 108 m/sec), so a range measurement can be calculated if the travel time is recorded (Ackermann, 1999). Combining the laser range, laser scan angle, laser position using GPS and laser orientation from IMU, accurate xyz ground coordinates can be calculated for each pulse, in addition to the recorded laser backscatter (i.e., intensity) (Jensen, 2007). The laser pulse may hit leaves at the top of tree canopy, while part of the pulse travels further and may hit more leaves or branches reaching to the ground. Those pulses are reflected back and produce a set of recorded multiple returns, each having xyz coordinates (Ackermann, 1999; Morgan, 2012).

The history of LiDAR began about 50 years ago, when NASA was working with airborne prototypes to measure properties of the atmosphere and ocean water, forest canopy, and ice sheets (Petrie and Toth, 2009). In the mid-1980s, scientific investigations at Stuttgart University proved the high geometric accuracy of a laser profiler system, and hence it can be used in topographic mapping. However, the lack of a reliable commercial GPS/IMU solution for sensor

7

positioning at that time presented a significant roadblock to further development (Shuckman, 2010).

Z Y GPS X IMU

Y

X Z

Figure 2.1. Airborne LiDAR system

With the evolution of GPS/IMU technology, new airborne kinematic GPS solutions were developed. This was made possible as high-accuracy IMU became available and the GPS satellite constellation reached full configuration, providing the coverage needed for widespread operations (Shuckman, 2010). In the last 20 years, progress accelerated dramatically as LiDAR emerged as a viable and economical technology for mapping the Earth's surface and its objects. LiDAR systems have been improved from simple laser altimeters to the sophisticated multiple returns sensors with up to 300,000 pulses per second. LiDAR data are in high demand not only due to their usefulness in mapping the bare earth surface and producing DTMs, but also due to their usefulness for extracting objects such as buildings and roads as well as in characterizing forest canopies (Vosselman and Maas, 2010).

The LiDAR systems combine the advantages of photogrammetry by producing accurate point clouds and radar images by penetrating land objects (e.g., forest canopy). The photogrammetric methods of flight planning could be directly applied to LiDAR as the ground coverage of an airborne LiDAR sensor is similar to that of a traditional aerial camera (Shan and Toth, 2009).

8

Moreover, LiDAR is also capable of penetrating between trees canopies in forested areas, while the elevation of the ground is difficult to interpret photogrammetric methods. LiDAR presents a fast, accurate, effective and direct method of generating 3D data; therefore, it quickly became a very attractive mapping solution (Shuckman, 2010). Interested readers can consult some recent sources on airborne LiDAR systems (see Shan and Toth, 2009; Vosselman and Maas, 2010; Renslow, 2012).

2.2. Related Work to Land/Water Discrimination

Three decades ago, airborne LiDAR Bathymetry (ALB) systems were developed to measure water depth in coastal zones (Guenther, 1985). ALB systems typically operate at dualwavelength, namely near infrared and green, in order to detect the water surface and bottom. Allouis et al. (2010) presented a combination of mathematical and heuristic methods to estimate the water depth in very shallow water areas. The near infrared and green LiDAR waveforms were first synchronized and the peak of the near infrared waveform was amplified. The approximate positions of the water surface and bottom were then performed by a simple maxima detection algorithm. Afterwards, a non-linear least square fitting of a sum of two Gaussian functions on the green signal were conducted. Finally, the water depth was calculated from the accurate measurements of water surface and bottom positions while the minimum detectable depth was around 1 m. This approach is found to be computationally expensive and requires many processing steps; moreover, a delineation of the land/water interface was not reported in this study.

When the green laser contacts water molecules, a small portion of energy returns at a wavelength of 647 nm called the Red (Raman) channel (Guenther et al., 2000). Pe'eri and Philpot (2007) analyzed the recorded waveform at the Raman channel, which represents the received intensity (digital number) relative to time (in nanoseconds), and divided it into 41 bins. Then, a normalized difference index (NDI) was created between bin 11 and bin 27 to discriminate water from land by applying a threshold value to the NDI. The LiDAR measurements were manually classified into land, water, and suspected as water. Despite the good accuracy of the results (97%), the transition class suspected as water cannot be identified 9

as land or water. In addition, manual processing of the waveform is required for each tested dataset.

Over the past decade, land/water discrimination has been investigated using LiDAR data collected from monochromatic wavelength airborne LiDAR systems. Brzank et al. (2008) classified the water surface in the Wadden Sea using a supervised fuzzy classifier. Three features, namely height, intensity, and 2D point density, were first extracted. For each feature, a membership value (between 0 and 1) was assigned to each LiDAR point based on a membership function and two threshold values, where the value 1 indicates the water class and the value 0 indicates the mudflat class. Then, the fuzzy logic membership was calculated using individual feature membership in addition to the weight for each used feature. After that, every point was classified into either water class or mudflat class using a threshold value. Two study areas were tested and the results yielded a completeness and correctness value of up to 99.2% and 98.5%, respectively.

Smeeckaert et al. (2013) extracted a set of features from 3D LiDAR points, namely height of the LiDAR point, local mean point density, majority density, density ratio, volume ratio, and scatter. These features were used to train a Support Vector Machine (SVM) algorithm to classify water areas. The results were then refined by incorporating contextual knowledge to remove pixelwise misclassification. Their proposed work achieved an overall accuracy of more than 95% in coastal areas.

Schmidt et al. (2012) introduced a point-wise supervised classification using LiDAR data collected at the Wadden Sea based on geometric and intensity features. Five features were first extracted from the LiDAR data, including intensity, point density, distance to ground, average height, and difference of average heights calculated from neighbouring points using various radii. Three other features related to determining a point's deviation from a plane, namely lowest eigenvalue, Gaussian curvature, and mean curvature were also extracted. The aforementioned eight features were used to build a classifier based on conditional random fields to distinguish between water, mudflat, and mussel bed classes. The water class was classified with a completeness of 82.4% and a correctness of 66.3%. Although some previous studies achieved 10

considerable overall accuracies in land/water discrimination, their methods required prior knowledge of land/water interfaces to aid in the supervised discrimination process.

Höfle et al. (2009) proposed a workflow for water surface classification and land/water boundary delineation based on a region growing segmentation algorithm. Three features including roughness, intensity density, and intensity variation were first extracted. Then, seed points were selected based on a minimum intensity density threshold value, followed by the growing criteria in which threshold values were applied on the height difference, intensity density, and intensity variation in order to create segments. All segments were classified into land and water segments by applying a threshold on minimum segment size and mean segment roughness. Finally, land/water boundaries were delineated in the transition area. Two river datasets were tested and validated using a real time kinematic (RTK) GPS field survey in combination with a terrestrial ortho-image. The algorithm demonstrated an overall classification accuracy of 97% with a root mean square error (RMSE) of 0.45 m for the land/water boundaries. Despite achieving a higher overall accuracy, this method requires a significant pre-processing step, which is the model of LiDAR data dropouts. This step cannot be conducted without the knowledge of the sensor position, GPS timestamps, and scan angle. In addition, many threshold values were manually selected to fit the tested data. Also, this method could not discriminate water surface from asphalt because they both exhibited low intensity and smooth surface.

The green wavelength has the ability to penetrate water bodies, which allowing it to detect water surfaces and/or bottoms. This adds a new capability to the LiDAR system, especially at the land/water interface. Therefore, the dual-wavelength or multispectral airborne LiDAR system, including the green wavelength, is expected to overcome the aforementioned drawbacks and aid in the automation of the land/water discrimination process. Also, to the best of my knowledge, there is a lack of studies using dual-wavelength or multispectral LiDAR data in discrete returns format for land/water discrimination.

11

2.3. Related Work to Land Cover Classification

Numerous studies have been conducted on the use of airborne LiDAR height and intensity data for land cover classification (Yan et al., 2015; Brennan and Webster 2006; Im et al., 2008; Antonarakis et al., 2008). Initial studies have combined multispectral aerial/satellite imagery with LiDAR-derived height surfaces in the format of digital surface model (DSM) or normalized digital surface model (nDSM) (Huang et al., 2008; Chen et al., 2009). Other investigations have combined multispectral aerial/satellite imagery with LiDAR height and intensity data (Charaniya et al., 2004; Hartfield et al., 2011; Singh et al., 2012). The combination with aerial/satellite imagery is due to the capability of spectral information through using the derived image NDVI (Huang et al., 2008; Chen et al., 2009; Hartfield et al., 2011).

Since most of the previous studies converted 3D LiDAR points into 2D images, this required the creation of typical LiDAR images such as intensity (Brennan and Webster 2006; Im et al., 2008; Antonarakis et al., 2008; Charaniya et al., 2004; Hartfield et al., 2011), multiple returns (Brennan and Webster 2006; Charaniya et al., 2004), DSM and digital terrain model (DTM) (Brennan and Webster 2006, Antonarakis et al., 2008), and nDSM (Huang et al., 2008; Chen et al., 2009; Hartfield et al., 2011). Traditional supervised pixel-based classification techniques such as maximum likelihood (Huang et al., 2008; Singh et al., 2012), rule-based classification (Brennan and Webster 2006; Im et al., 2008; Hartfield et al., 2011), and Gaussian mixture model (Charaniya et al., 2004) were applied. Other studies accounted for the spatial coherence of different objects to avoid the noise in pixel-based classification results by using objectorientated classification techniques (Antonarakis et al., 2008; Chen et al., 2009).

For studies that used LiDAR data only, Brennan and Webster (2006) used a rule-based classification approach for segmenting and classifying a total of five bands created from LiDAR data. The five bands include DSM, digital terrain model, intensity, multiple returns, and normalized height. The image pixels were first segmented into objects by applying threshold values on mean intensity, standard deviation of intensity, mean DSM, mean normalized height, and mean multiple returns. The segmentation process included four levels reaching to ten classes. The overall classification accuracy was 94% and 98% for ten and seven classes, 12

respectively. This study relied mainly on height LiDAR data. In addition, the threshold values, which were used in the classification rules, were applied to the mean values of objects (e.g., mean intensity). That was a source of error, especially where building edges and ground returns formed one image object in the normalized height band. Also, some dense coniferous tress exhibited single returns as they could not be penetrated by the laser beam. Those trees were misclassified as buildings because the separation of trees from buildings primarily relied on the multiple returns band.

Im et al. (2008) exhibited a sensitive analysis of eight different LiDAR-derived features from height and intensity for three different sites. First, image objects segmentation was conducted based on five generated bands from the LiDAR returns, namely bare soil, first returns, last returns, height, and intensity. Second, calculations were made of six features based on height, including mean, standard deviation, homogeneity, contrast, entropy, and correlation, in addition to mean intensity, and compactness. Finally, a decision tree was used to classify the image objects into five land cover classes, which achieved a greater than 90% overall classification accuracy. It should be pointed out that the classification process relied on the three features: mean height, height standard deviation, and mean intensity, while the use of other features did not improve the land cover classifications. This might be due to the fact that the tested sites were not complex landscapes (e.g., no interference between trees and buildings). Also, the intensity band was assigned a weight of 0.1 in the segmentation process, while other bands were assigned an equal weight of 1. The reason of this is that the recorded intensity was manually adjusted during data acquisition. As a result, the intensity data were inconsistent along different flight lines, meaning that the intensity data were not sufficiently utilized in this study.

Antonarakis et al. (2008) used a supervised object-orientated approach to classify terrain into nine classes. Eight LiDAR-derived bands were first created; they are canopy surface model, terrain model, vegetation height model (VHM), intensity model, intensity difference model, skewness model, kurtosis model, and the percentage canopy model. A decision tree was then applied in order to classify three urban area datasets. This approach achieved more than 93% overall accuracy for the three datasets. However, two essential classes (roads and buildings) were not considered in the presented approach even though some buildings were present in one 13

of the three investigated sites. The VHM was calculated by subtracting the digital terrain model (created from last return) from the canopy surface model (created from first return). Some last return values had higher elevations than the first pulse return due to noise in the LiDAR receiver, which affected the calculation of the VHM. As a result, this approach could not accurately distinguish between the ground and canopy tops. Another source of error resulted from the triangulated irregular network interpolation of the LiDAR points to create images, whereas high elevations were recorded on the river surface.

Other investigations have explored the use of LiDAR-derived height surfaces such as the nDSM with multispectral imagery in land cover classification. Huang et al. (2008) incorporated LiDAR-derived nDSM with high-resolution RGB aerial image and near infrared band imagery. A pixel-based classification method ­ maximum likelihood ­ was used to obtain four land cover classes, namely buildings, trees, roads, and grass. This method achieved an overall accuracy of up to 88.3%. The classification accuracy was further improved up to 93.9% using knowledgebased classification and correction systems. This technique was based on a set of threshold values applied to the height, height difference, smoothness, anisotropic smoothness, intensity, NDVI, transformed vegetation index, area, and shape in order to detect the four land classes.

Chen et al. (2009) incorporated LiDAR-derived nDSM with Quick-Bird image in order to classify the terrain. First, two bands were derived from Quick-Bird imagery, namely NDWI and NDVI, and then combined with nDSM. Second, a hierarchical object oriented classification method was used. This method involved image segmentation before threshold values to image objects were applied. The hierarchical classification process achieved an overall accuracy of 89.4% for nine land classes. However, this method could not separate road from vacant land as these objects exhibit similar spectral and elevation characteristics. Neither aforementioned study incorporated LiDAR intensity data in its research work. In addition, the optical images were resampled to coarser resolution in order to be consistent with the created LiDAR images that resulted in mixed pixels. These pixels presented more than one land cover and caused classification errors.

14

Other studies used multispectral imagery with LiDAR data (height and intensity) to take advantage of reflectivity variation from spectrum ranges (e.g., visible and near infrared (NIR)) of different land objects. Charaniya et al. (2004) generated four LiDAR bands from height, intensity, height variation, and multiple returns, and luminance band, measured in the visible spectrum, from aerial imagery. A Gaussian mixture model was then used to model the training data of four classes, including roads, grass, buildings, and trees. The model parameters and the posterior probabilities were estimated using the Expectation-Maximization algorithm. Subsequently, those parameters were used to classify the tested dataset and resulted in an overall accuracy of 85%. The results demonstrated that height variation played an important role in classification, so the worst results were obtained by excluding the height band. Also, the overall accuracy was decreased by excluding the aerial imagery. The use of different of multiple returns improved the classification of roads and buildings. However, it decreased the classification accuracy of other terrain covers because it misclassified grass patches.

Hartfield et al. (2011) combined LiDAR data with a 1 m resolution multispectral aerial image. Two LiDAR bands, namely intensity and nDSM were generated from the LiDAR data, and NDVI was derived from the multispectral aerial image. A Classification and Regression Tree was tested on the number of band combinations. The combination of LiDAR nDSM, multispectral image, and NDVI produced the highest overall accuracy of 89.2% for eight land cover classes. Shadows in aerial images significantly affected classification results. In addition, misclassification between the bare ground and herbaceous (grass) classes occurred due to the use of intensity data because the intensity data needed to be calibrated (Hartfield et al., 2011).

Singh et al. (2012) combined Landsat Thematic Mapper (TM) imagery with LiDAR-derived bands, which included intensity, canopy height model, and nDSM. The maximum likelihood classifier was applied to classify land cover into six classes. A number of band combinations were tested on different resolutions of TM imagery, including 1 m, 5 m, 10 m, 15 m, and 30 m. The classification of 1 m resolution TM imagery combined with the three LiDAR bands brought out the highest overall accuracy of 85%. The classification results were affected by two main sources of errors: first, the LiDAR data gaps contributed to misinterpretation when creating 2D LiDAR images; second, the LiDAR intensity data were not normalized to a standard range. 15

In addition to the explained drawback of each study, the discussed studies used images created from 3D LiDAR points, causing a loss of some details. Moreover, the created images are based on the selected pixel size and the interpolation methods used to fill the gaps between pixels. As a result, mixed pixels are created at the interference between different objects (e.g., building edges).

In 3D point classification of airborne LiDAR data, multiple-class labeling has become an essential topic for 3D city modeling, change detection, map updating, disaster evaluation and emergency purposes. However, most recent studies focus on geometric characteristics of the LiDAR data (Mallet et al., 2011; Xu et al., 2014; Blomley et al., 2016; Vosselman et al., 2017). Blomley et al. (2016) extracted geometric features from the LiDAR point clouds based on multiple scales and different neighbourhood types. Then, Random Forest (RF) classifier was applied to classify the terrain into ground, buildings, cars, trees and low vegetation. A standard benchmark dataset was used for evaluations. The results of five classes, namely ground, building, cars, trees and low vegetation, demonstrated an overall accuracy of 76.8% and 86.6% for two data subsets. Vosselman et al. (2017) proposed a contextual segment-based classification using Conditional Random Field (CRF) for LiDAR point clouds classification. A combination between different point cloud segmentation methods was used to avoid under- and oversegmentation. The results demonstrated that the overall accuracy of seven classes in a 30 points/m2 dataset was 91.2% compared to 82.8% obtained from point-based classification using CRF.

Xu et al. (2014) used three different entities of the LiDAR data, namely points, planar segments, and segments derived by mean shift to classify seven urban objects through a four-level rulebased classification process. First, the LiDAR data were filtered into ground and non-ground points. Second, planar segments were used to classify points into water, ground, roof, vegetation, and undefined objects. Third, walls and roof elements were identified using the contextual information of a building. Finally, the vegetation points covering the roofs, which were wrongly classified in the third level as roof elements, were re-segmented using the mean shift method and then re-classified. For comparison, a single-entity (planar segment or points) 16

was used and four classifiers, namely Random Tree, extended AdaBoost for Multiple Classes, Artificial Neural Networks­Multiple Layer Perceptrons and Supported Vector Machine (SVM) were applied. For multiple-entity, the rule-based achieved the highest overall accuracy of 97%, which was 2.9% higher than the lowest overall accuracy obtained by SVM (94.1%). In comparison with the single-entity (planar segment), all classifiers obtained almost the same overall accuracies as multiple-entity. Thus, no significant improvement was achieved using the multiple-entity. In addition, the segmentation process is computationally expensive. For all classifiers, point classification showed lower overall accuracies (between 5.2 to 10.7%) than multiple-entity.

Niemeyer et al. (2014) applied contextual classification on airborne full waveform LiDAR data by integrating an RF classifier into a CRF framework. A total of 36 features such as height above DTM, point density and variation of intensity were extracted from the LiDAR data to accomplish this classification process. An overall accuracy of 83.4% was achieved when the LiDAR data were classified into grassland, roads, buildings with gabled roofs, low vegetation, façades, buildings with flat roofs, and trees. Although the presented method was applied on points and no segmentation was performed, it is computationally expensive due to the huge number of features extracted. Moreover, reports were confirmed of confusion between a number of classes such as trees and façades as well trees and gable roofs. The reason for this confusion is that the data were collected during leaf-on conditions, when trees partially cover roofs and façades. Also, confusion between low vegetation, trees and grassland was reported and this is attributed to the generation of reference data, which was very difficult for a human operator. This study showed the importance of using the LiDAR intensity in distinguishing grassland from road. Thus, intensity should not be eliminated by the feature importance selection and should be included for all cases.

The acquisition of LiDAR data at different wavelengths allows for collecting of different spectral information from land objects. Thus, the spectral information adds more features space, thereby allowing efficient classification of land covers. Therefore, the use of multispectral LiDAR data is expected to reduce or eliminate the need of multispectral aerial/satellite imagery in land cover classification. 17

2.4. Historical Development of Multispectral LiDAR Systems

In the past few years, numerous attempts have been conducted to develop multispectral LiDAR systems. Laboratory-based multispectral LiDAR systems have been developed to collect data at different wavelengths (Woodhouse et al., 2011; Wei et al., 2012; Shi et al., 2015). An analysis of multispectral LiDAR data, collected from Terrestrial Laser Scanning (TLS) platforms, was conducted in order to retrieve the biophysical and/or biochemical vegetation parameters (Wallace et al., 2012; Danson et al., 2014; Hakala et al., 2012; Puttonen et al., 2015; Douglas et al., 2015). A few reported attempts exist of multispectral airborne LiDAR using various airborne LiDAR systems and combining different flight missions of the same study area (Briese et al., 2012; Briese et al., 2013; Wang et al., 2014).

Laboratory-based multispectral LiDAR systems have been developed to collect data at wavelengths of 531, 550, 660, and 780 nm (Woodhouse et al., 2011), and at wavelengths of 556, 670, 700, and 780 nm (Wei et al., 2012) in order to measure the 3D structures of forest canopies. Shi et al. (2015) developed a calibration method for the backscatter intensity from laboratorybased multispectral LiDAR systems operating at wavelengths of 556, 670, 700, and 780 nm. This method accounted for incidence angle and surface roughness. After that, different vegetation indices were defined and explored in order to improve the classification accuracy.

Other investigations used TLS platforms to collect multispectral LiDAR data. For instance, a dual-wavelength full-waveform TLS platform was developed by (Danson et al., 2014), operating at two wavelengths (NIR: 1063 nm and mid infrared (MIR): 1545 nm). The platform was used to record the full-waveform returned from the forest canopies in order to measure their threedimensional structures. The Finnish Geodetic Institute developed a Hyperspectral LiDAR (HSL) system transmitting a continuous spectrum of 400 to 2500 nm (Hakala et al., 2012). An outdoor experiment was performed using seven wavelength bands ranging from 500 to 980 nm in order to discriminate between man-made targets and vegetation based on their spectral response (Puttonen et al., 2015). Douglas et al. (2015) designed a portable ground-based full-waveform TLS operating at 1064 and 1548 nm wavelengths. This system was used to collect data in the

18

Sierra Nevada National Forest. Subsequently, based on the fact that leaves absorb more strongly at 1548 nm compared to stems, leaves were discriminated from woody materials.

For multispectral airborne LiDAR attempts, Briese et al. (2012) proposed a practical radiometric calibration workflow of multi-wavelength airborne LiDAR data. Their approach was based on full waveform observations (range, amplitude and echo width), flight trajectory, and in-situ reference targets. The datasets used in this study were acquired by three flight missions based on the same flight plan within three months. Three RIEGL sensors, namely VQ-820-G (532 nm), VQ-580 (1064 nm), and LMS-Q680i (1550 nm) were utilized as one sensor for each mission. Important observations related to this study can be summarized as follow. First, the RIEGL VQ820-G was mainly designed to survey seabed, rivers, or lakes, where its scan pattern is an arclike pattern on the ground. As a result, data collected using this sensor covered a smaller area with a curved boundary. In comparison, the other two sensors produced linear and parallel scan lines. Second, the in-situ measurements of reference targets, which were used in the radiometric calibration, were performed using different sensors under specific conditions (i.e., dry conditions at zero angle of incidence). Third, the LiDAR data and the in-situ measurements of reference targets were collected at different times (in different seasons from August to December). Thus, surface conditions during different flight missions were not identical.

Briese et al. (2013) calibrated the multi-wavelength airborne LiDAR data acquired using the aforementioned three RIEGL sensors. The LiDAR data were acquired by two flight missions with an aircraft equipped with two sensors and during a short time period, four days, in order to ensure more stable reflectance behaviour of the study site at all wavelengths. The calibrated intensity data collected at 532 nm were quite dark while the data acquired at 1064 nm was brighter than those acquired at the other wavelengths. In this study, no classification process is reported. In addition, the different viewing angle of the RIEGL VQ-820-G with respect to the other two nadir-looking sensors produced LiDAR data with different boundaries. Generally, surface conditions during individual flight missions were not identical due to temporal surface changes, atmospheric conditions, and the influence of moisture content (Briese et al., 2013).

19

Wang et al. (2014) demonstrated the potential use of dual-wavelength full waveform LiDAR data for land cover classification. The LiDAR data were acquired by two laser sensors: Optech ALTM Pegasus HD400 and RIEGL LMS-Q680i. These operated at 1064 nm and 1550 nm, respectively. A radiometric correction model was first applied to the LiDAR data acquired from both sensors. The LiDAR points were then converted into spectral images with 1 m resolution and combined for subsequent processing. Three features were then derived from the Optech and RIEGL sensors' data, namely amplitude (intensity), echo width, and surface height. Finally, a supervised classification algorithm, the Support Vector Machine, was used to classify the terrain into six classes, including soil, low vegetation, road and gravel, high vegetation, building roofs, and water. Different feature combinations were tested and overall accuracies of 84.3% to 97.4% were achieved.

The conversion of the 3D LiDAR points into 2D spectral images affected the canopy reflectance information in the spectral images on account of objects under the canopy so that the canopy could not be separated from the understory vegetation and soil. This study only considered for processing the first return, extracted from each full waveform. However, land covers such as trees, building roofs and low vegetation may reflect more than one return. Also, when the RIEGL and Optech amplitude information were tested, the building roofs were not completely separated from soil or low vegetation. One possible reason is that the intensity data came from different missions conducted at different times. Since weather and/or surface conditions change over the time, the same object can exhibit different intensity values. As a result, surface height and echo width were considered major features for land cover discrimination, while the amplitude information (i.e., intensity) was treated as complementary information (Wang et al., 2014).

The first multispectral airborne LiDAR sensor, namely the Optech Titan, was recently launched by Teledyne Optech. This sensor acquires LiDAR data in three channels C1, C2, and C3 at wavelengths of 1550 nm, 1064 nm, and 532 nm, respectively. Thanks to this development, multispectral information is for the first time simultaneously available for 3D point clouds from one sensor. Few investigations over the past two years have been conducted to analyze the Optech Titan multispectral information and to explore its capability for different applications 20

such as classifying land cover, measuring water depth in shallow water areas and mapping forestry without delving too deeply into one specific application (Fernandez-Diaz et al., 2016). Some of these investigations have focused on one application such as vegetation mapping (Nabucet et al., 2016) and road mapping (Karila et al., 2017). Other investigations have focused on extracting two objects such as separation of vegetation from built-up areas (Morsy et al., 2016a) and land/water discrimination (Morsy et al., 2017a). The sensor's description and its related applications are provided in detail in Chapter 3.

21

CHAPTER 3

STUDY AREA AND DATASETS This chapter describes the world's first operational multispectral airborne LiDAR sensor, Optech Titan, which was utilized for the data acquisition used in this thesis. The study area, datasets and the reference data for land/water discrimination and land cover classification are explained in the following sections. 3.1. The Multispectral airborne LiDAR sensor "Optech Titan"

In 2014, Teledyne Optech developed the world's first operational multispectral airborne LiDAR sensor, which is known as Optech Titan. The sensor offers the possibility of obtaining multispectral active data during day and night times. This ability facilitates new applications and new information extraction capabilities for LiDAR. The sensor operates simultaneously at three wavelengths through a single oscillating mirror and collects point clouds in three channels with different looking angles, namely MIR (1550 nm) in C1 at 3.5° forward-looking, NIR (1064 nm) in C2 at 0° nadir-looking, and green (532 nm) in C3 at 7° forward-looking. Specifications of the Optech Titan sensor are provided in Table 3.1 (Titan Brochure and Specifications, 2015).

Table 3.1. Optech Titan sensor specifications Parameter Wavelength Specification Channel 1 = 1550 nm, Channel 2 = 1064 nm, Channel 3 = 532 nm Altitude Topographic: 300-2000 m AGL, all channels Bathymetric: 300-600 m AGL, channel 3 Scan Angle (FOV) Beam Divergence Programmable; 0 - 60° max Channel 1 & 2 = 0.35 mrad , Channel 3 = 0.7 mrad Pulse Repetition Frequency 50 - 300 kHz/channel; 900 kHz total 22

Scan Frequency Swath width Point Density 1
1

Programmable; 0 - 210 Hz 0 - 115% of AGL Bathymetric: >15 points/m2 Topographic: >45 points/m2

Assumes 400 m AGL, 60 m/s aircraft speed, 40° FOV.

Figure 3.1 shows the expected spectral reflection curve of different objects (drawn from USGS Digital Spectral Library splib06a; Clark et al., 2007) in the three channels. Water bodies are penetrated from the green wavelength, while MIR and NIR laser beams are completely absorbed by clear calm water. Using the green wavelength ensures high density point clouds for shallow water mapping. Vegetation (e.g., grass and trees) is strongly reflective at the NIR wavelength, while soil (e.g., clay) is more reflective than vegetation at the MIR and green wavelengths. As such, combining multispectral LiDAR data collected with the aforementioned three wavelengths ensures a higher reliability and accuracy of information extraction compared to monochromatic wavelength LiDAR systems. In addition, this system can be used for topographic mapping, vegetation mapping and 3D land classification.

Figure 3.1. Spectral reflection curve of different terrain covers 23

Over the past two years, the multispectral Titan data have been explored for land cover classification by converting the LiDAR points into raster images (El-Ashmawy, 2015; Bakula et al., 2016; Morsy et al., 2016b; Zou et al. 2016; Matikainen et al., 2017). Morsy et al. (2016b) showed the improvement of land cover classification results using multispectral LiDAR data rather than single intensity data. Three intensity images were first created from the collected point clouds at the three wavelengths as well as the digital surface model (DSM). A maximum likelihood classifier was then applied to each intensity image, combined three-intensity images and three-intensity images combined with DSM in order to classify terrain into buildings, trees, roads, grass, soil and wetland. The use of a single intensity LiDAR image, created from C1, C2 and C3, led to overall classification accuracies of 34.0%, 48.5% and 41.5%, respectively. The overall classification accuracy improved to 65.5% using the combined three-intensity images. Moreover, the overall classification accuracy was increased to 72.5% by incorporating the height LiDAR data (i.e., DSM image). Zou et al. (2016) segmented the intensity and height images into image objects based on multi-resolution segmentation integrating different scale parameters. The objects were then classified based on a set of indices, namely NDVI, ratio of green, ratio of returns counts, and difference of elevation. The method used achieved above 90% overall accuracy for classifying the terrain into nine classes.

Matikainen et al. (2017) presented an object-based analysis of multispectral LiDAR data for land cover classification. Three intensity images, maximum and minimum DSMs, and DTM were first created and segmented into homogeneous regions. Then, about 41 features were calculated for each segment based on intensity and height from the three channels. The segments were divided into high and low based on their mean height with a threshold value of 2.5 m. Training segments for high objects (i.e., buildings and trees) and low objects (i.e., asphalt, gravel, rocky areas and low vegetation) were picked from high and low segments, respectively. The training segments were used as input for an RF classifier in order to investigate the potential of the 41 features for separating the six classes. The classification results of a suburban area demonstrated an overall accuracy of 95.9%. The aforementioned studies focused on image data classification and supervised classification methods that require prior knowledge/training areas of the obtained classes.

24

Nabucet et al. (2016) used the data collected at near infrared and green wavelengths for vegetation mapping. The NDVI, defined by Morsy et al. (2016a), was used to separate vegetation from built-up areas. Then, threshold values were applied on height to map different types of vegetation. This method achieved an overall accuracy of 61%. Karila et al. (2017) created number of LiDAR images based on intensity from the three channels and height data for road mapping. Image segmentation was then applied and an RF classifier was used to map asphalt and gravel. This method achieved an overall accuracy of 80.5%. With the focus on LiDAR points' classification, Wichmann et al. (2015) studied and analyzed spectral patterns of different classes and showed that the multispectral data could potentially be used in land cover classification. Morsy et al. (2016a) evaluated three spectral indices, derived from the recorded intensity values in the three channels. The spectral indices demonstrated high capability in discriminating water from land in coastal zones and in separating low and high vegetation from built-up areas in urban areas. Morsy et al. (2017b) proved that the 3D points classification achieves a higher overall accuracy than image-based classification by about 3% for four land covers (buildings, trees, roads and grass).

3.2. Study Areas and Datasets for Land/Water Discrimination

Two study areas were considered for land/water discrimination evaluations of coastal zones (see Figure 3.2). The first area covers part of Lake Ontario, which is located in Scarborough, Ontario, Canada, and it includes stones and a loose land/water interface with a gentle slope that is affected by the tidal range. The second area covers part of Tobermory Harbour, which is located in Tobermory, Ontario, Canada, and it includes sharp solid land/water interface that is clearly visualized and identified. The two areas also cover a variety of land objects, including roads, bare soil, grass, shrubs, buildings and trees. The LiDAR point clouds were collected in the three channels during a flight mission in September, 2014 at Lake Ontario and in May, 2015 at Tobermory Harbour. The collected data were provided by Teledyne Optech. After the data were processed and calibrated, the three channels were automatically aligned and the data were georeferenced to WGS 1984 UTM Zone 17N using Optech's LiDAR Mapping Suite (LMS) software. 25

Data subsets at Lake Ontario (Area_LO) and at Tobermory Harbour (Area_TH) were clipped for experimental testing. Table 3.2 summarizes the specifications of each data subset. The LiDAR data of Area_LO and Area_TH are shown in Figure 3.3 and Figure 3.4, respectively. Most points have intensity values range from 1 to 1000, so that the intensity data are displayed with that range. The difference in the number of points between channels is attributed to the interaction of land objects or water bodies with different wavelengths (e.g., reflection from the water surface and/or water bottom, and greenness of the vegetation).

Figure 3.2. Study areas for land/water discrimination

Table 3.2. Data subsets specifications Specification Parameter Dimension (m x m) Altitude (m) 26 Area_LO 400 x 200 ~ 430 Area_TH 400 x150 ~ 475

Scan Angle Pulse (PRF) Scan Frequency Number of Returns Number of points: Channel 1 Channel 2 Channel 3 Average Point Spacing (m) Repetition Frequency

± 15° 200 kHz/channel; 600 kHz total 40 Hz Up to 4 returns 971,490 1,123,418 1,403,386 0.27

± 20° 225 kHz/channel; 675 kHz total

590,829 599,153 640,677 0.25

C1 (MIR)

C1 (NIR)

C3 (green)

91.60 38.68

89.48 38.64

92.43

(a)

(b)

33.17

(c)

1000 1

1000

1000

(d)

1

(e)

1

(f)

(g) (h) (i) Figure 3.3. LiDAR point clouds of Area_LO colorized by elevation: a, b and c; intensity: d, e and f; and number of returns g, h and i

27

C1 (MIR)

C1 (NIR)

C3 (green)

166.11 138.75

163.10

165.12

(a)

135.41

(b)

119.86

(c)

1000 1

1000

1000

(d)

1

(e)

1

(f)

(g) (h) (i) Figure 3.4. LiDAR point clouds of Area_LH colorized by elevation: a, b and c; intensity: d, e and f; and number of returns g, h and i

As shown in Figure 3.3 and 3.4, the highest elevation found in the three channels is very close, while the lowest elevation found in C3 is lower than that found in C1 and C2 by about 5 m for Area_LO and 16 ­ 19 m for Area_TH. This can be attributed to the recorded points from the water bottom. The intermediate part of the water body has high intensity variation in all channels. The vegetation area (e.g., trees) has a relatively high intensity variation in C1 and C2. The water points have single returns in C1 and C2, while some of them have double returns in C3 due to the reflectance from the water surface and bottom. Most water points in C3 have double returns for Area_LO as it is a shallow water area, while they have single returns for Area_TH as it represents a clear and deep water body.

28

Since no aerial images during the LiDAR data acquisition were available, reference images available in the GIS database were used. An ortho-image captured on January 15th, 2015 with 0.08 m resolution and a World View 2 image captured in April 21st, 2012 with 0.5 m resolution were used for Area_LO (Figure 3.5) and Area_TH (Figure 3.6), respectively. The land/water boundary was manually digitized from the reference images. The points in the water body were labeled as reference water points and the points on land side were labeled as reference land points for the purpose of accuracy assessment.

Figure 3.5. Reference image for the land/water interface of Area_LO

Figure 3.6. Reference image for the land/water interface of Area_LH 29

3.3. Study Area and Datasets for Land Cover Classification

The study area for this part is located in Oshawa, Ontario, Canada as shown in Figure 3.7. The LiDAR point clouds were acquired in the three channels for a single strip during a flight mission in September, 2014. The strip includes various types of land covers of buildings with different roof materials, parking lots, sidewalks, roads, bare soil, open spaces with grass cover, shrubs, trees with green leaves, trees with red leaves, power lines and swimming pools. The collected data were provided by the supplier after sensor calibration. The data were geo-referenced to WGS 1984 UTM Zone 17N using Optech's LMS software.

Figure 3.7. Study area for land/water classification

Three subsets from the LiDAR strip were clipped for experimental testing. The three subsets are described as following. Area 1 is divided into two parts: mixed built-up with vegetation and

30

vegetation area. It also includes power lines and a number of swimming pools. Area 2 is a mixed area between built-up and vegetation consisting of residential and industrial buildings with different roof materials (colors), road surfaces, high and low vegetation, power lines and swimming pools. Area 3 consists mainly of vegetation area but includes a small built-up area with few buildings and roads. This area does not contain power lines but includes a few swimming pools.

Table 3.3 summarizes the specifications of the three data subsets. The LiDAR data of Area 1, Area 2 and Area 3 are shown in Figure 3.8, Figure 3.9 and Figure 3.10, respectively. It should be pointed out that the data acquisition was under leaf-on conditions, and hence most points of the vertical distribution within trees describe only the canopy. As aforementioned, the number of points is different from each channel due to the interaction of land objects with different wavelengths. The lowest number of points is in C3 because there are no recorded returns from the trees with red leaves in this channel.

Table 3.3. Data specifications for Area 1, Area 2 and Area 3 Specification Parameter Dimension (m x m) Altitude (m) Scan Angle Pulse (PRF) Scan Frequency Number of Returns Number of points: Channel 1 Channel 2 Channel 3 Average Point Spacing (m) 833216 887744 723102 Repetition Frequency Area 1 600 x 410 Area 2 490 x 470 ~ 1075 ± 20° 200 kHz/channel; 600 kHz total 40 Hz Up to 4 returns 796226 825176 742158 0.51/Channel 707534 781169 510879 Area 3 550 x 330

31

C1 (MIR)

C1 (NIR)

C3 (green)

106.34 68.02

106.25

106.13

(a)

67.73

(b)

67.33

(c)

1000 1

1000

1000

1 1 (d) (e) (f) Figure 3.8. LiDAR point clouds of Area 1 colorized by elevation: a, b and c; and intensity: d, e

and f

C1 (MIR)

C1 (NIR)

C3 (green)

115.51 76.19

115.38

115.21

(a)

76.17

(b)

76.17

(c)

32

1000 1

1000

1000

1 1 (d) (e) (f) Figure 3.9. LiDAR point clouds of Area 2 colorized by elevation: a, b and c; and intensity: d, e

and f

C1 (MIR)

C1 (NIR)

C3 (green)

122.15 76.10

121.79 78.31

121.60 78.16

(a)

(b)

(c)

4096 1

4096 1

4096 1

(d) (e) (f) Figure 3.10. LiDAR point clouds of Area 3 colorized by elevation: a, b and c; and intensity: d, e and f

Aerial images of the tested areas were captured at different time of LiDAR data acquisition mission. These images were geo-referenced with the LiDAR data with 0.5 m pixel size. A set of polygons was randomly selected for land classes, and the points within those polygons were used as reference points to be used for validating the land cover classification results. Figure 33

3.11, Figure 3.12 and Figure 3.13 show aerial images of the tested areas along with polygons of different classes. The polygons are coloured as follows: buildings (dark red), trees with green leaves (pink), trees with red leaves (red), roads (yellow), grass (green) and swimming pools (blue).

Figure 3.11. Aerial image of Area 1

Figure 3.12. Aerial image of Area 2

34

Figure 3.13. Aerial image of Area 3

35

CHAPTER 4

LAND/WATER DISCRIMINATION

This chapter presents the first main objective of this dissertation research, which is evaluating a set of LiDAR point features for discriminating water from land in coastal zones. Also, this chapter presents an automated land/water discrimination approach based on a seeded region growing algorithm. The methodology is first explained with definition and extraction of LiDAR point features from the three channels. Following that, comes a description of the automation process of the seeded region growing algorithm. The results and analysis of the discrimination process are then presented.

4.1. Methodology

4.1.1. Point Features Extraction

A set of point features was extracted for each LiDAR point. These point features are divided into three categories: elevation-based features including elevation difference (Z) and roughness (R); intensity-based features including normalized difference water index (NDWI), intensity coefficient of variation (ICOV) and intensity density (ID); and geometry-based features including point density (PD) and multiple returns (MR). The elevation and the three recorded intensity values of the LiDAR data were also evaluated for land/water discrimination. Figure 4.1 shows the workflow of land/water discrimination using different point features.

The water points were labeled based on the following assumptions. First, the water points are assumed to have the lowest elevation and intensity in the scene (Antonarakis, et al., 2008; Brzank et al., 2008). Second, the intensity variation is higher in water bodies than land areas (Höfle et al., 2009) due to the wavy effect and different scan angles. Third, the MIR and NIR wavelengths are usually used to acquire LiDAR data from water surfaces, while the green wavelength is used to acquire LiDAR data from both water surfaces and bottoms due to its ability to penetrate water bodies. Thus, these characteristics increase the usefulness of the 36

LiDAR data in land/water discrimination, when the C3 (green) is combined with C1 (MIR) or C2 (NIR) channel.

LiDAR Point Clouds (C1, C2, C3)

Point Features Extraction

Elevation­based Features

Intensity­based Features

Geometry­based Features

Z

R

NDWI

ICOV

ID

PD

MR

Discrimination by Threshold Values

Labeled LiDAR Points

Ortho-image

Accuracy Assessment

Figure 4.1. Workflow of land/water discrimination based on extracted point features Z: elevation difference, R: roughness. NDWI: normalized difference water index, ICOV: intensity coefficient of variation, ID: intensity density, PD: point density and MR: multiple returns

The water points were labeled by testing the elevation and intensity attributes of each single point. The other features were extracted based on a local neighbourhood of each point, where neighbouring points were considered using a fixed searching radius in 2D space. A searching radius of 1 m was used in order to define the land/water interface with high resolution and to ensure sufficient number of points in the neighbourhood (Morsy et al., 2017a). The details of those point features are presented below, assuming that a LiDAR point pi and its neighbouring points are j, where j = 1, 2, 3, . . , n, and n is the total number of neighbouring points of pi.

37



Elevation Difference (Z)

Elevation difference is the difference between the maximum and the minimum elevations of neighbouring points of a LiDAR point. The water points were assumed to have high ZC3 values when using the green wavelength (i.e., 532 nm) due to the recorded returns from water surfaces and bottoms as well as low Z C1 or C2 values when using wavelengths in the infrared range (e.g., 1064 nm) due to the recorded returns from water surfaces only. 

Roughness (R)

Roughness is the standard deviation of a LiDAR point's elevation in relation to neighbouring points. The water points were assumed to create a horizontal surface (i.e., low RC1 or C2) when using wavelengths in the infrared range (e.g., 1064 nm). The RC3 of water points should be increased when using the green wavelength (i.e., 532 nm) due to the recorded returns from the water surface and bottom. Roughness can be calculated from Equation (4-1), where Zj is the point's elevation and is the mean elevation.

(

)

(4-1)



Normalized Difference Water Index (NDWI)

The Normalized Difference Water Index was first defined by McFeeters (1996) based on the green and NIR wavelengths in order to discriminate water bodies from vegetation and soil using satellite imagery. The green wavelength serves to maximize reflectance from water bodies, and the NIR wavelength serves to minimize low reflectance from water bodies as well as to capitalize on high reflectance from vegetation and soil. The shortwave infrared (i.e., 1500 ­ 2500 nm) was used with the green wavelength to enhance McFeeters's NDWI in distinguishing water bodies (Xu, 2006) and was used with the NIR wavelength for estimating water content of vegetation canopy (Gao, 1996). Similarly, three recorded intensity values ( IC1, IC2 and IC3) were employed to form the three NDWIs described below (Morsy et al., 2016a):

(4-2) 38

(4-3)

(4-4)



Intensity Coefficient of Variation (ICOV)

The intensity coefficient of variation is calculated by dividing the standard deviation by the mean of the intensity values. The water points were assumed to have ICOVC1 or C2 or C3 greater than those of land objects. The reason of this is the intensity variation in water bodies is relatively high as a result of wavy effect and because intensity values were recorded at different scan angles. For each LiDAR point in any channel, the ICOVC1 or C2 or C3 can be calculated from Equation (4-5), where Ij is the point's intensity value and is the mean of the intensity values.

 

(

) (4-5)



Intensity Density (ID)
or C2 or C3

Within the searching radius, the IDC1

is the percentage of LiDAR points that have

intensity values below a threshold value (Ithresh) as shown in Equation (4-6). The water points were assumed to have higher IDC1 or C2 or C3. This is due to the water points usually having the lowest intensity values in the scene.

(4-6)



Point Density (PD)

Point density refers to the number of LiDAR points per square meter. The water points were assumed to have low PDC1 or C2 when using wavelengths in the infrared range (e.g., 1064 nm), and high PDC3 when using wavelengths in the green wavelength (i.e., 532 nm) due to the presence of additional points from water bottoms. 39



Multiple Returns (MR)

The multiple returns feature is defined as the recorded number of returns for each laser's echo. Table 4.1 shows the number of possible returns which can be recorded with the infrared and green wavelengths. The water point should have a single return when using wavelengths in the infrared range (e.g., 1064 nm) due to the recorded returns from the water surface only, and the water points should have double returns when using wavelengths in the green wavelength (i.e., 532 nm) due to the recorded returns from water surfaces and bottoms.

Table 4.1. Recorded returns from different classes at infrared and green wavelengths Classes Built-up areas Water Vegetation Infrared wavelengths single single single/more Green wavelength single single/double single/more

The point feature was extracted considering single or double channels. For instance, the evaluation of elevation, intensity, ICOV and ID features were conducted using the data of a single channel (C1, C2 or C3), thus different results were obtained from the three channels for each point feature. The three NDWI were calculated using the combination of the three channels, and hence three different results were obtained. The evaluation of Z, R, PD and MR features were conducted using the combined data from two channels (i.e., C3 with C1 or C3 with C2).

4.1.2. Land/Water Points Labeling Based on LiDAR Point Features

A threshold value was applied to each extracted point feature in order to label land and water points. Threshold values were automatically defined as follows: For elevation, an elevation histogram was constructed from each channel with bin size of 1 m. A histogram is the graphical representation of the distribution of any data. The histogram has been used to create intensity histograms by plotting the number of pixels for each digital number in an image (Silverman, 1986). Next, peak values were detected using the peak detection algorithm, where the local

40

maxima (peaks) were detected. A local maxima is a data sample that is larger than its two neighbouring samples.

The elevation of the first peak represents the average water level for C1 or C2. The second peak represents the average water level for C3, whereas the first peak is for returns from the water bottom. Therefore, the elevation threshold value was automatically defined compared to previous studies such as Brzank et al. (2008), Schmidt et al., (2012) and Smeeckaert et al. (2013), which selected this threshold manually. As aforementioned for MR, double returns are recorded from the water body in C3, while single returns are recorded in C1 or C2. Based on these characteristics, the water and land points were labeled.

Previously, the water points were labelled from single wavelength LiDAR data based on R (Höfle et al., 2009) or PD (Brzank et al., 2008; Smeeckaert et al., 2013) by applying manually selected threshold values. In this dissertation, a combination of C3 with C1 or C2 was used to automatically label water points. The values of Z, R and PD are higher for C3 than C1 or C2. Therefore, a LiDAR point was labeled as land or water as follows:

{

(4-7)

{

(4-8)

{

(4-9)

The Jenks natural breaks optimization method was used to determine threshold values for intensity, NDWI, ICOV and ID (Jenks, 1967). This optimization method has been designed to minimize within-class variances and maximize the between-classes variance. Let any of the aforementioned LiDAR point features value ranges from [a, , b], and the threshold values t  [a, , b]. The t was identified in order to separate land from water by maximizing the betweenclasses sum of the squared mean differences. This was done as follows: 41

*(

)

(

) +

(4-10)

Where M is the mean of point feature values, M1 and M2 are the mean values of the first and second class, respectively. M was first calculated. Then, the points were divided into two classes with ranges [a, , t] and [t, , b]. The mean values M1 and M2 were calculated. Finally, the optimal threshold value t was obtained from Equation (4-10).

The tested data was assumed to consist of three main classes: water, vegetation and built up areas. Therefore, two threshold values were identified using the Jenks optimization based on intensity. As the water points were assumed to have the lowest intensity in the scene, the first threshold value (Ithresh) was used to label water points. If the point's intensity is lower than the Ithresh, the point was labeled as water; otherwise it was labeled as land. The (Ithresh) was used to calculate the ID from Equation (4-6), and a percentage (IDthresh) was used. For NDWI and ICOV, the LiDAR point clouds were separated into land and water using one threshold value for each point feature, namely NDWIthresh and ICOVthresh, respectively. This was done as follows:

{

(4-11)

{

(4-12)

{

(4-13)

The threshold values for intensity-based features were automatically selected, while previous studies which used intensity (Brzank et al., 2008; Höfle et al., 2009; Schmidt et al., 2012; Smeeckaert et al., 2013), ID or ICOV ( Höfle et al., 2009) selected threshold values for those points manually.

42

4.1.3. Seeded region growing for labeling water points

The seeded region growing algorithm is divided into two main steps: selection of seed points and criteria for region growing. Previous studies mainly relied on the manual selection of seed points or applying threshold values on the LiDAR data to select the seed points, which make this step specific for the tested data (Höfle et al., 2009). Also, in region growing criteria, threshold values were manually selected to fit the tested data (Höfle et al., 2009). This dissertation presents an automated seed point selection for labeling water points based on seeded region growing in coastal zones using the automated extracted point feature MR. The workflow of this algorithm is shown in Figure 4.2. In seed points selection, the point feature MR was used to find the possible seed points. As mentioned in Table 4.1, the water points should have single and double returns when the data are collected at the infrared and green wavelengths, respectively. First, the points that have single returns (points_single) at the infrared wavelengths (e.g., 1064 nm) were extracted, which included points from built-up areas (building roofs and road surfaces) and vegetation as well as water surfaces. Second, the first points of the double returns (points_first_of_double) at the green wavelength (i.e., 532 nm) were extracted. These points included points from vegetation as well as water surfaces. For each point in (points_single), the nearest point within the footprint of the green wavelength from (points_first_of_double) was found. For simplicity's sake, the footprint was assumed to be a circle with radius rG, where rG=0.5*altitude*beam divergence (). The footprint was used to ensure that the point belongs to the same object. This step is essential for removing points that belong to built-up areas and vegetation with single returns. Thus, all points in (points_single) that have nearest points from (points_first_of_double) were considered as the possible seed points.

It should be pointed out that the vegetation could be recorded with single or more returns at various wavelengths due to different interactions between vegetation and those wavelengths as well as the change of scan angles. Therefore, the point feature Z was used to refine the possible seed points. The elevation difference within a searching radius for each point was checked against the data fluctuation (relative error in the collected range data; e.g., 0.5 m) in the 43

infrared range to preserve horizontal surfaces, and hence filter out non-water points. Thus, the remaining points were considered as water seed points. In the region growing criteria, all points in the scene were considered as neighbouring points of each water seed point. Then, points within neighbourhood of each seed point were arranged according to their distance to this seed point. Next, the Z was tested point by point against the data fluctuation in order to label all water points in the scene.

LiDAR Point Clouds (C1, C2, C3)

Point's Features Extraction

Elevation­based Features

Geometry­based Features

Elevation Difference (Z)

Multiple Returns (MR)

Seed Points Selection

Possible Seed Points

Refinement

Water Seed Points

Growing Criteria

Labeled LiDAR Points

Ortho-image

Accuracy Assessment

Figure 4.2. Workflow of land/water discrimination based on seeded region growing algorithm 44

4.1.4. Accuracy Assessment

The ortho-image (for Area_LO) and the World View 2 image (for Area_TH) were used to label the points in the water body as reference water points by digitizing the land/water boundary. The number of reference points for land and water in each channel are provided in Table 4.2. To evaluate the success of using different LiDAR point features and the region growing algorithm in labeling water points, the confusion matrix was constructed as shown in Table 4.3.

Table 4.2. Number of reference points Area_LO Channel C1 C2 C3 Land 834,108 931,126 884,586 Water 137,382 192,292 518,800 Area_TH Land 456,437 450,323 451,111 Water 134,392 148,830 189,566

Table 4.3. Confusion matrix elements Reference Data Discrimination Land Water Land True Negative (TN) False Positive (FP) Water False Negative (FN) True Positive (TP)

Where TP: the point was labeled as water in both discrimination process and reference data TN: the point was labeled as land in both discrimination process and reference data FP: the point was labeled as water in discrimination process and as land in reference data FN: the point was labeled as land in discrimination process and as water in reference data

Completeness, correctness and overall accuracy were then calculated for accuracy assessment of the results between the extracted water points and the manually labelled water points by delineating land/water boundaries from reference images. The completeness (or recall) indicates how complete the extracted water points are; whereas the correctness, (or precision), indicates 45

how correct the extracted water points are. The overall accuracy indicates how successful the discrimination process is. The three quality measures were defined by Heipke et al. (1997) as following:

( )

(4-14)

( )

(4-15)

( )

(4-16)

Since the land/water interface is very important to monitor, the land/water boundary was delineated and compared with the digitized boundary from the reference images. About ten sectors were drawn perpendicular to the land/water boundary. The differences between the reference boundary and the delineated boundaries were measured and the root mean square error (RMSE) was calculated.

4.2. Results and Analysis

The acquired data were received from the supplier as a time-tagged 3D point clouds file with multiple returns in LAS file format for each channel. The LAS is a compact file format specific for LiDAR data. It contains xyz coordinates, raw intensity values, scan angle, return number, number of returns and the GPS time of each LiDAR point. The LAS data files were converted into ASCII files using lastools so that they could be processed. The LiDAR point features extraction, the seeded region algorithm and point labeling were implemented using MATLAB. The threshold values were identified using Jenks break optimization method by using the embedded function in ArcGIS. The accuracy assessment and the LiDAR data visualization were both conducted using ArcGIS as well.

46

4.2.1. Land/Water Discrimination from Elevation-based Features

In coastal zones, the water points were assumed to have the largest area with the lowest elevations in the scene. Therefore, an elevation histogram was first constructed with a bin size of 1 m, where the entire range of elevations is divided into a series of equal intervals (i.e., bins), as shown in Figure 4.3 and 4.4. Then, the first highest peak was automatically detected and considered the average water surface elevation. A threshold of 0.5 m was added to the average water surface elevation, and all points with elevation less than or equal to that elevation (i.e., 39.5 m for Area_LO and 139.5 for Area_TH) were labeled as water points. The Z and R were also calculated and Equation (4-7) and (4-8) were used to label water points. The labeled points from the three channels based on elevations, Z and R are shown in Figure 4.5 and 4.6 for Area_LO and Area_TH, respectively.

Figure 4.3. Elevation histogram of the three channels for Area_LO

Figure 4.4. Elevation histogram of the three channels for Area_TH

47

(a)

(b)

(c)

(d)

(e)

Water Land

(f)

(g)

Figure 4.5. Labeled LiDAR points of Area_LO based on: elevation using (a) C1; (b) C2; and (c) C3; Z using (d) C1 and C3; and (e) C2 and C3; and R using (f) C1 and C3; and (g) C2 and C3

(a)

(b)

(c)

48

(d)

(e)

Water Land

(f)

(g)

Figure 4.6. Labeled LiDAR points of Area_TH based on: elevation using (a) C1; (b) C2; and (c) C3; Z using (d) C1 and C3; and (e) C2 and C3; and R using (f) C1 and C3; and (g) C2 and C3 Table 4.4. Accuracy measures of labeled point for Area_LO based on elevation, Z and R. The best results are highlighted in bold. Completeness (%) Point Feature Elevation Channel (s) C1 C2 C3 Z C1C3 C2C3 R C1C3 C2C3 Land 99.3 99.3 99.3 62.9 67.3 57.5 59.8 Water 99.6 99.6 99.9 98.1 97.5 98.0 97.6 Correctness (%) Land 99.9 99.9 99.9 98.3 97.9 98.0 97.7 Water 95.8 96.9 98.8 60.8 63.6 57.5 58.8 Overall Accuracy (%) 99.3 99.4 99.5 75.9 78.5 72.5 73.8

Table 4.5. Accuracy measures of labeled point for Area_TH based on elevation, Z and R. The best results are highlighted in bold.

49

Completeness (%) Point Feature Elevation Channel (s) C1 C2 C3 Z C1C3 C2C3 R C1C3 C2C3 Land 99.8 99.8 99.1 68.8 71.3 51.7 51.7 Water 99.9 99.9 99.9 57.6 55.6 57.3 55.5

Correctness (%) Land 99.9 99.9 99.9 79.5 79.2 74.2 73.5 Water 99.2 99.3 97.9 43.7 44.9 33.3 32.6

Overall Accuracy (%) 99.8 99.8 99.4 65.5 66.6 53.4 52.8

The first peak in C1 or C2 elevation histograms represents the average water surface elevation, while the first peak in C3 elevation histogram represents the average water bottom elevation. However, the highest peak in C3 represents the average water surface elevation. Since water bodies in coastal zones have the lowest elevation in the scene, the labeling of water points using elevation produces high overall accuracies of more than 99% in all channels, as presented in Table 4.4 and 4.5 for Area_LO and Area_TH, respectively. Confusion matrices of the results using different point features are illustrated in Appendix A. The discrimination based on Z and R features varies between the two tested areas and the two used combinations. The reason of this is the objects (e.g., vegetation) that have double or more returns in C3, but have single returns in C1 or C2 were misclassified as water points. On the contrary, water areas that have single returns in C3 were wrongly classified as land. As a result, the correctness of the combination of C1 with C3 is lower than of C2 with C3 because the classification errors of land points as water points are much higher in the first combination. However, the completeness of the water class is high in Area_LO (over 97.5%) for Z and R features. The completeness and correctness for Area_TH are relatively low due to the discrimination errors. In general, the discrimination results based on Z and R features demonstrated higher overall accuracy for Area_LO than Area_TH due to the greater number of returns from water bottoms. This is because the water body of Area_TH is deeper than of Area_LO. The Z and R features help enhance discrimination capacity wherever bottom returns

50

are present, which reflects the potential use of multi-channels that have various characteristics in land and water regions.

4.2.2. Land/water discrimination from intensity-based features

The water points were assumed to have the lowest intensity values in the scene, the Jenks break optimization was used to define Ithresh based on this assumption. As presented in Equation (4-11) and (4-12), respectively, the NDWIthresh and ICOVthresh were defined using Jenks break optimization as well to label water points. Table 4.6 presents the threshold values for Area_LO and Area_TH. The IDthresh was selected as 0.7 to minimize the type I (FP) and II (FN) errors. The labeled LiDAR points from different channels were based on intensity, NDWI, ICOV and ID for Area_LO and Area_TH, as shown in Figure 4.7 and 4.8, respectively. The completeness, correctness and overall accuracy of the labeled points are provided in Table 4.7 and 4.8. Confusion matrices of the results using different point features are illustrated in Appendix A.

Table 4.6. Threshold values of Ithresh and ICOVthresh Threshold Channel (s) Ithresh C1 C2 C3 ICOVthresh C1 C2 C3 NDWIthresh C3C2 C3C1 C1C2 Area_LO 164 194 57 0.71 0.73 0.78 -0.19 -0.12 0.18 Area_TH 137 99 93 0.65 0.63 0.49 -0.12 0.01 0.12

51

(a)

(b)

(c)

(d)

(e)

(f)

(g)

(h)

(i)

Water Land

(j)

(k)

(l)

Figure 4.7. Labeled LiDAR points of Area_LO based on: intensity using (a) C1; (b) C2; and (c) C3; NDWI using (d) C3C2; (e) C3C1; and (f) C1C2; ICOV using (g) C1; (h) C2; and (i) C3; and ID using (j) C1; (k) C2; and (l) C3

52

(a)

(b)

(c)

(d)

(e)

(f)

(g)

(h)

(i)

Water Land

(j)

(k)

(l)

Figure 4.8. Labeled LiDAR points of Area_TH based on: intensity using (a) C1; (b) C2; and (c) C3; NDWI using (d) C3C2; (e) C3C1; and (f) C1C2; ICOV using (g) C1; (h) C2; and (i) C3; and ID using (j) C1; (k) C2; and (l) C3

53

Table 4.7. Accuracy measures of labeled points for Area_LO based on intensity, NDWI, ICOV and ID. The best results are highlighted in bold. Completeness (%) Point Feature Intensity Channel (s) C1 C2 C3 NDWI C3C2 C3C1 C1C2 ICOV C1 C2 C3 ID C1 C2 C3 Land 54.0 51.6 44.0 69.8 78.6 65.7 51.8 39.1 76.8 72.4 76.4 51.8 Water 73.1 72.1 89.4 38.8 43.7 54.6 96.6 96.0 54.5 62.5 59.0 89.3 Correctness (%) Land 92.4 90.0 87.6 84.7 89.5 87.5 98.9 97.9 74.2 92.1 90.0 89.2 Water 20.7 23.5 48.4 21.0 25.2 24.7 24.8 24.6 57.9 27.2 34.1 52.1 Overall Accuracy (%) 56.7 55.1 60.8 64.5 73.7 63.8 58.1 48.8 68.5 71.0 73.5 65.7

Table 4.8. Accuracy measures of labeled points for Area_TH based on intensity, NDWI, ICOV and ID. The best results are highlighted in bold. Completeness (%) Point Feature Intensity Channel (s) C1 C2 C3 NDWI C3C2 C3C1 C1C2 ICOV C1 C2 C3 ID C1 Land 49.4 52.7 46.4 37.6 74.6 33.6 60.8 56.4 57.1 54.2 Water 85.3 77.0 95.7 47.3 42.5 56.0 96.0 94.4 61.1 90.4 54 Correctness (%) Land 92.0 87.4 96.3 68.3 81.5 69.8 98.1 96.8 77.7 95.1 Water 33.2 35.0 42.9 20.0 33.0 21.8 41.9 41.7 37.4 36.8 Overall Accuracy (%) 57.6 58.7 61.0 40.0 67.3 39.1 68.8 65.8 58.3 62.4

C2 C3

59.6 54.2

67.9 97.5

84.9 98.1

35.7 47.2

61.7 67.0

Generally, the accuracy measures using intensity-based features are relatively low because water points were assumed to have the lowest intensity values in the scene, but vegetation points (Area_LO) and built-up areas (Area_TH) also have low intensity values and close to the intensity values range of water points in all channels. Therefore, vegetation points (Area_LO) or built-up points (Area_TH) were misclassified as water points. The results of C3 based on intensity in Area_LO and Area_TH show higher completeness and correctness of water class than of C1 or C2. The reason of this is the low intensity values of recorded water bottom points, and hence they were correctly classified as water class.

In addition, the intermediate part of the water body was misclassified as land class because of the high intensity variation due to the fact that the intensity values were recorded at different scan angles and affected by the wavy effect. Therefore, this characteristic was used as the basis of ICOV, where higher ICOV values represent water points. However, water points in C3 exhibit low ICOV and were misclassified as land. As a result, the completeness of the water class from C1 and C2 is higher than from C3. Also, points at the water edges of Area_LO were misclassified as land points due to low intensity variation. Another reason for discrimination errors is some vegetation areas have high intensity variation, so they exhibited high ICOV and were misclassified as water points.

The NDWI results demonstrated that the NDWIs cannot be directly used for discrimination. This could be the variation of the transmitted signal power at different wavelengths. The discrimination results based on ID, demonstrated the highest completeness and correctness of water class in C3. The intermediate part of the water area was misclassified as land due to high intensity variation. In land area, points that have low intensity values (vegetation points for Area_LO and vegetation and asphalt surface points) were wrongly classified as land.

4.2.3. Land/Water Discrimination from Geometry-based Features

55

As mentioned earlier, the labeling of water points using geometry-based features depends on the returns from the water bottom. Consequently, the combination of C1/C2 and C3 is required. Figure 4.9 and 4.10 show the labeled LiDAR points for Area_LO and Area_TH, respectively, based on PD and MR, while the completeness, correctness and overall accuracy of the classified points are provided in Table 4.9 and 4.10. Confusion matrices of the results using PD and MR are illustrated in Appendix A.

(a)

(b)

Water Land

(c)

(d)

Figure 4.9. Labeled LiDAR points of Area_LO based on: PD using (a) C1 and C3; and (b) C2 and C3; and MR using (c) C1 and C3; and (d) C2 and C3

(a)

(b)

56

Water Land

(c)

(d)

Figure 4.10. Labeled LiDAR points of Area_TH based on: PD using (a) C1 and C3; and (b) C2 and C3; and MR using (c) C1 and C3; and (d) C2 and C3

Table 4.9. Accuracy measures of labeled point for Area_LO based on PD and MR. The best results are highlighted in bold. Completeness (%) Point Feature PD Channel (s) C1C3 C2C3 MR C1C3 C2C3 Land 27.3 44.1 97.7 98.3 Water 97.3 95.8 76.7 79.0 Correctness (%) Land 94.5 94.7 87.7 88.9 Water 44.0 50.1 95.2 96.5 Overall Accuracy (%) 53.2 63.2 90.0 91.2

Table 4.10. Accuracy measures of labeled point for Area_TH based on PD and MR. The best results are highlighted in bold. Completeness (%) Point Feature PD Channel (s) C1C3 C2C3 MR C1C3 C2C3 Land 44.4 43.3 96.6 97.0 Water 87.6 79.9 53.9 53.8 Correctness (%) Land 89.5 83.7 83.3 83.3 Water 39.8 37.2 87.0 88.2 Overall Accuracy (%) 57.2 54.1 84.0 84.2

The MR feature demonstrated promising results. However, this is attributed to the presence of bottom returns from C3. This is clearly shown in Area_LO where the overall accuracy is higher than Area_TH due to the fact that the number of returns from the water bottom in Area_LO was higher than it was in Area_TH. Despite the relatively high overall classification accuracies 57

obtained using MR, there are misclassified points in water bodies. This is mainly due to single returns from water surfaces or bottoms in C3 (single return), and hence the necessary condition for labeling water points was not achieved. The PD feature shows lower overall accuracy and most land points were misclassified as water. This is mainly because the land area reflecting more returns in C3 than in C1 and C2 because the land objects exhibited various characteristics at different wavelengths.

4.2.4. Land/Water Discrimination from Seeded Region Growing The point feature MR was used to find possible seed points by detecting points that have single returns in C1 or C2 (points_single) and by detecting the first of double returns in C3 (points_first_of_double), as shown in Figure 4.11a, 4.11b and 4.11c for Area_LO, respectively and Figure 4.12a, 4.12b and 4.12c for Area_TH. For each point in points_single, the nearest point from and within the circle footprint of points_first_of_double was found. The searching radius was set at 0.15 m for Area_LO and 0.16 for Area_TH, which was calculated by multiplying the altitude (430 m or 470 m) by half of the beam divergence of C3 (0.35 mrad).

The selected points may represent other land objects such as vegetation, which could have the same characteristics. The extracted points_single and points_first_of_double are shown in Figure 4.11d and 4.11e for Area_LO, and Figure 4.12d and 4.12e for Area_TH. Therefore, the Z was confirmed within a 10 m searching radius to be less than 0.5 m, which is necessary in order to preserve horizontal surfaces (i.e., water surface). Not only Figure 4.11f and 4.11g for Area_LO, but also Figure 4.12f and 4.12g for Area_TH show the output results that represent the water seed points. These points were used as an input for region growing, where the Z was checked point by point to be less than 0.5 m so that all water surface points could be labelled accurately. Figure 4.11i and 4.11j for Area_LO, and Figure 4.12i and 4.12j for Area_TH show the labeled LiDAR points using the combination of C3 with either C1 or C2 and Table 4.11 and 4.12 provide the completeness, correctness and overall accuracy for the two areas. Confusion matrices of the results using seeded region growing method are illustrated in Appendix A.

58

(a)

(b)

(c)

(d)

(e)

(f)

(g)

Water Land

(i)

(j)

Figure 4. 11. LiDAR points of Area_LO based on: (a) single returns in C1; (b) single returns in C2; (c) first of double returns in C3; (d) possible seed points using C1 and C3; (e) possible seed points using C2 and C3; (f) water seed points using C1 and C3; (g) water seed points using C2 and C3; (h) seeded region growing using C1 and C3; and (i) seeded region growing using C2 and C3

59

(a)

(b)

(c)

(d)

(e)

(f)

(g)

Water Land

(h)

(i)

Figure 4.12. LiDAR points of Area_TH based on: (a) single returns in C1; (b) single returns in C2; (c) first of double returns in C3; (d) possible seed points using C1 and C3; (e) possible seed points using C2 and C3; (f) water seed points using C1 and C3; (g) water seed points using C2 and C3; (h) seeded region growing using C1 and C3; and (i) seeded region growing using C2 and C3

60

Table 4.11. Accuracy measures of labeled point for Area_LO based on seeded region growing. The best results are highlighted in bold. Completeness (%) Channels C1C3 C2C3 Land 99.8 99.7 Water 97.2 97.6 Correctness (%) Land 99.5 99.5 Water 98.7 98.7 Overall Accuracy (%) 99.4 99.4

Table 4.12. Accuracy measures of labeled point for Area_TH based on seeded region growing. The best results are highlighted in bold. Completeness (%) Channels C1C3 C2C3 Land 99.9 99.9 Water 99.2 98.3 Correctness (%) Land 99.8 99.5 Water 99.6 99.6 Overall Accuracy (%) 99.7 99.5

Using either C1 or C2 together with C3 in this dissertation achieved a high overall accuracy of more than 99% when using the automatic seeded region growing algorithm. Höfle et al. (2009) achieved an overall accuracy of 97% for land/water discrimination using seeded region growing algorithm from single wavelength LiDAR data. However, this method requires a significant preprocessing step, which is the model of LiDAR data dropouts. This step cannot be conducted without the knowledge of the sensor position, which is not always available from the data supplier. Additionally, threshold values were manually selected to fit the tested data. Moreover, confusion between water surface and asphalt affected the discrimination results because they both exhibited low intensity and smooth surface.

The completeness and correctness of the water class is also high with more than 97.2%. The completeness of the water class in Area_LO was affected by the misclassification of a small pond at the edge of the dataset while the correctness was affected by the digitized land/water boundary. The correctness of the water class in Area_TH is very high with 99.6% as the land/water boundary is visible and can be easily identified. The completeness of the combination of C1 with C3 is higher than of C2 with C3 because the misclassified part of the water edge in the first combination is smaller than the misclassified part in the second combination. 61

This high accuracy rate shows the benefit of using multi-channel in such applications. However, this approach requires recorded returns at wavelengths in the green wavelength (i.e., 532 nm) from the surface and bottom of water bodies in order to fully automate the discrimination process. At water body edges, there are still a few misclassified points due to low point density or high variation in point elevation. Figure 4.13 and 4.14 summarize the overall accuracies obtained from all point features and the seeded region growing algorithm for Area_LO and Area_TH.

As aforementioned, the elevation and seed region growing algorithm have achieved the highest overall accuracy in Area_LO and Area_TH. The results from MR feature are promising; however it is restricted to have double returns from the water body at the green wavelength. Also, other features such as Z, R and PD require double returns from the water body at the green wavelength, but the results were affected by the recorded returns from land objects at different wavelengths. The intensity-based features are not stable and change according to the interaction of land and water with different wavelengths. Thus, intensity correction might maximize the use of intensity data.

Figure 4.13. Average overall accuracies for Area_LO based on different point features and seeded region growing 62

Figure 4.14. Average overall accuracies for Area_TH based on different point features and seeded region growing

4.2.5. Land/Water Boundary Delineation

Land/water discrimination based on elevation as well as on a seeded region growing algorithm achieved the highest overall accuracy of more than 99% with clear visualized land/water boundaries. The land/water boundary was delineated from region growing results produced by combining C1 with C3 (RG_C1C3_bound) and C2 with C3 (RG_C2C3_bound), as well as from elevation results of C1 (H_C1_bound), C2 (H_C2_bound) and C3 (H_C3_bound). The labeled points were converted into raster images with a pixel size of 1 m, and contour lines were automatically drawn using the embedded function in ArcGIS Contour list. The output contour lines were connected and compared with the digitized boundary from the reference images as shown in Figure 4.15. The RMSE was calculated from about ten sectors distributed perpendicular to the land/water boundary. The sectors at the edges were excluded from the evaluation due to biased boundaries as a result of discrimination errors. Table 4.13 provides the RMSE for Area_LO and Area_TH.

63

The average RMSE of Area_LO and Area_TH is 1.33 m and 0.69 m, respectively. This relatively high RMSE is due to the discrimination errors at land/water interface. Also, the reference images were not captured at the same time of the LiDAR data acquisition that the water level could be different and affected the digitized land/water boundary. Another possible reason is that the land/water boundary was manually digitized, so human error is possible. The Area_LO shows a higher RMSE than Area_TH because the land/water interface of Area_LO is not defined and changes dynamically due to the gentle slope and the wavy effect at this interface.

Figure 4.15. Land/water boundary delineation for Area_LO (left) and Area_TH (right)

Table 4.13. RMSE (m) of land/water boundary Area_LO Area_TH RG_C1C3 RG_C2C3 H_C1 1.36 1.27 1.23 64 0.58 0.48 0.65

H_C2 H_C3

1.44 1.35

0.56 1.16

4.2.6. Limitations of Land/Water Discrimination

The water body might be a lake, a coastal zone with gradually sloped terrain or a river in urban or mountain areas. Therefore, in this section, the limitations of the proposed methods for land/water discrimination are discussed. Water bodies in coastal areas usually occupy the lowest elevation in the scene so that an elevation threshold might be enough for discriminating water from land. However, the elevation threshold would not help in the discrimination process for elevated water areas. The MR feature is expected to achieve promising results for elevated water areas if and only if LiDAR returns from water surfaces and bottoms are recorded at the green wavelength. Other features such as Z, R and PD require double returns from the water body at the green wavelength as well, but the results were affected by the recorded returns from land objects at different wavelengths. The results of intensity-based features change according to the interaction of land objects and water bodies with different wavelengths. Thus, intensity correction and/or normalization might maximize the usefulness of the intensity data.

The automation of seeded region growing method is also restricted by the presence of double returns from the water body at the green wavelength. Thus, this condition might not be achieved in case of some water areas as it is attributed to the water clarity and the maximum water depth that could be detected by the sensor. In general, a combination of the presented point features could have a better performance in discriminating water bodies with different characteristics.

65

CHAPTER 5

LAND COVER CLASSIFICATION

This chapter presents the second main objective of this dissertation research of developing an approach for land cover classification from multispectral airborne LiDAR data. A point-based classification approach is described in order to assign each LiDAR point a unique label. The methodology of this part is explained and followed by a presentation of the results and analysis.

5.1. Methodology

5.1.1. Overall Classification Scheme

The LiDAR point clouds collected at the three channels were first merged as a pre-processing step, which is described in Section 5.1.2., followed by a three-level classification approach. First, the non-ground points are separated from the ground points using the ground filtering method. Second, the non-ground and ground points are clustered into buildings or trees and roads or grass. The roads class includes asphalt surface, parking lots, bare soil and sidewalks. Third, more classes are labeled based on spectral characteristics acquired at the three wavelengths. The trees class is separated into trees with green leaves and trees with red leaves. Power lines and swimming pools are labeled as well. The overall workflow of the classification process is shown in Figure 5.1.

5.1.2. Multi-wavelength LiDAR Points Merging

As the new multispectral sensor acquires LiDAR data at different wavelengths, point clouds are collected for the same coverage area but with different intensity values produced by different wavelengths. However, merging those point clouds and predicting the intensity values for each single point at all wavelengths makes the available data denser and more reliable. Although Optech Titan operates simultaneously at the three wavelengths, it acquires LiDAR points in the

66

three channels at different angles. Consequently, collected points from the same object in different channels may not coincide completely at the same location.

LiDAR Point Clouds C1, C2 and C3

Points Merging LiDAR Point Clouds (X, Y, Z, IC1, IC2, IC3)

Spectral Characteristics

Ground Filtering

Non-Ground Points

Ground Points

More Classified Objects

NDVIs Computation & Histograms Construction Clustering Non-Ground NDVIs Histograms Ground NDVIs Histograms

Multivariate Gaussian Decomposition (MVGD) Classified NonGround Points Classified Ground Points

Points Combination Classified 3D Points

Ortho-image

Accuracy Assessment

Figure 5.1. Classification workflow

67

A 3D spatial joining technique could provide a possible solution for merging points from all channels, where an intensity value of a point from one channel is assigned to the nearest point from another channel (Wichmann et al., 2015). However, this technique might lead to incorrect matching between points as shown in Figure 5.2 and as explained in the following scenarios. Case (1) indicates the perfect point matching from channels C2 and C3. In case (2), a point from C2 could be matched twice with two different points from C3, as this point is the nearest neighbour to both points. Case (3) shows two possible neighbouring points from C2 which have the same distance to a point from C3. Case (4) indicates that no neighbouring points from C2 to a point from C3 exist within a sphere of predefined radius. Therefore, the intensity values of each point cannot be treated as if they were the same as the intensity value of the nearest point.

4 3 3D Spatial Join 2

C1 C2

C3

C1 C2

1
C3

Figure 5.2. 3D spatial joining between points from C2 and C3

In order to correctly predict intensity, a median value of a point was calculated from its neighbouring points from another channel. The median intensity value was used to avoid any intensity data noise. This was done as follows: Let pi, pj, and ph represent points in C1, C2 and C3, respectively; where i=1, 2, 3, . . , nC1; j=1, 2, 3, . . , nC2; h=1, 2, 3, . . , nC3; and nC1, nC2 and nC3 are the total number of LiDAR points collected in C1, C2 and C3, respectively. The LiDAR points in each channel were first organized using a K-d tree in order to efficiently apply a multidimensional range search. A K-d tree is a data structure used to organize spatial data in a space with k dimensions (3D in this case) (Bentley, 1975). It is a binary search in which each node is used to partition one of the dimensions. Each level of a K-d tree splits all points along a specific dimension into two pieces (subtree) using a hyperplane that is perpendicular to the corresponding axis. Each level down in the tree divides the points on the next dimension until all

68

points are grouped. The neighbouring points

and

of pi derived from C2 and C3,

respectively, were obtained within a sphere of predefined search radius r as follows:

{(

) (

)

(

)

(

)

}

(5-1)

{(

) (

)

(

)

(

)

}

(5-2)

The r value was set at 1 m in order to fulfill two conditions: first, to have a sufficient number of points, second to avoid including any points from different land objects. The and

points were then arranged in ascending order according to their intensity values. The intensity values and of pi derived from C2 and C3, respectively were calculated as follows:

( ( { )

) (5-3) ( )

( ( { )

) (5-4) ( )

In case of no neighbouring points had not been found, the intensity value was assigned a zero value. Equations (5-1) to (5-4) were applied at any point pj in C2 to obtain the neighbouring points and as well as the intensity values and were derived from C1 and C3,

respectively. The same procedures were applied for any point ph in C3, where the neighbouring points and as well as the intensity values and were obtained from C1 and C2,

69

respectively. Therefore, the total number of points N=nC1+nC2+nC3, and each LiDAR point have six attributes: x, y, z, IC1, IC2 and IC3.

5.1.3. Ground Filtering

The ground filtering aims to separate non-ground points from ground points through the decision rules shown in Figure 5.3. Skewness balancing, a statistical analysis algorithm, was applied to the elevation of points as a first step for ground filtering. The naturally measured data lead to a normal distribution (Duda et al., 2001). Thus, the ground points collected within the LiDAR data are assumed to follow the normal distribution, while the other non-ground (object) points may disturb the distribution (Bartels et al., 2006; Morsy et al., 2017b). By removing those non-ground points from the LiDAR data, the ground points were obtained. The higher order moments (e.g., skewness) can characterize the distribution of LiDAR points. The skewness (Sk) is defined by:

(

)

(5-5)

where N is the total number of the LiDAR points, Zi is the elevation and i  {1, 2, . . .,N},  and  are the standard deviation and the arithmetic mean of elevation, defined by Equations (5-6) and (5-7) respectively:



(

)

(5-6)



(5-7)

70

LiDAR Point Clouds (X, Y, Z, IC1, IC2, IC3)

Skewness Balancing

Non-Ground Points

Ground Points

Points' Slope Calculation

Yes

Is the slope> S_thrd?

No Moving Circle & find Z_min

Yes

Is the elevation> Z_min+H_thrd?

No Ground Points

Figure 5.3. Ground filtering workflow

The elevations of the point clouds were first sorted in ascending order. The skewness was then calculated from all points using Equation (5-5). If the skewness was greater than zero, the point with the highest elevation was removed and classified as a non-ground point. The remaining points were used to calculate the skewness and the process was repeated until the skewness of the point clouds was balanced (Sk=0). After the skewness balancing was performed, the remaining points were classified as potential ground points and assumed to be within a specific 71

slope. As such, the output separation was refined based on the measurement of slope changes of each LiDAR point with respect to its neighbouring points. A threshold value (S_thrd) was applied to label points with higher slopes as non-ground points. The S_thrd was identified based on the slope of the road surface, so that all road surface points were labeled as ground points. In addition, a few points with higher elevations were not labeled as non-ground points. Therefore, the remaining points were filtered out using a moving circle with a radius of r and a threshold value on elevation difference (H_thrd). Point that were filtered out and the remaining points were labeled as non-ground and ground points, respectively.

5.1.4. NDVIs Computation and Histograms Construction For non-ground and ground points, the three intensity values IC1, IC2 and IC3 were employed to form three NDVIs, namely NDVIC2-C1, NDVIC2-C3 and NDVIC1-C3, as defined in Morsy et al. (2016a). This was done as follows:

(5-8)

(5-9)

(5-10)

The NDVIs values range from -1 to 1. However, if a point has zero intensity value in two channels, the NDVI will not be a number (NaN). In this case, the point is labeled as an unclassified point. The NDVI values were then divided into bins (or intervals) of 0.1 each and counted the number of points (i.e., frequency) in each bin. Next, the NDVIs' histograms were constructed. Figure 5.4 shows examples of NDVIs' histograms.

72

Figure 5.4. Examples of NDVIs' histograms constructed from non-ground points (left) and ground points (right)

5.1.5. Multivariate Gaussian Decomposition (MVGD)

Gaussian decomposition uses the Gaussian function, provided by Equation (5-11), to model the full waveform that have more than one peak (Wagner et al., 2006) . A Gaussian decomposition, also known as Gaussian mixture model, is a probabilistic model that is used to decompose the data points into number of Gaussian distributions with unknown parameters. The histograms, constructed from the NDVIs of the discrete returns LiDAR data, were modelled using Gaussian decomposition. A sum of Gaussian distributions G(x), described by Persson et al. (2005), was used to fit each NDVI histogram as presented in Equation (5-12).

( ) 

(

(

)

)

(5-11)

Where, : the mean of Gaussian (j) : the standard deviation of Gaussian (j)

( )



( )

(5-12)

73

Where, x : the bin value N : the number of Gaussian components : relative weight of Gaussian (j)

Two main challenges were identified related to the fitting of the modeling function (i.e., Gaussian). First, prior knowledge of the number of Gaussian components is required. Second, the fitting is sensitive to the initial values of Gaussian's parameters. Therefore, the peak detection algorithm was used to detect all histograms' peaks (K) and their locations (i.e., the mean values: ) (Jutzi and Stilla, 2006). Based on the fact that the single Gaussian has two

inflection points, the zero crossing of the second derivative was used to obtain the positions of the inflection points of each Gaussian component, and hence the Gaussian's half width ( ) was calculated (Hofton et al., 2000).

To be able to model the histograms, the number of Gaussian components was considered equal to the number of peaks (N=K). So that for each Gaussian component (j), the , and were

fitted using the maximum likelihood estimate produced by the Expectation ­ Maximization (EM) algorithm (Dempster et al., 1977). The expectation (E) step computes the probability (wij) that each data bin (xi) belongs to Gaussian (j), starting with that the two Gaussians has equal relative weight ( ), using the following equation: ( ) ( )



(5-13)

The maximization (M) step computes the maximum likelihood estimates of the parameters (pj, µj and j) as follows:  

(5-14)

74

 

(5-15)

 

( 

)

(5-16)

Where : the amplitude at bin xi n : the number of histogram's bins

The two steps were repeated until convergence or a maximum number of iteration was achieved. The process stopped when either (1) the difference between any iteration and the previous iteration was less than 0.001, or (2) the number of iterations was greater than 1000 (Oliver et al., 1996). The same procedures were repeated for all possible number of Gaussians, where N = K-1, K-2, ..., 2. In addition, a non-linear least squares (NLS) adjustment was used to optimize each component's parameters ( and ) (Hofton et al., 2000; Wagner et al., 2006) so that they could

be compared with the fitted Gaussians from EM.

The quality of the fitted Gaussians has been tested for fitting the full waveform LiDAR data (Hofton et al., 2000; Chauve et al., 2007) using the following formula:

 (

( ))

(5-17)

Similarly, the quality of the Gaussians fitted with either EM or NLS was tested using the aforementioned equation. According to Chauve et al. (2007), a histogram was considered to be well fitted if 1,...,1, the was less than 0.5. However, in this dissertation, in each case where N = K, Kwas calculated and the number of Gaussian components were eventually used when

was minimal. Finally, a multivariate Gaussian decomposition (MVGD) was applied to cluster the LiDAR data into a number of classes using the following Equation (5-18). 75

( )

| |

(

(

)

(

))

(5-18)

m: number of variables (NDVIC2-C1, NDVIC2-C3, and NDVIC1-C3) X: variables matrix [NDVIC2-C1 NDVIC2-C3 NDVIC1-C3]T M: means vector : covariance matrix

5.1.6. LiDAR Points Classification 

Point Labeling

Eight classes were labeled from the airborne multispectral LiDAR data, namely buildings, trees with green leaves (green trees), trees with red leaves (red trees), roads, grass, swimming pools, power lines and unclassified points. Vegetation (i.e., trees or grass) exhibits different reflectance values in the three channels, whereas reflectance from C2 > C1 > C3, as shown in Figure 3.1 and mentioned in Morsy et al. (2017b). As a result, the calculated NDVIs of the vegetation points exhibited higher values than the built-up areas (i.e., buildings or roads). Thus, the output cluster classes from MVGD that have high NDVIs were labeled vegetation and those with low NDVIs were labeled built-up areas.

The trees class was separated into two classes including trees with green leaves (green trees) and trees with red leaves (red trees). The red trees had no reflectance in C3 (green channel) as shown in Figure 5.5 (upper). As a result, for a point belonging to red trees, IC3 = 0, and hence the NDVIC2-C3 = NDVIC1-C3 = 1. Power lines had reflectance mainly in C1 (1550 nm) as shown in Figure 5.5 (lower), which is sensitive to the high temperature emitted from those lines. That means IC2 = IC3 = 0 for power lines points, and hence NDVIC2-C1 = ­ 1 and NDVIC1-C3 = 1. Swimming pools points were labeled based on the fact that their surfaces have returns in C1 and C2, and returns from the swimming pools' surfaces and/or beds in C3. As aforementioned, if a point had zero intensity value in any two channels, this point was labeled as an unclassified point unless it was a power lines point. 76

Figure 5.5. Recorded points of two types of trees (upper) and power lines (lower) in C1, C2 and C3, respectively 

Spatial Coherence

The proposed point-based classification approach labels each individual point according to its spectral characteristics and its position in the feature space. In general, point-based classification methods are full of salt and pepper noises, as they do not account for spatial coherence of neighbourhoods. As a result, the derived labeling was expected to be noisy. Therefore, the spatial coherence was considered by applying a max voting 3D filter similarly to the one presented by Charaniya et al. (2004). This filter assigns each individual point to a class that occurs most frequently in its neighbourhood within a circle of 3 m radius. This filter provided smooth classification results as shown in Figure 5.6.

Figure 5.6. Classified area before (left) and after (right) the application of the 3D filter

77

5.1.7. Evaluation of Classification Results

Previous studies used 3D labelled LiDAR data benchmarks (Blomley et al., 2016; Vosselman et al., 2017) or 2D labelled LiDAR data benchmarks that were extended to 3D labelled points (Niemeyer et al., 2014), which were not available for the tested study area. Also, the 3D reference points could be provided by the supplier or manually labeled (Xu et al., 2014); however this is time consuming and might introduce errors from the human operator or analyst.

This dissertation research presents an alternative method for generating 3D reference points based on polygons digitized from aerial images. First, a set of polygons for each class (e.g., buildings) was randomly selected and digitized from aerial images. Second, all 3D points within each polygon were labeled as reference points for their class. This method is applicable for buildings, roads, grass and swimming pools classes. For the trees class, only the canopy points were labeled as reference points. However, other classes (e.g., power lines) could not be quantitatively assessed as they are vertically distributed in the LiDAR data and appeared as linear objects in the aerial images (see Figure 5.7); moreover, reference polygons cannot be digitized. The confusion matrix for each classified data subset was then created and the accuracy measures (overall, producer's and user's accuracies) were calculated.

Figure 5.7. The power lines display in an aerial image and the LiDAR data

78

5.2. Results and Analysis

The data were received as a 3D point clouds file in LAS format for each channel. The LAS data files contain xyz coordinates, raw intensity values, scan angle, return number, number of returns and the GPS time of each LiDAR point. In preparation for processing, the LAS data files were converted into ASCII files using lastools. MATLAB codes were written for LiDAR point merging, the ground filtering method, NDVIs calculations, NDVIs histograms construction, Gaussian decomposition using EM and NLS, MVGD, the 3D max voting filter and for labeling the LiDAR points. The labeled points were exported as ASCII files, which were converted into Geodatabase to be processed for accuracy assessment using ArcMap and visualized using ArcScene.

After the LiDAR points from the three Titan channels were combined, a ground filtering method based on the skewness balancing and the points' slopes with respect to neighbouring points was applied. Thus, if the point's slope was greater than a threshold value (S_thrd = 10), the point was labeled as a non-ground point. The consideration of the points' slopes made the ground filtering method applicable for not only flat areas but also for sloped terrains. The output ground points were filtered using a moving circle with a radius of (r = 10 m), so if the height difference was greater than a threshold value (H_thrd = 1 m), the point was labeled as a non-ground point and the remaining points were labeled as ground points. Results of ground filtering for the three tested areas are illustrated in Appendix B. The three NDVIs were subsequently calculated using Equations (5-8) to (5-10). NDVIs' histograms were then constructed and normalized for non-ground and ground points. A sum of Gaussian distributions was used to model the histograms using EM and NLS algorithms and eventually a fitting with minimum was used. The fitted Gaussian results are shown in Figure

5.8, 5.9 and 5.10 for Area 1, Area 2 and Area 3, respectively. The number of Gaussians, the optimization algorithm and the final of the fitted Gaussians are provided in Table 5.1.

79

NDVIC2-C1

NDVIC2-C3

NDVIC1-C3

(a)

(b)

(c)

(d)

(e)

(f)

Figure 5.8. Gaussian decomposition of NDVIs' histograms of Area 1 (upper: non-ground and lower: ground)

NDVIC2-C1

NDVIC2-C3

NDVIC1-C3

(a)

(b)

(c)

80

(d)

(e)

(f)

Figure 5.9. Gaussian decomposition of NDVIs' histograms of Area 2 (upper: non-ground and lower: ground)

NDVIC2-C1

NDVIC2-C3

NDVIC1-C3

(a)

(b)

(c)

(d)

(e)

(f)

Figure 5.10. Gaussian decomposition of NDVIs' histograms of Area 3 (upper: non-ground and lower: ground)

81

Table 5.1. The number of components/optimization method/

of the fitted Gaussians and

number of clusters obtained from MVGD for the three Areas NDVIC2-C1 Area 1 Non-ground Ground Area 2 Non-ground Ground Area 3 Non-ground Ground 1/NLS/0.051 2/EM/0.033 1/NLS/0.079 2/EM/0.032 1/NLS/0.057 NDVIC2-C3 2/EM/0.048 NDVIC1-C3 2/EM/0.042 MVGD 4 4 2 4 4 2

2/NLS/0.040 1/NLS/0.082 2/EM/0.062 2/EM/0.097 2/EM/0.017 1/NLS/0.065 1/NLS/0.092 2/EM/0.030

1/NLS/0.026 1/NLS/0.008 1/NLS/0.038

As shown in Table 5.1, the max

is less than 0.1 which is lower than the prescribed error level and ) were

(i.e., 0.5) by Chauve et al. (2007). The fitted Gaussians with their parameters (

used as input for the multivariate Gaussian function, Equation (5-18), in order to cluster nonground/ground points into four classes. For the three defined NDVIs of the non-ground or ground points, the lower values represent buildings or roads, and the higher values represent trees or grass (Morsy et al., 2017b). Based on these facts and based on a visual interpretation of the output clusters, the following decisions were taken:   If the output produced two clusters, the first cluster was labeled as buildings or roads, and the second cluster was labeled as trees or grass. If the output produced four clusters, the first two clusters were labeled as buildings or roads, and the last two clusters were labeled as trees or grass.

Therefore the MVGD divided the multispectral airborne LiDAR data into four main classes: buildings, trees, roads and grass. The number of classes could be increased if more Gaussian components are considered and a higher error level of output classes might be noisy and not representative. is allowed; and however, in this case the

Therefore, more classes were labeled using LiDAR data of land objects produced by three different wavelengths as described in Section 5.1.6.1. First, the trees class was separated into two classes: green trees and red trees. The power lines were labeled in Area 1 and Area 2, while Area 3 does not include power lines. In addition, the swimming pools and unclassified points 82

were labeled. Finally, a total of eight classes were labeled for Area 1 and Area 2 including buildings, green trees, red trees, roads, grass, power lines swimming pools, and unclassified objects. Area 3 was labeled with the same classes, excluding the power lines class. Figure 5.11, 5.12 and 5.13 show the classified point clouds of Area 1, Area 2 and Area 3 respectively. The confusion matrices and overall accuracies are provided in Table 5.2, 5.3 and 5.4.

Figure 5.11. Classified LiDAR points of Area 1

Figure 5.12. Classified LiDAR points of Area 2 83

Figure 5.13. Classified LiDAR points of Area 3

Table 5.2. Confusion matrix of Area 1 Reference data Classification data Unclassified Buildings G_Trees R_Trees Roads Grass SP Total column Producer's Acc. (%) Buildings G_Trees R_Trees 4 10293 78 22 0 0 1 10398 99.0 160 566 5195 1214 0 0 5 7140 72.8 50 1 33 2422 0 0 0 2506 96.6 Roads 56 20 0 69 3774 1 158 4078 92.5 Grass 44 25 89 15 47 8527 45 8792 97.0 SP 42 16 6 0 0 0 474 538 88.1 Total row 356 10921 5401 3742 3821 8528 683 33452 94.2 96.2 64.7 98.8 99.9 69.4 User's Acc. (%)

Overall accuracy: 91.7%

Table 5.3. Confusion matrix of Area 2 Reference data Classification data Buildings G_Trees R_Trees 84 Roads Grass SP Total row User's Acc. (%)

Unclassified Buildings G_Trees R_Trees Roads Grass SP Total column Producer's Acc. (%)

0 10145 84 5 0 0 0 10234 99.1

193 413 5879 965 0 0 3 7453 78.9

95 0 108 4269 0 0 0 4472 95.5

3 0 0 0 5719 11 1 5734 99.7

61 24 281 271 65 8311 0 9013 92.2

67 20 3 0 0 0 1199 1289 93.0

419 10602 6355 5510 5784 8322 1203 38195 95.7 92.5 77.5 98.9 99.9 99.7

Overall accuracy: 93.0%

Table 5.4. Confusion matrix of Area 3 Reference data Classification data Unclassified Buildings G_Trees R_Trees Roads Grass SP Total column Producer's Acc. (%) Buildings G_Trees R_Trees 0 7259 138 1 0 0 0 7398 98.1 1093 2348 23597 6598 0 0 2 33638 70.1 33 0 92 1448 0 0 0 1573 92.1 Roads 157 0 0 0 2867 0 0 3024 94.8 Grass 43 0 2 0 194 36977 0 37216 99.4 SP 48 3 0 0 0 0 336 387 86.8 Total row 1374 9610 23829 8047 3061 36977 338 83236 75.5 99.0 18.0 93.7 100.0 99.4 User's Acc. (%)

Overall accuracy: 87.1%

The presented approach achieved an overall accuracy of 91.7%, 93.0% and 87.1% for Area 1, Area 2 and Area 3, respectively. There are two sources of classification errors, including the unclassified class and ground filtering. During the intensity prediction and within the searching radius of any points, if no neighboring points were found, the intensity values were set to zero. As a result, the point's NDVI was not a number and labeled as unclassified point. The error of unclassified class ranges from 1.1% to 1.7%. 85

The classification errors due to the ground filtering range from 0.0% to 3.7%. These errors are highlighted with a light blue color in the confusion matrices tables, whereas the ground points (roads, grass or swimming pools) were misclassified as non-ground points (buildings or trees) or vice versa. Some points were wrongly filtered out as non-ground points and classified as buildings. These points could belong to curbs or bare soil and classified as buildings due to their low intensity values. Other points were wrongly filtered out as non-ground points and classified as green trees or red trees. For points that were classified as green trees could belong to shrubs with green leaves due to their high intensity values. For points that were classified as red trees could belong to shrubs with red leaves due to they do not have returns in C3. Also, some swimming pools are on a higher elevation than the ground points, so they were filtered out and wrongly classified as buildings or green trees.

Another main source of classification errors is the confusion between green trees and red trees in the three tested areas. This is primarily due to the fact that intensity values along the vertical distribution of the tree profile are not the same as they represent different returns. Also, for some points of green trees, there were not neighbouring points in C3 to estimate their intensity values. Therefore, the intensity values were set to zero and those points were misclassified as red trees. For example in Area 2, about 21.1% of green trees points were omitted (78.9% producer accuracy), and those omitted points were wrongly classified as red trees, buildings or unclassified objects. This omission caused a classification error rate of red trees class of about 17.5% and of buildings of about 3.9%. As vegetation cover increased, the error rate increased. About 27.2% (72.8% producer accuracy) and 29.9% (70.1% producer accuracy) of green trees points were omitted in Area 1 and Area 3, respectively. As a result, the classification error rate of red trees class and buildings increased to 32.4% and 5.2% in Area 1 and to 82.0% and 24.4% in Area 3.

In some cases, the tree points were wrongly classified as buildings or vice versa because the various moisture content of the vegetation (Gao, 1996) and various roof materials exhibit different intensity values leading to classification errors (Morsy et al., 2017b). For instance, dark roof materials (dark red or dark gray colors) which have intensity values in C3 lower than C1 or C2 were wrongly classified as trees. Also, when the moisture content of green trees decreases, 86

the intensity values of those trees in C1 are higher than in C2, and hence they were wrongly classified as buildings. Classification errors also occurred due to confusion between roads and grass classes. This is because road markings, which are painted on road surfaces, have high intensity values and misclassified as grass. Also, bare soil in grass patches have low intensity values and misclassified as roads. Therefore, classification of more classes including bare soil and road markings improves the classification results accuracy.

Power lines points were visually assessed and most of these points were correctly labeled in Area 2 but not in Area 1. The misclassified points were labeled as trees or unclassified objects. This is due to those points had returns from two or the three channels (labeled as trees) or returns in one channel, either: C2 or C3, so they were labeled as unclassified objects, as shown in Figure 5.5 (lower). One obvious object in Area 1 and Area 2 is a group of cars, which was classified as power lines, red trees, swimming pools or unclassified objects. This is mainly due to possibility that they could have reflection in one or more channels because they have different colors, and hence the conditions of labeling the aforementioned classes were satisfied.

5.2.1. The Impact of Spatial Coherence

To improve the classification results, a 3D max voting (mode) filter with a circle of 3 m radius was applied to the classification output classes. The overall accuracies increased from 91.7% to 95.7%, 93.0% to 98.3% and 87.1% to 95.3% for Area 1, Area 2 and Area 3, respectively. Figure 5.14, 5.15 and 5.16 show the classified point clouds of the three areas after the application of the 3D filter. Significant improvements of the producer's and user's accuracies were achieved for the individual land covers as shown in Figure 5.17, 5.18 and 5.19. For instance, the user's accuracy in classifying buildings, red trees and swimming pools improved by up to 22.7%, 13.9% and 13.3%, respectively. Also, the producer's accuracy improved by 10.2%­18.6% and by 4.5%­13.3% for green trees and swimming pools, respectively. Confusion matrices of the results after the application of the 3D filter are illustrated in Appendix B.

87

Figure 5.14. Classified LiDAR points of Area 1 after mode filter

Figure 5.15. Classified LiDAR points of Area 2 after mode filter

88

Figure 5.16. Classified LiDAR points of Area 3 after mode filter

Buildings
MVGD 100 90 80 70 60 50 40 30 20 10 0 Uder's Acc. Producer's Acc. MVGD_filtered 100 90 80 70 60 50 40 30 20 10 0

Green_Trees
MVGD MVGD_filtered 100 90 80 70 60 50 40 30 20 10 0 Uder's Acc. Producer's Acc.

Red_Trees
MVGD MVGD_filtered

Uder's Acc.

Producer's Acc.

Roads
MVGD 100 90 80 70 60 50 40 30 20 10 0 Uder's Acc. Producer's Acc. MVGD_filtered 100 90 80 70 60 50 40 30 20 10 0 MVGD

Grass
MVGD_filtered 100 90 80 70 60 50 40 30 20 10 0 Uder's Acc. Producer's Acc.

Swimming Pools
MVGD MVGD_filtered

Uder's Acc.

Producer's Acc.

Figure 5.17. Producer's and user's accuracies for MVGD and MVGD_filtered of Area 1

89

Buildings
MVGD 100 90 80 70 60 50 40 30 20 10 0 User's Acc. Producer's Acc. MVGD_filtered 100 90 80 70 60 50 40 30 20 10 0

Green_Trees
MVGD MVGD_filtered 100 90 80 70 60 50 40 30 20 10 0 User's Acc. Producer's Acc.

Red_Trees
MVGD MVGD_filtered

User's Acc.

Producer's Acc.

Roads
MVGD 100 90 80 70 60 50 40 30 20 10 0 User's Acc. Producer's Acc. MVGD_filtered 100 90 80 70 60 50 40 30 20 10 0 MVGD

Grass
MVGD_filtered 100 90 80 70 60 50 40 30 20 10 0 User's Acc. Producer's Acc.

Swimming Pools
MVGD MVGD_filtered

User's Acc.

Producer's Acc.

Figure 5.18. Producer's and user's accuracies for MVGD and MVGD_filtered of Area 2

Buildings
MVGD 100 90 80 70 60 50 40 30 20 10 0 User's Acc. Producer's Acc. MVGD_filtered 100 90 80 70 60 50 40 30 20 10 0

Green_Trees
MVGD MVGD_filtered 100 90 80 70 60 50 40 30 20 10 0 User's Acc. Producer's Acc.

Red_Trees
MVGD MVGD_filtered

User's Acc.

Producer's Acc.

90

Roads
MVGD 100 90 80 70 60 50 40 30 20 10 0 User's Acc. Producer's Acc. MVGD_filtered 100 90 80 70 60 50 40 30 20 10 0 MVGD

Grass
MVGD_filtered 100 90 80 70 60 50 40 30 20 10 0 User's Acc. Producer's Acc.

Swimming Pools
MVGD MVGD_filtered

User's Acc.

Producer's Acc.

Figure 5.19. Producer's and user's accuracies for MVGD and MVGD_filtered of Area 3

5.2.2. Comparison with Previous Studies

The classification results have demonstrated the usefulness of multispectral LiDAR data for land cover classification. Previous studies combined multispectral aerial/satellite imagery with either LiDAR height data (Chen et al., 2009) or with both LiDAR height and intensity data (Hartfield et al., 2011; Singh et al., 2012) in order to classify the terrain into multi-class. Those studies achieved overall classification accuracies ranging from 85.4% to 89.9%. The approach presented in this dissertation, which used LiDAR data only, achieved higher overall accuracies of 95.7%, 98.3% and 95.3% for Area 1, Area 2 and Area 3, respectively. Thus, the availability of the multispectral LiDAR data reduces the need for multispectral aerial/satellite imagery for classification purposes. In contrast, by focusing on classifying LiDAR points into multi-class (seven classes) in urban areas, previous studies achieved overall classification accuracies of 89.1% (Xu et al., 2014), 83.4% (Niemeyer et al., 2014) or 82.8% (Vosselman et al., 2017). The classification results in this dissertation demonstrated higher overall accuracies compared to the aforementioned studies.

The output classes were combined into four classes (buildings, trees, roads and grass) in addition to the unclassified objects to be used for another comparison with previous studies. The results revealed overall accuracies of more than 95% for the three tested areas and this improved to more than 99% after the application of the 3D filter. In contrast, the results of studies that combined multispectral aerial/satellite imagery with LiDAR height data (Charaniya et al., 2004; 91

Huang et al., 2008) for obtaining the same four classes, demonstrated overall classification accuracies of 84.0% and 93.9%, respectively. Other investigations used LiDAR data only for classification (Chehata et al., 2009) and achieved an overall accuracy of 94.4%.

For quantitative comparison, the same clustering method presented in Chapter 4, Section 4.1.2 was applied. The Jenks natural breaks optimization method was used to cluster the three NDVIs (NDVIC2-C1, NDVIC2-C3 and NDVIC1-C3) histograms of the non-ground and ground points into buildings or trees and roads or grass. Figure 5.20 shows the overall accuracies obtained from the three NDVIs based on the Jenks break optimization, MVGD, and MVGD after applying the 3D filter. Using MVGD achieved overall accuracies similar to or higher than using NDVIs based on Jenks breaks for all the tested areas. Except in Area 3, the NDVIC2-C3 demonstrated a 1.0% higher overall accuracy than that provided by MVGD. However, the application of the 3D filter obtained the highest overall accuracies for the three tested areas. Figures and confusion matrices of the results for the five cases of the three tested areas are illustrated in Appendix B.

Overall Accuracy (%)
NDVI (C2-C1) NDVI (C2-C3) NDVI (C1-C3) MVGD MVGD_filtered

100 90 80 70 60 50 40 30 20 10 0 Area 1 Area 2 Area 3

Figure 5.20. Overall accuracies comparison

92

CHAPTER 6

SUMMARY, CONCLUSIONS AND RECOMMENDATIONS

6.1. Summary and Conclusions

This dissertation research presents a full scheme for multispectral LiDAR data classification, including land/water discrimination in coastal zones and land cover classification of urban areas. The multispectral LiDAR data used in this research were collected by the world 's first operational multispectral airborne LiDAR system, Optech Titan. This system acquires point clouds in three channels at wavelengths of 1550 nm, 1064 nm and 532 nm. The diversity of collected spectral information increases the benefits that can be obtained from the LiDAR data.

6.1.1. Land/Water Discrimination

In this approach, a set of distinctive LiDAR point features was extracted and evaluated for land/water discrimination. Those features were divided into three categories. The first category is elevation-based features, including elevation, elevation difference (Z) and roughness (R). The second category is intensity-based features, including intensity, normalized difference water index (NDWI), intensity coefficient of variation (ICOV) and intensity density (ID). The third category is geometry-based features including point density (PD) and multiple returns (MR). In addition, the limitations of land/water discrimination using the aforementioned features were discussed.

In order to label water and land points, water points were assumed to have the lowest elevation and intensity in the scene while water bodies were assumed to have an intensity variation higher than land areas. Also, returns from water surfaces were only recorded at the MIR or NIR wavelengths, while returns from water surfaces and bottoms were recorded at the green wavelength. The water points were labeled by testing the elevation and intensity attributes of each single point, while the other features were extracted based on a local neighbourhood of each point. Single or double channels were considered for extracting those point features. The 93

elevation, intensity, ICOV and ID features were extracted from each single channel (C1, C2 or C3), while the three NDWI, Z, R, PD and MR features were extracted using the combined data from two channels. Threshold values for labeling water and land points were automatically identified using an elevation histogram and peak detection algorithm for elevation, the Jenks break optimization method for intensity, NDWI, ICOV and ID, and reasoning for Z, R, PD and MR.

An unsupervised land/water discrimination method based on a seeded region growing algorithm was also presented. This method includes two main phases, namely seed points selection and region growing criteria. The seed points selection is a fully automated process using the MR point feature, although this was usually a manual step in previous studies. An elevation difference was used as growing criterion for each seed point in order to label all water points in a scene. The contributions of this dissertation to land/water discrimination in coastal zones are as follows: 1. Predefined LiDAR point features, such as elevation, intensity, ICOV and ID were extracted and evaluated from three different channels. 2. Two point features were used for first time from multispectral LiDAR data, including NDWI and MR features. 3. New definitions for discriminating water from land were presented for Z, R, and PD features. 4. The threshold for elevation was automatically selected by constructing an elevation histogram and a peak detection algorithm. 5. The threshold values for discriminating water from land were automatically selected based on the Jenks break optimization algorithm. 6. The seed points in the seeded region growing algorithm were automatically selected based on the MR feature. 7. Two alternatives were presented for discriminating water from land in coastal zones: one based on elevation and the other uses a seeded region growing algorithm.

In the evaluation, two coastal zones were used: loose and stoned land/water interface at Lake Ontario and sharply distinguished land/water interface at Tobermory Harbour (both are located 94

in Ontario, Canada). The interaction of the water bodies at the MIR and NIR wavelengths is almost identical, whenever the water surface is detected. The green wavelength can penetrate the water surface and detect both the water surface and bottom. Land/water discrimination based on elevation and seeded region growing demonstrated the highest overall accuracy of over 99%. The Z, R, PD and MR results revealed a higher average overall accuracy of 11.7%, 10.1%, 2.5% and 16.5%, respectively at Lake Ontario than at Tobermory Harbour. This difference is mainly because those point features relied on returns from the water bottom. Lake Ontario's bottom has more returns than Tobermory Harbour's bottom because the water area at Tobermory Harbour's is deeper than Lake Ontario's.

The land/water discrimination using intensity-based features revealed inconsistent performance for the two tested areas due to different interaction of land objects and water bodies with the three wavelengths. The intensity-based features achieved a relatively low average overall accuracy of 59.0%­63.4%. This is due to the water points have the same intensity range as land objects. Thus, discrimination results based on intensity, ID and NDWI were affected. Also, the high intensity variation of some land objects reduced the performance of ICOV in land/water discrimination. The NDWI, which is commonly used for land/water discrimination from satellite imagery, cannot be used directly from airborne multispectral LiDAR data for land/water discrimination.

High overall discrimination accuracy was achieved based on elevation because the water body occupies the lowest region in the collected data. However, the use of elevation for discrimination is ineffective when working with elevated water bodies. In addition, multispectral LiDAR data is required to fully automate the land/water discrimination process using the seeded region growing algorithm. However, the automation process is restricted by the double returns recorded from water bodies at the green wavelength (532 nm). It should be pointed out that dualwavelength LiDAR systems are suitable for this application if the green wavelength (532 nm) is used.

95

6.1.2. Land Cover Classification

In this research, a hierarchal point-based classification method for multispectral airborne LiDAR data covering an urban area was presented. Eight urban covers were classified, namely buildings, trees with green leaves, trees with red leaves, roads, grass, swimming pools, power lines and unclassified points. The data were first merged and three intensity values for each LiDAR point were estimated in a pre-processing step. A ground filtering method was applied to separate non-ground from ground points. Three NDVIs were then calculated and NDVIs' histograms were constructed. The NDVIs' histograms were fitted using a Gaussian decomposition with EM and NLS. The output Gaussian parameters were used as an input to MVGD, whereas four main classes (buildings, trees, roads and grass) were automatically clustered from the multispectral LiDAR data. The data acquired at different wavelengths from land objects allowed for labeling additional classes based on their spectral characteristics. Thus, the trees class was separated into trees with green leaves and trees with red leaves, which have no returns in the green channel (C3). In addition, swimming pools were classified based on returns from pools surfaces in C1 and C2, and returns from pools surfaces and/or beds in C3. Moreover, power lines were classified based on the fact that they mainly have reflection in C1.

Generally, the ability to use geometric and radiometric information of LiDAR data only in land cover classification of urban areas was presented. This includes the following contributions: 1. Eight classes were labeled directly from the 3D LiDAR points in urban areas. 2. A ground filtering mechanism was explained in order to separate ground point from nonground points. 3. A Gaussian decomposition was employed to automatically cluster the LiDAR data, whereas the number of Gaussian components and their initial parameters were automatically detected by testing the quality of the fitted Gaussians. 4. A multivariate Gaussian decomposition was presented to maximize the use of multispectral data in the classification process. 5. Each object's spectral information was used to identify swimming pools, power lines and two types of trees. 6. New procedures were presented to obtain 3D reference points from aerial images. 96

Three data subsets collected from an urban area were used for evaluation. The data subsets represent different complexities of urban areas. The presented approach revealed an overall accuracy of 91.7%, 93.0% and 87.1% for Area 1, Area 2 and Area 3, respectively. Trees with green leaves and trees with red leaves are most confusing and caused the most classification errors because tree canopies produce multiple returns and that are not identical for the same tree. Also, the different building roof materials caused confusion and misclassification with trees. Radiometric correction and normalization of the multispectral data might improve the classification accuracy of those features. In addition, the ground filtering and unclassified points contributed to classification errors. The power lines points were labeled based on they have returns at the MIR wavelength. Many power lines points were correctly labeled and some points were misclassified due to having returns at the NIR and/or green wavelength.

The presented ground filtering method shows a high potential for separating non-ground from ground points, where the slope of the LiDAR point with respect to neighbouring points was considered. The MVGD has automatically clustered the multispectral LiDAR data into four classes. The results demonstrated that using the NDVIs for separating vegetation areas from built-up areas produced more than a 95.0% overall accuracy rate, as shown in Figure 5.20. Moreover, consideration of spatial coherence using a 3D filter showed a significant improvement in the classified LiDAR data. The overall accuracies for eight classes of Area 1, Area 2 and Area 3 were improved to 95.7%, 98.3% and 95.3%, respectively.

A data clustering method was also employed to divide the LiDAR data into buildings, trees, roads and grass based on Jenks breaks optimization algorithm. The three NDVIs were evaluated separately by clustering NDVIs' histograms of the non-ground and ground points into buildings or trees and roads or grass. The use of NDVIC2-C1, NDVIC2-C3 and NDVIC1-C3 achieved an overall accuracy of 85.1%, 95.6% and 92%, respectively for Area 1, 86%, 95.5% and 90.6%, respectively for Area 2, and 71.1%, 96.2% and 94.7%, respectively for Area 3. Despite achieving a high overall accuracy, the power lines points' classification could not quantitatively be assessed because they are vertically distributed in the LiDAR data and could 97

not be digitized from the aerial images. Although the 3D filter has significantly improved the classification results, the window size of the 3D filter could be changed according to the required level of classified details.

6.2. Recommendations

The presented approaches illustrate the capability of multispectral LiDAR data in land/water discrimination in coastal areas and in land cover classification of urban areas. However, the multispectral data could be more effective in land/water discrimination if researchers explore more point features and normalize the intensity data of the three channels. Land/water discrimination could be extended to consider shorelines with submerged vegetation and debris and inland water areas. Also, the multispectral LiDAR data could be investigated for water depth estimation and seabed classification.

For land cover classification, LiDAR point features based on the geometric distribution of the objects could be considered in order to improve the classification of obtained objects (e.g., power lines) and to classify more land objects. Although the point-based classification approach produces noisy results, the segment-based classification method could overcome this problem. Vegetation mapping in urban areas as well as in forestry areas has recently become an important topic, especially that multispectral LiDAR data are available. Also, more land objects could be classified as buildings with different roof materials, bare soil, parking lots, sidewalks and road markings.

98

APPENDICES

Appendix A: Confusion Matrices for Land/Water Discrimination

This appendix presents the confusion matrices of the land/water discrimination based on all point features as well as the region growing algorithm for Area_LO and Area_TH.

A.1 Elevation-based Features

Area_LO A.1.1 Elevation Reference Data Discrimination C1 Land Water Land 813977 20131 Water 433 136949

Area_TH

Reference Data Discrimination Land Water Land 455346 1091 Water 47 134345

Reference Data Discrimination C2 Land Water Land 910969 20157 Water 579 191713 Discrimination Land Water

Reference Data Land 449221 1102 Water 52 148778

Reference Data Discrimination C3 Land Water Land 864216 20370 Water 631 518169 Discrimination Land Water

Reference Data Land 447023 4088 Water 72 189494

99

A.1.2 Elevation Difference (Z) Reference Data Discrimination C1C3 Land Water Land 556258 328328 Water 9643 509157 Discrimination Land Water Reference Data Land 310533 140578 Water 80293 109273

Reference Data Discrimination C2C3 Land Water Land 595532 289054 Water 12790 506010 Discrimination Land Water

Reference Data Land 321633 129478 Water 84237 105324

A.1.3 Roughness (R) Reference Data Discrimination C1C3 Land Water Land 508506 376080 Water 10297 508503 Discrimination Land Water Reference Data Land 310533 140578 Water 80293 109273

Reference Data Discrimination C2C3 Land Water Land 529114 355472 Water 12248 506552 Discrimination Land Water

Reference Data Land 321633 129478 Water 84237 105324

100

A.2 Intensity-based Features

Area_LO A.2.1 Intensity Reference Data Discrimination C1 Land Water Land 450165 383943 Water 36963 100419

Area_TH

Reference Data Discrimination Land Water Land 225704 230733 Water 19690 114702

Reference Data Discrimination C2 Land Water Land 480254 450872 Water 53617 138675 Discrimination Land Water

Reference Data Land 237101 213222 Water 34207 114623

Reference Data Discrimination C3 Land Water Land 389117 495469 Water 54844 463956 Discrimination Land Water

Reference Data Land 209187 241924 Water 8078 181488

A.2.2 Normalized Difference Water Index (NDWI) Reference Data Discrimination C3C2 Land Water Land 649972 281154 Water 117740 74543 Discrimination Land Water Reference Data Land 169322 281001 Water 78480 70349

Reference Data Discrimination C3C1 Land Water Land 655837 178271 Water 77278 60096 Discrimination Land Water

Reference Data Land 340416 116021 Water 77340 57052

101

Reference Data Discrimination C1C2 Land Water Land 611515 319611 Water 87388 104895 Discrimination Land Water

Reference Data Land 151182 299141 Water 65494 83335

A.2.3 Intensity Coefficient of Variation (ICOV) Reference Data Discrimination C1 Land Water Land 432219 401889 Water 4724 132658 Discrimination Land Water Reference Data Land 277445 178992 Water 5404 128988

Reference Data Discrimination C2 Land Water Land 363963 567163 Water 7709 184583 Discrimination Land Water

Reference Data Land 253929 196394 Water 8326 140504

Reference Data Discrimination C3 Land Water Land 679301 205285 Water 236282 282518 Discrimination Land Water

Reference Data Land 257640 193471 Water 73789 115777

A.2.4 Intensity Density (ID) Reference Data Discrimination C1 Land Water Land 603670 230438 Water 51472 85910 Discrimination Land Water Reference Data Land 247312 209125 Water 12840 121552

102

Reference Data Discrimination C2 Land Water Land 711823 219303 Water 78846 113446 Discrimination Land Water

Reference Data Land 268378 181945 Water 47717 101113

Reference Data Discrimination C3 Land Water Land 458415 426171 Water 55266 463534 Discrimination Land Water

Reference Data Land 244450 206661 Water 4733 184833

A.3 Geometry-based Features

Area_LO A.3.1 Point Density (PD) Reference Data Discrimination C1C3 Land Water Land 241581 643005 Water 13988 504812

Area_TH

Reference Data Discrimination Land Water Land 200295 250816 Water 23543 166023

Reference Data Discrimination C2C3 Land Water Land 390126 494460 Water 21833 496967 Discrimination Land Water

Reference Data Land 195408 255703 Water 38185 151381

103

A.3.2 Multiple Returns (MR) Reference Data Discrimination C1C3 Land Water Land 864542 20045 Water 120982 397813 Discrimination Land Water Reference Data Land 435905 15206 Water 87416 102150

Reference Data Discrimination C2C3 Land Water Land 869559 15028 Water 108946 409849 Discrimination Land Water

Reference Data Land 437516 13595 Water 87588 101978

A.4 Region Growing (RG)

Area_LO Reference Data Discrimination C1C3 Land Water Land 832363 1745 Water 3800 133583

Area_TH Reference Data Discrimination Land Water Land 455848 589 Water 1092 133300

Reference Data Discrimination C2C3 Land Water Land 928551 2575 Water 4705 187589 Discrimination Land Water

Reference Data Land 449760 563 Water 2460 146370

104

Appendix B: Confusion Matrices and Figures for Land Cover Classification

This appendix presents the figures of the ground filtering results of the three tested areas in Section B.1, confusion matrices of the three areas after the application of the 3D filter for seven classes in Section B.2, and figures and confusion matrices of the three areas for five classes based on NDVIC2-C1, NDVIC2-C3, NDVIC1-C3, MVGD, and MVGD_filtered in Section B.3.

B.1 Ground Filtering Figures

Figure B.1. LiDAR points of Area 1; non-ground points (left) and ground points (right)

Figure B.2. LiDAR points of Area 2; non-ground points (left) and ground points (right) 105

Figure B.3. LiDAR points of Area 3; non-ground points (left) and ground points (right)

B.2 Confusion Matrices after the Application of the 3D Filter

Table B.1. Confusion matrix of Area 1 Reference data Classification data Unclassified Buildings G_Trees R_Trees Roads Grass SP Total column Producer's Acc. (%) Buildings G_Trees R_Trees 2 10359 33 4 0 0 0 10398 99.6 36 214 5926 964 0 0 0 7140 83.0 5 1 7 2493 0 0 0 2506 99.5 Roads 13 1 0 2 3959 0 103 4078 97.1 Grass 0 9 0 0 0 8783 0 8792 99.9 SP 0 32 14 0 0 0 492 538 91.4 Total row 56 10616 5980 3463 3959 8783 595 33452 97.6 99.1 72.0 100.0 100.0 82.7 User's Acc. (%)

Overall accuracy: 95.7%

106

Table B.2. Confusion matrix of Area 2 Reference data Classification data Unclassified Buildings G_Trees R_Trees Roads Grass SP Total column Producer's Acc. (%) Buildings G_Trees R_Trees 0 10227 7 0 0 0 0 10234 99.9 30 48 6993 381 0 0 1 7453 93.8 21 3 75 4373 0 0 0 4472 97.8 Roads 0 0 0 0 5734 0 0 5734 100.0 Grass 5 0 35 30 0 8943 0 9013 99.2 SP 0 32 0 0 0 0 1257 1289 97.5 Total row 56 10310 7110 4784 5734 8943 1258 38195 99.2 98.4 91.4 100.0 100.0 99.9 User's Acc. (%)

Overall accuracy: 98.3%

Table B.3. Confusion matrix of Area 3 Reference data Classification data Unclassified Buildings G_Trees R_Trees Roads Grass SP Total column Producer's Acc. (%) Buildings G_Trees R_Trees 0 7403 0 0 0 0 0 7403 100.0 165 126 29862 3490 0 0 0 33643 88.8 4 0 87 1484 0 0 0 1575 94.2 Roads 0 0 0 0 3025 0 0 3025 100.0 Grass 6 0 0 0 0 37212 0 37218 99.9 SP 0 6 0 0 0 0 381 387 98.4 Total row 175 7535 29949 4974 3025 37212 381 83251 98.2 99.7 29.8 100.0 100.0 100.0 User's Acc. (%)

Overall accuracy: 95.3%

107

B.3 Figures and Confusion Matrices of Four Classes

B.3.1 Figures and Confusion Matrices of Area 1

Figure B.4. Classified LiDAR points based on NDVIC2-C1

Figure B.5. Classified LiDAR points based on NDVIC2-C3

108

Figure B.6. Classified LiDAR points based on NDVIC1-C3

Figure B.7. Classified LiDAR points based on MVGD

Figure B.8. Classified LiDAR points based on MVGD_filtered

109

Table B.4. Confusion matrix based on NDVIC2-C1 Reference data Classification data Unclassified Buildings Trees Roads Grass Total column Producer's Acc. (%) Overall accuracy: 85.1% Buildings Trees 5 9083 1310 0 0 10398 87.4 215 2284 7147 0 0 9646 74.1 Roads 134 87 2 3808 47 4078 93.4 Grass 76 34 95 620 7967 8792 90.6 Total Row 430 11488 8554 4428 8014 32914 79.1 83.6 86.0 99.4 User's Acc. (%)

Table B.5. Confusion matrix based on NDVIC2-C3 Reference data Classification data Unclassified Buildings Trees Roads Grass Total column Producer's Acc. (%) Overall accuracy: 95.6% Buildings Trees 5 10066 327 0 0 10398 96.8 215 360 9071 0 0 9646 94.0 Roads 134 20 69 3772 83 4078 92.5 Grass 76 10 119 31 8556 8792 97.3 Total Row 430 10456 9586 3803 8639 32914 96.3 94.6 99.2 99.0 User's Acc. (%)

Table B.6. Confusion matrix based on NDVIC1-C3 Reference data Classification data Unclassified Buildings Trees Buildings Trees 5 9386 1007 215 627 8804 Roads 134 12 77 Grass 76 14 115 Total Row 430 10039 10003 93.5 88.0 User's Acc. (%)

110

Roads Grass Total column Producer's Acc. (%) Overall accuracy: 92.0%

0 0 10398 90.3

0 0 9646 91.3

3583 272 4078 87.9

80 8507 8792 96.8

3663 8779 32914

97.8 96.9

92.0

Table B.7. Confusion matrix based on MVGD Reference data Classification data Unclassified Buildings Trees Roads Grass Total column Producer's Acc. (%) Overall accuracy: 95.6% Buildings Trees 5 10293 100 0 0 10398 99.0 215 567 8864 0 0 9646 91.9 Roads 134 20 69 3774 81 4078 92.5 Grass 76 25 104 47 8540 8792 97.1 Total Row 430 10905 9137 3821 8621 32914 94.4 97.0 98.8 99.1 User's Acc. (%)

Table B.8. Confusion matrix based on MVGD_filtered Reference data Classification data Unclassified Buildings Trees Roads Grass Total column Producer's Acc. (%) Overall accuracy: 99.1% Buildings Trees 2 10356 40 0 0 10398 99.6 39 165 9442 0 0 9646 97.9 Roads 13 1 2 4046 16 4078 99.2 Grass 0 9 0 0 8783 8792 99.9 Total Row 54 10531 9484 4046 8799 32914 98.3 99.6 100.0 99.8 User's Acc. (%)

111

B.3.2 Figures and Confusion Matrices of Area 2

Figure B.9. Classified LiDAR points based on NDVIC2-C1

Figure B.10. Classified LiDAR points based on NDVIC2-C3

112

Figure B.11. Classified LiDAR points based on NDVIC1-C3

Figure B.12. Classified LiDAR points based on MVGD

113

Figure B.13. Classified LiDAR points based on MVGD_filtered

Table B.9. Confusion matrix based on NDVIC2-C1 Reference data Classification data Unclassified Buildings Trees Roads Grass Total column Producer's Acc. (%) Overall accuracy: 86.0% Buildings Trees 0 8956 1278 0 0 10234 87.5 291 2306 9328 0 0 11925 78.2 Roads 4 0 0 5622 108 5734 98.0 Grass 61 91 485 527 7849 9013 87.0 Total Row 356 11353 11091 6149 7957 36906 78.9 84.1 91.4 98.6 User's Acc. (%)

Table B.10. Confusion matrix based on NDVIC2-C3 Reference data Classification data Unclassified Buildings Buildings Trees 0 9794 291 240 Roads 4 0 114 Grass 61 15 Total Row 356 10049 97.5 User's Acc. (%)

Trees Roads Grass Total column Producer's Acc. (%) Overall accuracy: 95.5%

440 0 0 10234 95.7

11394 0 0 11925 95.5

0 5710 20 5734 99.6

561 37 8339 9013 92.5

12395 5747 8359 36906

91.9 99.4 99.8

Table B.11. Confusion matrix based on NDVIC1-C3 Reference data Classification data Unclassified Buildings Trees Roads Grass Total column Producer's Acc. (%) Overall accuracy: 90.6% Buildings Trees 0 8399 1835 0 0 10234 82.0 291 317 11317 0 0 11925 94.9 Roads 4 0 0 5588 142 5734 97.5 Grass 61 40 536 232 8144 9013 90.4 Total Row 356 8756 13688 5820 8286 36906 95.9 82.7 96.0 98.3 User's Acc. (%)

Table B.12. Confusion matrix based on MVGD Reference data Classification data Unclassified Buildings Trees Roads Grass Total column Producer's Acc. (%) Overall accuracy: 95.9% Buildings Trees 0 10145 89 0 0 10234 99.1 291 413 11221 0 0 11925 94.1 Roads 4 0 0 5719 11 5734 99.7 Grass 61 24 552 65 8311 9013 92.2 Total Row 356 10582 11862 5784 8322 36906 95.9 94.6 98.9 99.9 User's Acc. (%)

115

Table B.13. Confusion matrix based on MVGD_filtered Reference data Classification data Unclassified Buildings Trees Roads Grass Total column Producer's Acc. (%) Overall accuracy: 99.5% Buildings Trees 0 10227 7 0 0 10234 99.9 48 41 11836 0 0 11925 99.3 Roads 0 0 0 5734 0 5734 100.0 Grass 5 0 88 0 8920 9013 99.0 Total Row 53 10268 11931 5734 8920 36906 99.6 99.2 100.0 100.0 User's Acc. (%)

B.3.3 Figures and Confusion Matrices of Area 3

Figure B.14. Classified LiDAR points based on NDVIC2-C1

116

Figure B.15. Classified LiDAR points based on NDVIC2-C3

Figure B.16. Classified LiDAR points based on NDVIC1-C3

117

Figure B.17. Classified LiDAR points based on MVGD

Figure B.18. Classified LiDAR points based on MVGD_filtered

Table B.14. Confusion matrix based on NDVIC2-C1 Reference data Classification data Unclassified Buildings Trees Buildings Trees 0 6449 954 1128 9622 24468 Roads 157 0 0 118 Grass 43 0 2 Total Row 1328 16071 25424 40.1 96.2 User's Acc. (%)

Roads Grass Total column Producer's Acc. (%) Overall accuracy: 71.1%

0 0 7403 87.1

0 0 35218 69.5

2868 0 3025 94.8

12068 25105 37218 67.5

14936 25105 82864

19.2 100.0

Table B.15. Confusion matrix based on NDVIC2-C3 Reference data Classification data Unclassified Buildings Trees Roads Grass Total column Producer's Acc. (%) Overall accuracy: 96.2% Buildings Trees 0 7004 399 0 0 7403 94.6 1128 1431 32659 0 0 35218 92.7 Roads 157 0 0 2868 0 3025 94.8 Grass 43 0 2 22 37151 37218 99.8 Total Row 1328 8435 33060 2890 37151 82864 83.0 98.8 99.2 100.0 User's Acc. (%)

Table B.16. Confusion matrix based on NDVIC1-C3 Reference data Classification data Unclassified Buildings Trees Roads Grass Total column Producer's Acc. (%) Overall accuracy: 94.7% Buildings Trees 0 6802 601 0 0 7403 91.9 1128 2174 31916 0 0 35218 90.6 Roads 157 0 0 2637 231 3025 87.25 Grass 43 0 2 29 37144 37218 99.87 Total Row 1328 8976 32519 2666 37375 82864 94.7 75.8 98.1 98.9 99.4 User's Acc. (%)

119

Table B.17. Confusion matrix based on MVGD Reference data Classification data Unclassified Buildings Trees Roads Grass Total column Producer's Acc. (%) Overall accuracy: 95.2% Buildings Trees 0 7259 139 0 0 7398 98.1 1128 2348 31735 0 0 35211 90.1 Roads 157 0 0 2867 0 3024 94.8 Grass 43 0 2 194 36977 37216 99.4 Total Row 1328 9607 31876 3061 36977 82849 75.6 99.6 93.7 100.0 User's Acc. (%)

Table B.18. Confusion matrix based on MVGD_filtered Reference data Classification data Unclassified Buildings Trees Roads Grass Total column Producer's Acc. (%) Overall accuracy: 99.7% Buildings Trees 0 7430 0 0 0 7430 100.0 160 95 34963 0 0 35218 99.3 Roads 0 0 0 3025 0 3025 100.0 Grass 6 0 0 0 37212 37218 99.9 Total Row 166 7525 34963 3025 37212 82891 98.7 100.0 100.0 100.0 User's Acc. (%)

120

REFERENCES Ackermann, F., 1999. Airborne laser scanning--present status and future expectations. ISPRS Journal of Photogrammetry and Remote sensing, 54(2), pp.64-67.

Allouis, T., Bailly, J.S., Pastol, Y. and Le Roux, C., 2010. Comparison of LiDAR waveform processing methods for very shallow water bathymetry using Raman, nearinfrared and green signals. Earth Surface Processes and Landforms, 35(6), pp.640-650.

Antonarakis, A.S., Richards, K.S. and Brasington, J., 2008. Object-based land cover classification using airborne LiDAR. Remote Sensing of Environment, 112(6), pp.29882998. Bakula, K., Kupidura, P. and Jelowicki, L., 2016. Testing of land cover classification from multispectral airborne laser scanning data. International Archives of the Photogrammetry, Remote Sensing & Spatial Information Sciences, 41.

Baltsavias, E.P., 1999. Airborne laser scanning: basic relations and formulas. ISPRS Journal of photogrammetry and remote sensing, 54(2), pp.199-214.

Bartels, M., Wei, H. and Mason, D.C., 2006, August. DTM generation from LiDAR data using skewness balancing. In Pattern Recognition, 2006. ICPR 2006. 18th International Conference on (Vol. 1, pp. 566-569). IEEE.

Bentley,

J.L.,

1975.

Multidimensional

binary

search

trees

used

for

associative

searching. Communications of the ACM, 18(9), pp.509-517.

Blomley, R., Jutzi, B. and Weinmann, M., 2016. Classification of airborne laser scanning data using geometric multi-scale features and different neighbourhood types. ISPRS Annals of Photogrammetry, Remote Sensing & Spatial Information Sciences, 3(3).

121

Brennan, R. and Webster, T.L., 2006. Object-oriented land cover classification of LiDARderived surfaces. Canadian Journal of Remote Sensing, 32(2), pp.162-172.

Briese, C., Pfennigbauer, M., Lehner, H., Ullrich, A., Wagner, W. and Pfeifer, N., 2012. Radiometric calibration of multi-wavelength airborne laser scanning data. ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci, 1, pp.335-340.

Briese, C., Pfennigbauer, M., Ullrich, A. and Doneus, M., 2013. Multi-wavelength airborne laser scanning for archaeological prospection. Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci, 40, pp.119-124.

Brzank, A., Heipke, C., Goepfert, J. and Soergel, U., 2008. Aspects of generating precise digital terrain models in the Wadden Sea from LiDAR ­water classification and structure line extraction. ISPRS Journal of Photogrammetry and Remote Sensing, 63(5), pp.510-528.

Charaniya, A.P., Manduchi, R. and Lodha, S.K., 2004, June. Supervised parametric classification of aerial LiDAR data. In Computer Vision and Pattern Recognition Workshop, 2004. CVPRW'04. Conference on (pp. 30-30). IEEE.

Chauve, A., Mallet, C., Bretar, F., Durrieu, S., Pierrot-Deseilligny, M. and Puech, W., 2008, July. Processing full-waveform LiDAR data: modelling raw signals. In International archives of photogrammetry, remote sensing and spatial information sciences 2007 (pp. 102-107).

Chehata, N., Guo, L. and Mallet, C., 2009. Airborne LiDAR feature selection for urban classification using random forests. International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences, 38(Part 3), p.W8.

Chen, Y., Su, W., Li, J. and Sun, Z., 2009. Hierarchical object oriented classification using very high resolution imagery and LiDAR data over urban areas. Advances in Space Research, 43(7), pp.1101-1110. 122

Clark, Roger N., Gregg A. Swayze, Richard Wise, K. Eric Livo, T. Hoefen, Raymond F. Kokaly, and Stephen J. Sutley., 2007. USGS digital spectral library splib06a. US Geological Survey, Digital Data Series 231 (2007). 10 URL: January

https://speclab.cr.usgs.gov/spectral.lib06/ds231/datatable.html 2017)

(Accessed

Danson, F.M., Gaulton, R., Armitage, R.P., Disney, M., Gunawan, O., Lewis, P., Pearson, G. and Ramirez, A.F., 2014. Developing a dual-wavelength full-waveform terrestrial laser scanner to characterize forest canopy structure. Agricultural and Forest Meteorology, 198, pp.7-14.

Dempster, A.P., Laird, N.M. and Rubin, D.B., 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the royal statistical society. Series B (methodological), pp.1-38.

Ding, X. and Li, X., 2011. Monitoring of the water-area variations of Lake Dongting in China with ENVISAT ASAR images. International Journal of Applied Earth Observation and Geoinformation, 13(6), pp.894-901.

Douglas, E.S., Martel, J., Li, Z., Howe, G., Hewawasam, K., Marshall, R.A., Schaaf, C.L., Cook, T.A., Newnham, G.J., Strahler, A. and Chakrabarti, S., 2015. Finding leaves in the forest: the dual-wavelength Echidna LiDAR. IEEE Geoscience and Remote Sensing Letters, 12(4), pp.776-780.

Du, Z., Linghu, B., Ling, F., Li, W., Tian, W., Wang, H., Gui, Y., Sun, B. and Zhang, X., 2012. Estimating surface water area changes using time-series Landsat data in the Qingjiang River Basin, China. Journal of Applied Remote Sensing, 6(1), pp.063609-063609.

Duda, R. O.; Hart, P. E.; Stork, D. G., 2001. Pattern classification. Wiley: New York, USA, 55p. 123

El-Ashmawy, N., 2015. Innovative approach for automatic land cover information extraction from LiDAR data (Doctoral dissertation, Ryerson University).

Fernandez-Diaz, J.C., Carter, W.E., Glennie, C., Shrestha, R.L., Pan, Z., Ekhtari, N., Singhania, A., Hauser, D. and Sartori, M., 2016. Capability assessment and performance metrics for the titan multispectral mapping LiDAR. Remote Sensing, 8(11), p.936.

Forlani, G., Nardinocchi, C., Scaioni, M. and Zingaretti, P., 2006. Complete classification of raw LiDAR data and 3D reconstruction of buildings. Pattern Analysis and Applications, 8(4), pp.357-374. Gao, B.C., 1996. NDWI--A normalized difference water index for remote sensing of vegetation liquid water from space. Remote sensing of environment, 58(3), pp.257-266.

Giardino, C., Bresciani, M., Villa, P. and Martinelli, A., 2010. Application of remote sensing in water resource management: the case study of Lake Trasimeno, Italy. Water resources management, 24(14), pp.3885-3899. Guenther, G.C., Brooks, M.W. and LaRocque, P.E., 2000. New capabilities of the SHOALS airborne LiDAR bathymeter. Remote Sensing of Environment, 73(2), pp.247-255.

Guenther, Gary C., 1985. Airborne Laser Hydrography: System Design and Performance factors. Rockville, MD: NOAA Professional Paper Series, National Ocean Service 1, National Oceanic and Atmospheric Administration.

Hakala, T., Suomalainen, J., Kaasalainen, S. and Chen, Y., 2012. Full waveform hyperspectral LiDAR for terrestrial laser scanning. Optics express, 20(7), pp.7119-7127.

Hartfield, K.A., Landau, K.I. and Van Leeuwen, W.J., 2011. Fusion of high resolution aerial multispectral and LiDAR data: land cover in the context of urban mosquito habitat. Remote Sensing, 3(11), pp.2364-2383. 124

Heipke, C., Mayer, H., Wiedemann, C. and Jamet, O., 1997. Evaluation of automatic road extraction. International Archives of Photogrammetry and Remote Sensing, 32(3 SECT 4W2), pp.151-160.

Höfle, B., Vetter, M., Pfeifer, N., Mandlburger, G. and Stötter, J., 2009. Water surface mapping from airborne laser scanning using signal intensity and elevation data. Earth Surface Processes and Landforms, 34(12), pp.1635-1649.

Hofton, M.A., Minster, J.B. and Blair, J.B., 2000. Decomposition of laser altimeter waveforms. IEEE Transactions on geoscience and remote sensing, 38(4), pp.1989-1996.

Huang, H., Brenner, C. and Sester, M., 2013. A generative statistical approach to automatic 3D building roof reconstruction from laser scanning data. ISPRS Journal of photogrammetry and remote sensing, 79, pp.29-43.

Huang, M.J., Shyue, S.W., Lee, L.H. and Kao, C.C., 2008. A knowledge-based approach to urban feature classification using aerial imagery with LiDAR data. Photogrammetric Engineering & Remote Sensing, 74(12), pp.1473-1485.

Im, J., Jensen, J.R. and Hodgson, M.E., 2008. Object-based land cover classification using highposting-density LiDAR data. GIScience & Remote Sensing, 45(2), pp.209-228.

Jenks, G. F., 1976. The data model concept in statistical mapping. In International yearbook of cartography; Frenzel, K.; George Philip: England; Volume 7, pp. 186-190.

Jensen, J. R., 2005. Introductory digital image processing. Pearson Prentice Hall, Upper Saddle River, New Jersey, 526p.

125

Ji, L., Zhang, L. and Wylie, B., 2009. Analysis of dynamic thresholds for the normalized difference water index. Photogrammetric Engineering & Remote Sensing, 75(11), pp.1307-1317.

Jutzi, B. and Stilla, U., 2006. Range determination with waveform recording laser systems using a Wiener Filter. ISPRS Journal of Photogrammetry and Remote Sensing, 61(2), pp.95-107.

Karila, K., Matikainen, L., Puttonen, E. and Hyyppä, J., 2017. Feasibility of multispectral airborne laser scanning data for road mapping. IEEE Geoscience and Remote Sensing Letters, 14(3), pp.294-298.

Kuenzer, C., Guo, H., Huth, J., Leinenkugel, P., Li, X. and Dech, S., 2013. Flood mapping and flood dynamics of the Mekong Delta: ENVISAT-ASAR-WSM based time series analyses. Remote Sensing, 5(2), pp.687-715.

Lacaux, J.P., Tourre, Y.M., Vignolles, C., Ndione, J.A. and Lafaye, M., 2007. Classification of ponds from high-spatial resolution remote sensing: Application to Rift Valley Fever epidemics in Senegal. Remote Sensing of Environment, 106(1), pp.66-74.

Ma, M., Wang, X., Veroustraete, F. and Dong, L., 2007. Change in area of Ebinur Lake during the 1998­2005 period. International Journal of Remote Sensing, 28(24), pp.5523-5533. MacFaden, S.W., O'Neil-Dunne, J.P., Royar, A.R., Lu, J.W. and Rundle, A.G., 2012. Highresolution tree canopy mapping for New York City using LiDAR and object-based image analysis. Journal of Applied Remote Sensing, 6(1), pp.063567-1.

Mallet, C., Bretar, F., Roux, M., Soergel, U. and Heipke, C., 2011. Relevance assessment of full-waveform LiDAR data for urban area classification. ISPRS Journal of

Photogrammetry and Remote Sensing, 66(6), pp.S71-S84.

126

Matikainen, L., Karila, K., Hyyppä, J., Litkey, P., Puttonen, E. and Ahokas, E., 2017. Objectbased analysis of multispectral airborne laser scanner data for land cover classification and map updating. ISPRS Journal of Photogrammetry and Remote Sensing, 128, pp.298-313.

McFeeters, S.K., 1996. The use of the Normalized Difference Water Index (NDWI) in the delineation of open water features. International journal of remote sensing, 17(7), pp.1425-1432.

Morgan, T.W., 2012. North Carolina Technical Specifications for LiDAR Base Mapping. Land Records Management Division North Carolina Department of the Secretary of State 2012.

Morsy, S., Shaker, A. and El-Rabbany, A., 2017a. Evaluation of distinctive features for land/water classification from multispectral airborne LiDAR data at Lake Ontario. Proceedings of the 10th International conference on Mobile Mapping Technology (MMT), Cairo, Egypt.

Morsy, S., Shaker, A. and El-Rabbany, A., 2017b. Multispectral LiDAR Data for Land Cover Classification of Urban Areas. Sensors, 17(5), p.958.

Morsy, S., Shaker, A., & El-Rabbany, A., 2016b. Potential use of multispectral airborne LiDAR data in land cover classification. 37th Asian conference on Remote Sensing (ACRS), Colombo, Sri Lanka.

Morsy, S., Shaker, A., El-Rabbany, A. and LaRocque, P.E., 2016a. Airborne multispectral LiDAR data for land-cover classification and land/water mapping using different spectral indexes. ISPRS Annals of Photogrammetry, Remote Sensing & Spatial Information Sciences, 3(3).

Mueller, N., Lewis, A., Roberts, D., Ring, S., Melrose, R., Sixsmith, J., Lymburner, L., McIntyre, A., Tan, P., Curnow, S. and Ip, A., 2016. Water observations from space:

127

Mapping surface water from 25years of Landsat imagery across Australia. Remote Sensing of Environment, 174, pp.341-352.

Nabucet, J., Hubert-Moy, L., Corpetti, T., Launeau, P., Lague, D., Michon, C. and Quenol, H., 2016, October. Evaluation of bispectral LiDAR data for urban vegetation mapping. In SPIE Remote Sensing (pp. 100080I-100080I). International Society for Optics and Photonics.

Niemeyer, J., Mallet, C., Rottensteiner, F. and Sörgel, U., 2011. Conditional random fields for the classification of LiDAR point clouds. In International Archives of the

Photogrammetry, Remote Sensing and Spatial Information Sciences:[ISPRS Hannover Workshop 2011: High-Resolution Earth Imaging For Geospatial Information] 38-4 (2011), Nr. W19 (Vol. 38, No. W19, pp. 209-214). Göttingen: Copernicus GmbH.

Niemeyer, J., Rottensteiner, F. and Soergel, U., 2014. Contextual classification of LiDAR data and building object detection in urban areas. ISPRS journal of photogrammetry and remote sensing, 87, pp.152-165.

Oliver, J.J., Baxter, R.A. and Wallace, C.S., 1996, July. Unsupervised learning using MML. In ICML (pp. 364-372).

Pe'Eri, S. and Philpot, W., 2007. Increasing the existence of very shallow-water LiDAR measurements using the red-channel waveforms. IEEE Transactions on Geoscience and Remote Sensing, 45(5), pp.1217-1223.

Persson, Å., Söderman, U., Töpel, J. and Ahlberg, S., 2005. Visualization and analysis of fullwaveform airborne laser scanner data. International Archives of Photogrammetry, Remote Sensing and Spatial Information Sciences, 36(part 3), p.W19.

128

Petrie G. and Toth C.K., 2009. Introduction to laser ranging, profiling, and scanning. In J. Shan and C.K. Toth, editors, Topographic Laser Ranging and Scanning: Principles and Processing. CRC Press.

Puttonen, E., Hakala, T., Nevalainen, O., Kaasalainen, S., Krooks, A., Karjalainen, M. and Anttila, K., 2015. Artificial target detection with a hyperspectral LiDAR over 26-h measurement. Optical Engineering, 54(1), pp.013105-013105.

Qi, H. and Altinakar, M.S., 2011. Simulation-based decision support system for flood damage assessment under uncertainty using remote sensing and census block information. Natural hazards, 59(2), pp.1125-1143.

Renslow, M.S., 2012. Manual of Airborne Topographic LiDAR. (Bethesda, MD: American Society for Photogrammetry and Remote Sensing) 504p.

Rottensteiner, F. and Briese, C., 2002. A new method for building extraction in urban areas from high-resolution LiDAR data. International Archives of Photogrammetry Remote Sensing and Spatial Information Sciences, 34(3/A), pp.295-301.

Samadzadegan, F., Bigdeli, B. and Ramzi, P., 2010, March. A Multiple Classifier System for Classification of LiDAR Remote Sensing Data Using Multi-class SVM. In MCS (pp. 254263).

Samadzadegan, F., Hahn, M. and Bigdeli, B., 2009, May. Automatic road extraction from LiDAR data based on classifier fusion. In Urban Remote Sensing Event, 2009 Joint (pp. 16). IEEE.

Schmidt, A., Rottensteiner, F. and Sörgel, U., 2012. Classification of airborne laser scanning data in Wadden sea areas using conditional random fields. In International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences [XXII ISPRS

129

Congress, Technical Commission I] 39 (2012), Nr. B3 (Vol. 39, No. B3, pp. 161-166). Göttingen: Copernicus GmbH.

Schumann, G.J.P., Neal, J.C., Mason, D.C. and Bates, P.D., 2011. The accuracy of sequential aerial photography and SAR data for observing urban flood dynamics, a case study of the UK summer 2007 floods. Remote Sensing of Environment, 115(10), pp.2536-2546.

Shan, J. and Toth, C.K., 2008. Topographic laser ranging and scanning: principles and processing. (New York, NY: CRC Press) 590p.

Shi, S., Song, S., Gong, W., Du, L., Zhu, B. and Huang, X., 2015. Improving backscatter intensity calibration for multispectral LiDAR. IEEE Geoscience and Remote Sensing Letters, 12(7), pp.1421-1425.

Shuckman, K., 2010. History of LiDAR Development. The Pennsylvania State University, Pennsylvania, United States.

Silverman, B.W., 1986. Density estimation for statistics and data analysis. ; Volume 26. CRC press.

Singh, K.K., Vogler, J.B., Shoemaker, D.A. and Meentemeyer, R.K., 2012. LiDAR-Landsat data fusion for large-area assessment of urban land cover: Balancing spatial resolution, data volume and mapping accuracy. ISPRS Journal of Photogrammetry and Remote Sensing, 74, pp.110-121.

Smeeckaert, J., Mallet, C., David, N., Chehata, N. and Ferraz, A., 2013. Large-scale classification of water areas using airborne topographic LiDAR data. Remote sensing of environment, 138, pp.134-148.

130

Stephens, E.M., Bates, P.D., Freer, J.E. and Mason, D.C., 2012. The impact of uncertainty in satellite data on the assessment of flood inundation models. Journal of Hydrology, 414, pp.162-173. Teledyne Optech ­ Titan Brochure and Specifications, 2015. Optech Titan Multispectral LiDAR System High Precision Environmental Mapping. Available online:

http://www.teledyneoptech.com/wp-content/ uploads / Titan-Specsheet-150515-WEB.pdf (accessed on 30 August 2016).

Van Dijk, A.I.J.M. and Renzullo, L.J., 2011. Water resource monitoring systems and the role of satellite observations. Hydrology and Earth System Sciences, 15(1), pp.39-55.

Vosselman, G. and Maas, H.G., 2010. Airborne and terrestrial laser scanning. (New York, NY: CRC Press) 318p.

Vosselman, G., Coenen, M. and Rottensteiner, F., 2017. Contextual segment-based classification of airborne laser scanner data. ISPRS Journal of Photogrammetry and Remote Sensing, 128, pp.354-371.

Wagner, W., Ullrich, A., Ducic, V., Melzer, T. and Studnicka, N., 2006. Gaussian decomposition and calibration of a novel small-footprint full-waveform digitising airborne laser scanner. ISPRS Journal of Photogrammetry and Remote Sensing, 60(2), pp.100-112.

Wallace, A., Nichol, C. and Woodhouse, I., 2012. Recovery of forest canopy parameters by inversion of multispectral LiDAR data. Remote Sensing, 4(2), pp.509-531.

Wang, C.K., Tseng, Y.H. and Chu, H.J., 2014. Airborne dual-wavelength LiDAR data for classifying land cover. Remote sensing, 6(1), pp.700-715.

131

Wang, L., Huang, J., Du, Y., Hu, Y. and Han, P., 2013. Dynamic assessment of soil erosion risk using Landsat TM and HJ satellite data in Danjiangkou Reservoir area, China. Remote Sensing, 5(8), pp.3826-3848.

Wei, G., Shalei, S., Bo, Z., Shuo, S., Faquan, L. and Xuewu, C., 2012. Multi-wavelength canopy LiDAR for remote sensing of vegetation: Design and system performance. ISPRS journal of photogrammetry and remote sensing, 69, pp.1-9.

Wichmann, V., Bremer, M., Lindenberger, J., Rutzinger, M., Georges, C. and PetriniMonteferri, F., 2015. Evaluating the potential of multispectral airborne LiDAR for topographic mapping and land cover classification. ISPRS Annals of Photogrammetry, Remote Sensing & Spatial Information Sciences.

Woodhouse, I.H., Nichol, C., Sinclair, P., Jack, J., Morsdorf, F., Malthus, T.J. and Patenaude, G., 2011. A multispectral canopy LiDAR demonstrator project. IEEE Geoscience and Remote Sensing Letters, 8(5), pp.839-843.

Xiaoliang, Z.O.U., Guihua, Z.H.A.O., Jonathan, L.I., Yuanxi, Y.A.N.G. and Yong, F.A.N.G., 2016. 3D land cover classification based on multispectral LiDAR point

clouds. International Archives of the Photogrammetry, Remote Sensing & Spatial Information Sciences, 41.

Xu, H., 2006. Modification of normalised difference water index (NDWI) to enhance open water features in remotely sensed imagery. International journal of remote sensing, 27(14), pp.3025-3033.

Xu, S., Vosselman, G. and Elberink, S.O., 2014. Multiple-entity based classification of airborne laser scanning data in urban areas. ISPRS Journal of photogrammetry and remote sensing, 88, pp.1-15.

132

Yan, W.Y., Shaker, A. and El-Ashmawy, N., 2015. Urban land cover classification using airborne LiDAR data: A review. Remote Sensing of Environment, 158, pp.295-310.

Zhou, L. and Vosselman, G., 2012. Mapping curbstones in airborne and mobile laser scanning data. International Journal of Applied Earth Observation and Geoinformation, 18, pp.293304.

133

