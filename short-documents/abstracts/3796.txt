When we see or hear another person execute an action, we tend to automatically simulate that action.  Evidence for this has been found at the neural level, specifically in parietal and premotor brain regions referred to collectively as the mirror neuron system (MNS), and the behavioural level, through an observer's tendency to mimic observed movements.  This simulation process may play a key role in emotional understanding.  It is currently unclear the extent to which the MNS is driven by bottom-up automatic recruitment of movement simulation, or by top-down (task driven) mechanisms.  The present dissertation examines the role of the MNS in the bottom-up and top-down processing of action in the auditory and visual modalities, in response to emotional and neutral movements performed by humans.  Study 1 used EEG to demonstrate that the MNS is affected by bottom-up manipulations of modality, and shows that the MNS is activated to a greater extent towards multi-modal versus unimodal sensory input.  Study 2 employed an EEG paradigm utilizing a top-down emotion judgment manipulation.  It was found that the left STG, part of the extended MNS, is affected by top-down manipulations of emotionality, but there were no areas in classical MNS that met the statistical threshold to be affected by top-down forces.  Study 3 employed an fMRi paradigm combining bottom-up and top-down manipulations.  It was found that the classical MNS was strongly affected by bottom-up differences in emotionality and modality, and minimally affected by the top-down manipulation.  Together, the three studies presented in this dissertation support the premise that the classical mirror neuron system is primarily automatic.  More research is needed to determine whether top-down manipulations can uniquely engage the MNS.
