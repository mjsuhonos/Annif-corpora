Gestures are a fundamental part of human communication and are becoming a key component
of human-computer interaction. Traditionally, to teach computers to recognize specific gestures,
researchers have used a sensor, usually a camera, to collect large gesture datasets, which
are then classified and structured using machine learning techniques. Yet finding a way to confidently
differentiate between several gesture classes has proven to be rather difficult for those
working in the gesture recognition field.

To capture the samples of movements necessary to train gesture recognition systems, the first
step is to provide research participants with appropriate instructions. As collecting gesture data
is the crucial first step of creating a robust gesture dataset, this dissertation will examine the
modalities of instruction used in gesture recognition research to examine whether appropriate
directives are conveyed to research participants. These experiments will result in the creation
of a new dataset, the PJVA-20 dataset, comprised of 50 samples of 20 gesture classes sampled
from 6 participants.

After collecting the gesture samples of the PJVA-20 dataset, this dissertation will establish the
benchmark recognition system PJVA — chiefly comprised of AMFE, Polynomial Motion Approximation,
and Principal Component Analysis (PCA)—to contribute to the gesture recognition
literature in terms of novel gesture recognition algorithms that can achieve high speed and
accuracy results. This also involves examining studies in the gesture recognition literature to
determine which machine learning algorithms offer reliability, speed, and accuracy for solving
complex gesture recognition problems, as well as experimenting and testing the PJVA approach
against other researchers in the Computer Vision and Machine Learning fields.

In particular, the MSRC-12 research provides a benchmark point of comparison for research in
this field. To test the quality of samples on the PJVA-20 against the MSRC-12, a new method is
established for extracting motion feature vectors through a novel gesture recognition approach,
AMFE. This is tested by applying PJVA to extract and label gesture data from both the MSRC-
12 and PJVA-20 datasets.
