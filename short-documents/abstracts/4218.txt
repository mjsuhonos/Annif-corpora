Video Event Recognition is an important problem in video semantic analysis. In Video Event Recognition, video scenes can be summarized into video event through understanding their contents. Previous research has proposed many solutions to solve this problem. However, so far, all of them only target high-quality videos. In this thesis, we find, with the constraints in modern applications, that low-quality videos also deserve our attention. Compared to previous works, this brings a greater challenge, as low-quality videos are lacking essential information for previous methods to work. With the quality degraded, technical assumptions made by previous works no longer stay intact. Thus, this thesis provides a solution to address this problem. Based on the generic framework proposed by previous work, we propose a novel feature extraction technique that can work well with low-quality videos. We also improve the sequence summary model based on previous work. As a result, comparing to previous works, our method reaches a similar accuracy but is tested with a much lower video quality.
