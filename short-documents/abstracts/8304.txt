Parameters in neural network such as number of neurons and layers have a direct effect on efficiency and accuracy of the network. On the other hand, it is observed that structure of a network is dependent on data complexity. It is perceived that choosing a complex network for a simple data (e.g. linearly separable data) may not only result in a weak learner but also an increase in computation time. Here, accuracy of the network has been challenged by adding Gaussian Noise to these data to investigate the effect of noise.                                                                                                                                                           This additive noise flags overcomplexity thus, decision boundary is generalized which it directs the learner to have better classification as if the number of parameters have been decreased. Throughout these comparisons, other constrains have been minimized in MATLAB to solely observe the effect of each change.
