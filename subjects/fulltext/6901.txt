SUBSPACE PREDICTIVE CONTROL: STABILITY AND PERFORMANCE ENHANCEMENT

by

Saba Sedghizadeh
MSc. in Electrical Engineering, Azad University, Tehran, Iran, 2002 BSc. in Electrical Engineering, Azad University, Tehran, Iran, 1998

A dissertation presented to Ryerson University in partial fulfillment of the requirements for the degree of Doctor of Philosophy in the program of Electrical and Computer Engineering

Toronto, Ontario, Canada, 2017 c Saba Sedghizadeh, 2017

AUTHOR'S DECLARATION FOR ELECTRONIC SUBMISSION OF A DISSERTATION

I hereby declare that I am the sole author of this dissertation. This is a true copy of the dissertation, including any required final revisions, as accepted by my examiners.

I authorize Ryerson University to lend this dissertation to other institutions or individuals for the purpose of scholarly research.

I further authorize Ryerson University to reproduce this dissertation by photocopying or by other means, in total or in part, at the request of other institutions or individuals for the purpose of scholarly research.

I understand that my dissertation may be made electronically available to the public for the purpose of scholarly research only.

ii

Abstract Subspace Predictive Control: Stability and Performance Enhancement PhD, 2017
Saba Sedghizadeh Electrical and Computer Engineering Ryerson University
In the absence of prior knowledge of a system, control design relies heavily on the system identification procedure. In real applications, there is an increasing demand to combine the usually time consuming system identification and modeling step with the control design procedure. Motivated by this demand, data-driven control approaches attempt to use the input-output data to design the controller directly. Subspace Predictive Control (SPC) is one popular example of these algorithms that combines Model Predictive Control (MPC) and Subspace Identification Methods (SIM). SPC instability and performance deterioration in closed-loop implementations are majorly caused by either poor tuning of SPC horizons or changes in the dynamics of the system. Stability and performance analysis of the SPC are the focus of this dissertation. We first provide the necessary and sufficient condition for SPC closed-loop stability. The results introduce SPC stability graphs that can provide the feasible prediction horizon range. Consequently, these stability constraints are included in SPC cost function optimization to provide a new method for determining the SPC horizons. The novel SPC horizon selection enhances the closed-loop performance effectively. Note that time-delay estimation and order selection in system modeling have been a challenging step in applications and industry. Here, we propose a new approach denoted by RE-based TDE that simultaneously and efficiently estimates the time-delay for the SIM framework. In addition, we use the recently developed MSEE approach for estimating the system order. Moreover, we propose an artificial intelligence approach denoted by Particle Swarm Optimization Based Fuzzy Gain-Scheduled SPC (PSO-based FGS-SPC). The method overcomes the issue of on-line adaptation of SPC gains for systems with variable dynamics in the presence of the noisy data. The approach eliminates existing tuning problem of controller gain ranges in FGS and updates the SPC gains with no need to apply any external persistently excitation signals. As a result, PSO-based FGS-SPC provides a time efficient control strategy with fast and robust tracking performance compared to conventional and state of the art methods. iii

Contents
Declaration Abstract List of Tables List of Figures List of Acronyms List of Notations 1 Introduction 1.1 1.2 1.3 Dissertation Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ii iii vii x xi xiii 1 1 6 7 8 8 9

Dissertation Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Dissertation Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 Background 2.1 System Identification Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.1.1 2.1.2 2.2 Classical Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Subspace Approach

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14

Predictive Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 2.2.1 2.2.2 Model-based Approach: Model Predictive Control (MPC) . . . . . . . . . . . 27 Data-Driven Approach: Subspace Predictive Control (SPC) . . . . . . . . . . 36

2.3 2.4 2.5

Subspace-based Approach for a Priori Knowledge Extraction . . . . . . . . . . . . . 44 Fuzzy Gain-Scheduling Procedure (FGS) . . . . . . . . . . . . . . . . . . . . . . . . . 47 Particle Swarm Optimization Technique (PSO) . . . . . . . . . . . . . . . . . . . . . 50

iv

CONTENTS 2.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 53

3 Stability and Performance of SPC 3.1 3.2

Problem Statement and Chapter Summary . . . . . . . . . . . . . . . . . . . . . . . 53 SPC Stability Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 3.2.1 3.2.2 SPC Closed-loop Transfer Function . . . . . . . . . . . . . . . . . . . . . . . . 54 SPC Closed-loop Stability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57

3.3

SPC Tuning Parameters Selection 3.3.1 3.3.2 3.3.3

Weighting Matrices (Q and R) . . . . . . . . . . . . . . . . . . . . . . . . . . 57 Prediction Horizon (Np ) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 Control Horizon (Nc ) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59

3.4

Simulation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 3.4.1 3.4.2 3.4.3 3.4.4 3.4.5 3.4.6 3.4.7 Systems Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 Stability Analysis and Determination of NSFP and FR of Np . . . . . . . . . 63 Choice of Efficient SPC Horizons NEC and NEP . . . . . . . . . . . . . . . . 68 Effect of Weighting Matrices R and Q . . . . . . . . . . . . . . . . . . . . . . 74 Performance Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76 Effect of Prediction Horizon . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78 Effect of Disturbance and Noisy Data . . . . . . . . . . . . . . . . . . . . . . 79

3.5

Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88 90

4 SPC Gains Updating: PSO-based FGS-SPC 4.1 4.2 4.3

Problem Statement and Chapter Summary . . . . . . . . . . . . . . . . . . . . . . . 90 Re-arranging SPC Gain Formula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91 PSO-based FGS-SPC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93 4.3.1 4.3.2 FGS-SPC Procedure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93 PSO-based FGS Technique . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100

4.4

Simulation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101 4.4.1 4.4.2 4.4.3 4.4.4 4.4.5 System Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101 Stability Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102 Simulation Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103 Robustness and Adaptiveness Test . . . . . . . . . . . . . . . . . . . . . . . . 104 Performance Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108 v

CONTENTS 4.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112 113

5 Time-delay and Order Selection for SIM and SPC 5.1 5.2

Problem Statement and Chapter Summary . . . . . . . . . . . . . . . . . . . . . . . 113 Data-driven Approaches for Time-delay Estimation and Order Selection . . . . . . . 114 5.2.1 5.2.2 Reconstruction Error based (RE-based) Method . . . . . . . . . . . . . . . . 114 Mean Squared Eigenvalue Error (MSEE) Method . . . . . . . . . . . . . . . 115

5.3

Simulation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116 5.3.1 5.3.2 5.3.3 5.3.4 Systems Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116 Impulse/Step Response Estimation from Noisy Data . . . . . . . . . . . . . . 117 Time-delay Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118 Order Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122

5.4

Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126 127 129 131 133 136

6 Conclusion and Future Work A Re-configuration of SPC Control Law B SPC Closed-loop Transfer Function C Proof of Theorem 3.2.1 Bibliography

vi

List of Tables
3.1 3.2 3.3 3.4 3.5 3.6 3.7 3.8 3.9 Technique of determining the feasible SPC gains, NSF P and FR of Np . . . . . . . . 58 Technique of determining the efficient SPC horizons, NEC and NEP . . . . . . . . . 61

Sign of DC-gain for open-loop SISO systems . . . . . . . . . . . . . . . . . . . . . . . 64 Sign of DC-gain for each subsystem of Distillation System . . . . . . . . . . . . . . . 64 NSF P and FR of prediction Horizon for each system . . . . . . . . . . . . . . . . . . 67 NSF P and FR of Np for Distillation System (M = 80) . . . . . . . . . . . . . . . . . 68 System order, NSF P and NEC for each system . . . . . . . . . . . . . . . . . . . . . 69 NEC for each subsystem of MIMO Distillation System . . . . . . . . . . . . . . . . . 70 Selected NEP by SPC Cost Function Evaluation . . . . . . . . . . . . . . . . . . . . 72

3.10 NEP for each subsystem of MIMO Distillation System . . . . . . . . . . . . . . . . . 72 3.11 System III: FR of Np for different R, Q = I and Nc = Np - 1 . . . . . . . . . . . . . 75 3.12 System III: FR of Np for different Q, R = 100I and Nc = Np - 1 . . . . . . . . . . . 77 3.13 NSF P , FR of Np , NEC and NEP for System IV from Noisy Data (M = 150) . . . . . 81 3.14 NSF P and FR of Np for Distillation System with Disturbance (M = 80) . . . . . . . 86 4.1 4.2 4.3 4.4 5.1 5.2 5.3 ¯e Rule decision table to update k . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97

¯y . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97 Rule decision table to update k p ¯u . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97 Rule decision table to update k p Dryer System: Performance index value for each method . . . . . . . . . . . . . . . . 109 Time-delay estimation for System IV from noisy I/O data via different methods . . . 120 Estimated Time-delay for Distillation System using RE-based TDE method . . . . . 123 Time-delay estimation for System IV from noisy I/O data via different methods . . . 125

vii

List of Figures
2.1 2.2 2.3 2.4 2.5 2.6 2.7 2.8 3.1 3.2 3.3 3.4 3.5 General block diagram of the Prediction Error Method . . . . . . . . . . . . . . . . . 13 Comparing the N4SID and MOESP algorithms . . . . . . . . . . . . . . . . . . . . . 26 Comparing model-based approach and data-driven approach in MPC . . . . . . . . 28

A typical MPC strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 A Typical Subspace Predictive Control System . . . . . . . . . . . . . . . . . . . . . 42 Typical structure of GS technique Typical structure of FGS Basic principle of PSO . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51

System I: SPC stability graph for Q = I , R = 100I and Nc = Np - 1 . . . . . . . . . 64 System II: SPC stability graph for Q = I , R = 100I and Nc = Np - 1 . . . . . . . . 65 System III: SPC stability graph for Q = I , R = 100I and Nc = Np - 1 . . . . . . . . 65 System IV: SPC stability graph for Q = I , R = 100I and Nc = Np - 1 . . . . . . . . 66 Distillation System: SPC stability graphs for each subsystem of System V for Q = 0.01I , R = 400I and Nc = Np - 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67

3.6

System III: Reference signal and output for Q = I , R = 100I , Nc = Np - 1 and different unfeasible and feasible Np values . . . . . . . . . . . . . . . . . . . . . . . . 68

3.7 3.8 3.9

Distillation System: Singular values of Lw for Distillation System . . . . . . . . . . . 70 System I: SPC stability graphs for Q = I , R = 100I and 1  Nc  14 . . . . . . . . 71

System II: SPC stability graphs for Q = I , R = 100I and 1  Nc  14 . . . . . . . . 71 . . . . . . . 72 73 73

3.10 System III: SPC stability graphs for Q = I , R = 100I and 1  Nc  14

3.11 System III: SPC cost function evaluation for Q = I , R = 100I and Nc = NEC = 11 3.12 System IV: SPC cost function evaluation for Q = I , R = 100I and Nc = NEC = 12

3.13 System III: SPC stability graphs for Q = I and Nc = Np - 1 . . . . . . . . . . . . . 74

viii

LIST OF FIGURES 3.14 System III: SPC stability graphs for Q = I and Nc = Np - 1 . . . . . . . . . . . . . 75 3.15 System III: SPC stability graphs for R = 100I and Nc = Np - 1 . . . . . . . . . . . . 76 3.16 System III: SPC stability graphs for R = 100I and Nc = Np - 1 . . . . . . . . . . . . 77 3.17 System IV: Reference input and output for Q = I , R = 100I , Nc = NEC = 12 and Np = NEP = 50 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78 3.18 System IV: Control signal u, its variation u and the constraints for Q = I , R = 100I , Nc = NEC = 12 and Np = NEP = 50 . . . . . . . . . . . . . . . . . . . . . . . 78 3.19 Distillation System: Reference inputs and outputs y1 and y2 for Q = 0.01I , R = 400I , Nc = NEC = 9 and Np = NEP = 32 . . . . . . . . . . . . . . . . . . . . . . . . 79 3.20 Distillation System: Control signals u1 , u2 and their variations u1 , u2 for Q = 0.01I , R = 400I , Nc = NEC = 9 and Np = NEP = 32 . . . . . . . . . . . . . . . . . . 80 3.21 System II: Reference signal and output for Q = I , R = 100I , Nc = NEC = 9 and different values of feasible Np . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81 3.22 System II: SPC control signal and its variation for Q = I , R = 100I , Nc = NEC = 9 and different values of feasible Np . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82

3.23 System III: Reference signal and output for Q = I , R = 100I , Nc = NEC = 11 and different values of feasible Np . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82 3.24 System III: SPC control signal and its variation for Q = I , R = 100I , Nc = NEC = 11 and different values of feasible Np . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83 84

3.25 System IV: SPC stability graphs from noisy I/O data for SNR=0db and SNR=10db

3.26 System IV: Reference input and output for noisy system for SNR=0db and SNR=10db 85 3.27 System IV: Control signal u and its variations u for noisy system for SNR=0db and SNR=10db . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86 3.28 Distillation System: SPC stability graphs for each subsystem of System V with disturbance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87 3.29 Distillation System: Reference inputs and outputs y1 and y2 for System V with disturbance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88 3.30 Distillation System: Control signals u1 , u2 and their variations u1 , u2 for System V with disturbance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89 4.1 4.2 4.3 Fuzzy subsets for e(k ), ep and up . . . . . . . . . . . . . . . . . . . . . . . . . . . 94 ¯e , k ¯y and k ¯u Fuzzy subsets for k p p . . . . . . . . . . . . . . . . . . . . . . . . . . . 98

SPC system with PSO-based FGS . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99 ix

LIST OF FIGURES 4.4 4.5 4.6 Dryer System: Collected input-output data from open-loop system . . . . . . . . . . 102 Dryer System: SPC stability graph for Q = I , R = 30I and Nc = Np - 1 . . . . . . . 103 Dryer System: Reference input and outputs for PSO-based FGS-SPC, FGS-SPC and SPC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104 4.7 System I: Control signal u, and its variation u for PSO-based FGS-SPC, FGS-SPC and SPC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105 4.8 4.9 Dryer System: Updating the controller gains for PSO-based FGS-SPC method . . . 106 Dryer System: Updating the controller gains for FGS-SPC method . . . . . . . . . 106

4.10 Dryer System: Updating the controller gains for SPC method . . . . . . . . . . . . . 107 4.11 Dryer System: Reference input (dashed) and outputs for PSO-based FGS-SPC, FGSSPC and SPC methods for noisy system . . . . . . . . . . . . . . . . . . . . . . . . . 108 4.12 Dryer System: Performance points for each method . . . . . . . . . . . . . . . . . . 109 . . . . . . . . . . . 110

4.13 Dryer System: Performance index value for each of the methods

4.14 Dryer System: Reference input and outputs for SPC method in non-convergence case 111 4.15 Dryer System: Monte-Carlo simulations consisting of 30 runs for PSO-based FGSSPC and SPC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111 5.1 5.2 5.3 5.4 5.5 5.6 5.7 System IV: Estimated IRCs and SRCs from noisy I/O data for SNR=0db . . . . . . 117 System IV: Estimated IRCs and SRCs from noisy I/O data for SNR=10db . . . . . 118 Distillation System: Estimated IRCs for each subsystem by Subspace-based method 119 Distillation System: Estimated SRCs for each subsystem by Subspace-based method 120 System IV: True noise-free IRCs and estimated noisy IRCs for SNR=0db . . . . . . 121 System IV: True noise-free IRCs and estimated noisy IRCs for SNR=10db . . . . . . 121 Distillation System: Estimated IRCs from noisy I/O data for each subsystem (REbased TDE method and Subspace-based method) . . . . . . . . . . . . . . . . . . . . 122 5.8 5.9 System IV: Singular values of Lw from noisy I/O data for SNR=0db and SNR=10db 123 System IV: RE upper bound for order selection noisy data with SNR=0db and SNR=10db . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124 5.10 Distillation System: Singular values of Lw from noisy data . . . . . . . . . . . . . . . 125 5.11 Distillation System: RE upper bound for order selection of noisy system . . . . . . . 126

x

List of Acronyms
CGRs DC DMC FGS FGS-PID FGS-SPC FR GA GPC GS I/O IESS LQG LTI MIMO MOESP MPC MSEE N4SID NB NM NS PB PD Controller Gain Ranges Direct Current Dynamic Matrix Control Fuzzy Gain Scheduling Fuzzy Gain-Scheduled Proportional Integral Derivative Fuzzy Gain-Scheduled Subspace Predictive Control Feasible Range Genetic Algorithm Generalized Predictive Control Gain Scheduling Input-Output Integrator Embedded State-space Linear Quadratic Gaussian Linear Time Invariant Multi-Input Multi-Output Multivariate Output Error State-Space Model Predictive Control Mean Squared Eigenvector Error Numerical Subspace State-Space System Identification Negative Big Negative Medium Negative Small Positive Big Proportional-Derivative

xi

LIST OF ACRONYMS PE PEM PID PM PRBS PS PSO QMPC QP RE SFP SIM SISO SNR SP SPC SVD TDE ZZ Persistently Exciting Prediction Error Method Proportional Integral Derivative Positive Medium Pseudo Random Binary Signal Positive Small Particle Swarm Optimization Quadratic Dynamic Control Quadratic Programing Reconstruction Error Shortest-Feasible-Prediction-Horizon Subspace Identification Methods Single-Input Single-Output Signal to Noise Ratio Subspace Predictor Subspace Predictive Control Singular Value Decomposition Time Delay Estimation Zero

xii

List of Notations
x(k ) u(k ) y (k )  (k ) r(k ) d(k ) e(k ) y ^(k ) A, B, C, D K k n m l p f N M Up , Uf Yp , Yf Zp , Zf Xp , Xf Wp ^f Y System state variable System input System output Innovation sequence Reference signal Disturbance signal Tracking error Predicted output State-space system matrices Kalman gain matrix Sample step System order Number of system inputs Number of system outputs Predictor past horizon Predictor future horizon Length of available I/O data Order of subspace predictor Past and future input data block Hankel matrices Past and future output data block Hankel matrices Past and future noise block Hankel matrices Past and future state variable matrices Past I/O data block Hankel matrix Predicted future output data block Hankel matrix

xiii

LIST OF NOTATIONS M
d , Hs HM M s d M , M

Extended observability matrix Lower triangular block Toeplitz matrices (deterministic and stochastic) Reversed extended controllability matrices (deterministic and stochastic) Past and future input data vectors Past and future output data vectors Past I/O data vector Time-Delay Predicted future output data vector Iteration step of PSO algorithm Particle position Particle best position Global best position Future reference signal vector Subspace linear predictor coefficient matrices Truncated subspace linear predictor coefficient matrices SPC gain matrices Truncated SPC gain matrices Truncated SPC gain vectors Normalized SPC gain vectors Auxiliary matrices SPC cost function SPC weighting matrices Control Horizon Prediction Horizon Sample-Delay Efficient prediction horizon Efficient control horizon Shortest feasible prediction horizon Backward shift operators Open-loop transfer function Closed-loop transfer function DC-gain of compound system

up , uf yp , yf wp td y ^f t p(t) pb(t) pg rf Lu , Lw ~ u, L ~w L Ke , Kw ~ e, K ~ w K ~e , k ~w k ¯e , k ¯w k F1 , F2 J Q, R Nc Np Nd NEP NEC NSF P q -1 Gol Gcl Kco

xiv

LIST OF NOTATIONS Kol eRE DC-gain of open-loop system Reconstruction error

xv

Chapter 1

Introduction
1.1 Dissertation Objectives

Two fundamental steps in developing control systems are: (i) obtaining model of the system, and (ii) designing a controller for the system. Both steps have significant impacts on the closed-loop stability and performance of the implemented control system. General approaches for the first step are system modeling and system identification. System modeling uses physics laws such as, Newton's law [1], Ohm's law [2] and thermodynamic principles [3] to obtain an appropriate mathematical description of the system. This approach is well-developed in control system design and there are many successful implementations of such technique. Nevertheless, system modeling is not applicable for complex and multivariate systems, due to expensive, time consuming and inaccurate performance of such dynamic systems. System identification is another approach to obtain the system model. It develops mathematical model of the system by using experimental data [4]. There are many applications of system identification in control with substantial overlap in other areas such as signal processing, communication and statistics. Some of the identification methods have black-box approach to the identified system which is good for automatic identification, where no additional information about the identified system is available. However, the input-output (I/O) data may be corrupted and the pure blackbox approach may lead to a model, which is significantly deviated from the real system. There is often a priori information, which should be exploited to increase the quality of the identification and to ensure that the identified model is close to some expectations. These approaches are called gray-box methods [5]. In the second step, a controller is designed based on the system model developed in the first step 1

CHAPTER 1. INTRODUCTION and the specifications of the closed-loop system performance. There are various types of controllers from traditional Proportional-Integral-Derivative (PID) controllers, most popular controller in the industry, to the advanced sophisticated modern control methods. Among them Predictive Control Method is a class of control algorithms that determines the control law from a predictive model of the system by minimizing an appropriate objective function. There is a model-based approach to predictive control, which is called Model-based Predictive Control (MBPC) or Model Predictive Control (MPC) [6­10]. MPC was introduced in early 80's which calculates dynamic model of the system from I/O data using system identification techniques and uses the model to construct the predictor matrices. MPC utilizes on-line optimization of a linear or quadratic objective function subject to linear constraints over a finite prediction horizon [7, 11, 12]. The controller uses different system models, such as impulse response or step response series [13, 14], transfer function models [15, 16] and state-space realizations [12, 17] in the presence of both soft and hard constraints. On the other hand, it has been found in late 90's that these predictor matrices can be obtained directly from the experimental I/O data [18, 19]. This approach is called data-driven or model-free predictive control, which is based on an appropriate combination of MPC with Subspace Identification Methods (SIM) [18­20]. In data-driven approach the predictor matrices are determined directly from the experimental I/O data by using subspace predictor matrices which eliminates the intermediate parametric model identification step [18, 19]. The resultant methodology is called Data-driven Subspace Predictive Control or briefly Subspace Predictive Control (SPC) in the literature [18], [21]. In SPC methodology the system identification and the controller design steps are combined and implemented in a single-step which is calculating the subspace predictor. Some features of SPC, such as no pre-assumptions about system model, calculation of predictor matrices without iteration and no need to solving Diophantine equation are some of the advantages of SPC in practical applications [21, 22]. With increasing popularity of MPC and SPC in industrial applications [23] such as, chemical engineering [24­26], power systems [27­29], smart grids and buildings [30, 31], network control systems [32, 33], vehicle control [34], their closed-loop stability and performance have become significant and controversial issues in predictive control [35­38]. This dissertation focuses on studying the SPC methodology and overcoming some of its well-known issues.

2

CHAPTER 1. INTRODUCTION SPC Stability Due to the finite horizons formulation in MPC and SPC, stability and feasibility are not guaranteed for all time instants. Stability issue of MPC is studied in several papers in the literature, an exhaustive review of stability solutions for MPC is provided in [39]. In [40] and [41] closed-loop stability criterion is presented for extended predictive control by taking into account the prediction horizon value and the reference trajectory dynamics. Necessary and sufficient conditions of infinite horizon Generalized Predictive Control (GPC) GPC is presented in [42] with no need to solve a Lyapunov equation. Stability conditions for constrained GPC and constrained receding horizon predictive control are presented in [43] by imposing terminal constraints on inputs and outputs over some constraint horizon and checking the monotonicity of cost function. The MPC stability is investigated in [44] by checking the monotonicity of cost function for a nonlinear predictor based on neural network. In [45] several criteria are provided to guarantee the closed-loop stability of GPC in the field of self-tuning control. However, all these MPC closed-loop stability strategies have been developed with assumption that the system dynamic model or some prior information, such as open-loop settling time, rise time, time-delay are available and all state variables can be measured [46­49]. Since SPC is a model-free approach, the existing model-based techniques for tuning and stability are not applicable for SPC, and closed-loop stability of SPC is still an open problem. Therefore, stability analysis of the SPC is the main research objective of this dissertation. SPC Performance Performance monitoring of a closed-loop system has three steps: First, an appropriate benchmark is defined for comparison and to determine the controller capability. Second, the controller operation by proper monitoring statistics is assessed [50, 51]. Third, possible performance degradation of the controller is detected. The main focus of the existing works on performance monitoring of SPC concentrates on the first and second steps [36, 52], and the third step is not addressed adequately. Generally, there are some internal and external factors that cause performance deterioration in SPC [53]. Internal factors include changes in real process which affect the subspace predictor matrices in SPC, changes in the controller constraints, and poor tuning of the SPC parameters. External factors include noisy data, disturbances, length of data and malfunction of sensors/actuators. This dissertation focuses on changes in real process in the presence of noisy data and tuning of the SPC parameters to achieve an efficient performance.

3

CHAPTER 1. INTRODUCTION SPC Tuning Parameters MPC and SPC have same cost functions and tuning parameters, which include prediction horizon, control horizon and weighting matrices to penalty the tracking error, control signal and its variation. Appropriate choice of these parameters can significantly influence the closed-loop stability and performance. There are extensive studies in the literature that provide several tuning strategies for MPC [48, 54­58]. There exist some attempts to improve the performance and computational efficiency of MPC algorithms for fast dynamic applications by selecting small horizons. In [59] the cost function with a single horizon is considered and the optimality of control law with horizon one is discussed. In this particular application, however, the inputs are limited to a finite set of data and analysis is for a very restricted specific application. Moreover, a small deviation from the actual system can jeopardize the stability and optimality of the system. In [60] a nonlinear MPC formulation of I/O linearizion in continuous time is given for single-input single-output (SISO) systems, and shown that an I/O linearizing state feedback is a shortest-prediction-horizon MPC law. However, the method is suffering from instability for non-minimum-phase systems. This issue is addressed in [38] through the use of Lyapunov inequality constraints. All the above mentioned methods are restricted to special cases and cannot be generalize to obtain optimum horizon. Consequently, the second research objective of this dissertation is determining the feasible range of SPC horizons based on model-free algorithms to guarantee the closed-loop stability, and to achieve an optimum feasible performance by obtaining the optimum horizons. SPC Gains Updating In earlier SPC algorithms, the subspace predictor matrices are calculated off-line before the actual implementation by using open-loop I/O data [18, 21], as well as closed-loop I/O data [61, 62]. However, these approaches have not adequately addressed issue of on-line controller adaptation for time-varying systems. In application of SPC, due to existence of nonlinearities and parameter uncertainties in real processes, the input signals have to be persistently exciting (PE) the system by randomly generated signals. This feature enables SPC to obtain sufficient information from new I/O data and to predict the system outputs accurately. However, because of the high computational load and disruption in system operation, the all times persistent excitation should be avoided, especially in the steady-state mode. Therefore, much attention is given to replacing this time consuming updating process with an efficient algorithm [63]. There are several works that propose different adaptive algorithms to combine with SPC. For instance, in [64] the excitation method is illustrated 4

CHAPTER 1. INTRODUCTION by formulating additional constraints to the optimization problem, however, this approach results in solving non-convex optimization problem to compute the control signal at each time step. A similar approach is also found in [65] to assure PE in the input signal. Moreover, [66] presents a method to update the predictor in recursive manner by considering an exponential forgetting factor. Another recursive SPC technique that is based on the orthogonal Givens rotations and variable forgetting factors is demonstrated in [67] and [68]. In [68] a strategy to determine the sufficient time to apply the excitation signal to the system is proposed by considering the prediction error. A time-varying forgetting factor based strategy is presented in [69] to apply in an adaptive manner to the SPC. In [70] an extra term has been added to the cost function to ensure the PE, and to keep the optimization problem convex and quadratic, however, it degrades the control performance. Although, there have been some attempts to reduce the destructive effects of applying the all-time PE signals, but non of them could completely eliminate the requirement of PE in SPC. Therefore, providing an efficient method to eliminate the requirement of applying PE signals to update the SPC gains is the third research objective of this dissertation. Time-delay and Order Selection in SIM and SPC SIM and SPC are black-box data-driven approaches, which do not need to have prior knowledge about the system. In SPC, the subspace predictor matrices are derived directly from I/O data. This feature of SPC makes it appropriate for automatic control purposes. However, when I/O data is corrupted by noise, the pure black-box data-driven approach may lead to an incorrect result in prediction and deteriorate the SPC performance. In this scenario, incorporating some a priori information can help to appropriately select the SPC parameters and increase the quality of the prediction and performance. Some of the a priori information, which would be useful to incorporate are the system order, sign of DC-gain and time-delay. In SIM framework these parameters can be determined from the subspace predictor matrices, which are derived via I/O data block-Hankel matrices. However, estimation of these parameters from noisy I/O data is a tricky task. There are several well known order selection methods in the literature which are based on information criteria such as, Akaike's Information Criterion (AIC) [71] and Minimum Description Length (MDL) [72]. However, it has been shown that both AIC and MDL tend to overestimate the system order by increasing the Signal-to-Noise Ratio (SNR) [73]. Furthermore, MDL tends to underestimate the system order at low SNR. Moreover, most of the existing time-delay estimation techniques are threshold based methods such as, Cumulative Sum (CUSUM) method [74], Separating Frequency 5

CHAPTER 1. INTRODUCTION Method [75] and Subspace based method [76], which are not applicable for low SNR noisy data. Therefore, two new statistical approaches are presented in this dissertation to estimate the system order and time-delay, and the advantages of the proposed methods are shown over the existing methods.

1.2

Dissertation Outline

The dissertation structure are as follows: Technical background of the dissertation is provided in Chapter 2. Classical System Identification, Subspace System Identification, Model-based and Data-driven approaches of Predictive Control are reviewed in this Chapter. Model Predictive Control (MPC) and Subspace Predictive Control (SPC) algorithms are briefly discussed. Last part of the chapter focuses on a general review of Fuzzy Gain-Scheduling (FGS) and Particle Swarm Optimization (PSO) techniques. Chapter 3 provides SPC closed-loop stability analysis, and the necessary and sufficient condition for SPC closed-loop stability of open-loop stable systems is derived based on a model-free approach. Consequently, SPC stability graph is introduced, and a model-free technique is provided to find the feasible range (FR) of prediction horizon that guarantees the closed-loop stability. Next, a technique is introduced to select the efficient control horizon and prediction horizon by optimizing the SPC cost function. Chapter 4 addresses issues of SPC on-line adaption for time-varying systems, and introduces an artificial intelligence based approach to update the SPC gains. A PSO-based FGS-SPC method is proposed to eliminate the requirement of applying persistently exciting signal to system and their drawback. Moreover, existing issue of FGS in determining the controller gain ranges (CGRs) in PSO-based FGS technique is addressed and solved. Therefore, PSO-based FGS itself can be considered as a separate package for applying to any system which needs Gain-Scheduling (GS) technique. The formulation and simulation results are given in detail. The proposed method is compared with FGS-SPC and SPC method to illustrate its efficiency and performance in the presence of noisy data. In Chapter 5, the issue of black-box data-driven SPC approach in the presence of noisy data is discussed. The Chapter shows the benefits of incorporating some prior knowledge such as system order and time-delay in SIM and SPC. Mean Squared Eigenvalue Error (MSEE) method is implemented for order selection, and for time-delay estimation the Reconstruction Error Based

6

CHAPTER 1. INTRODUCTION (RE-based) method is introduced. Simulation results show the superiority of these methods over the existing methods.

1.3

Dissertation Contributions

The contributions of this dissertation are summarized as follows: · A necessary and sufficient condition for SPC closed-loop stability is provided for open-loop stable systems based on SPC gains and SPC tuning parameters. Consequently, SPC stability graph is introduced to determine the feasibility range of prediction horizon (FR of Np ) that guarantees SPC closed-loop stability. The method is applicable to non-minimum phase systems. · Efficient control horizon (NEC ) is determined by minimizing the dimension of subspace predictor matrix. Efficient prediction horizon (NEP ) is then provided by minimizing the SPC cost function to ensure the closed-loop stability holds in the presence of noise. Moreover, this optimal selection of SPC horizon length decreases the computational requirements of the SPC optimization algorithm without jeopardizing the closed-loop stability and performance. · PSO-based FGS-SPC method is proposed to optimally update the SPC gains for time-varying systems in the presence of constraints and noise. The proposed method eliminates the requirement of applying PE signals to update the subspace predictor matrices, and reduces the computational complexity load of the gain updating procedure in SPC. · The proposed PSO-based FGS procedure by itself is an optimal technique to tune the CGRs in FGS procedure, which is applicable for any system that uses FGS method. This technique overcomes the problem of calculating the CGRs in FGS algorithm, by optimally calculating the CGRs via minimizing the SPC cost function. · Novel data-driven method for order selection and new time-delay estimation method are presented in SIM framework. RE-based TDE is used for time-delay estimation and MSEE is utilized for order selection. These data-driven approaches can also be utilized for SPC in case the system order or time-delay are required.

7

Chapter 2

Background
2.1 System Identification Methods

System identification is a well-known procedure to obtain model of the system by using experimental data. There are comprehensive literature and many applications for system identification in different fields such as control systems, chemical and petroleum engineering, signal processing, statistics and more. In the area of control systems following publications by Ljung [4], Sinha and Kuszta [77], Soderstrom and Stoica [78] can be mentioned. Generally, system identification for linear systems has three main steps: Experiment Design and Data Acquisition In this step the I/O data is collected by exciting the system with appropriate persistently exciting (PE) input signals and choosing a good sampling rate around the operating point. These PE signals must sufficiently rich to excite the system so that all important aspects of the system behavior appear in the system's output. Gaussian white noise and Pseudo Random Binary Signals (PRBS) are two of the most popular PE signals in system identification. Data collecting procedure can be done in open-loop or closed-loop conditions by considering the safety and stability of the system. Sometimes, it needs to have some physical knowledge or a priori information about the system to enhance the identification [79], [80]. Model Structure Determination After gathering the enough data the suitable dynamic model structure is selected for the system based on the chosen control strategy. These models include state-space models, transfer functions 8

CHAPTER 2. BACKGROUND and time series models for parametric methods [81­83]. Also, it could be a non-parametric models such as impulse responses, frequency responses etc. [84­86]. One of the most challenging problems in fixing the good model structure is to find the system order and existing time-delay from I/O data. Parameter Estimation and Validation After choosing the model structure its parameters need to be estimated by one of the well-known system identification techniques. Typical techniques include the Prediction Error Method (PEM) [4, 87] which uses Least Squares and Maximum Likelihood techniques. The Instrumental Variable Method (IVM) which uses correlation algorithms [88, 89]. The Subspace Identification Methods (SIM) which use experimental data to estimate the state-space model of the system [19, 20, 90]. Finally, the model validation should be done to ensure that the estimated model can describe the system appropriately. In this step some techniques such as spectral and residual analysis may be hired to find the degree of confidence of the model. The whole procedure needs to be tried again, if the results are unsatisfactory. In addition, there are several criterion to select the best model among the several estimated models, such as, Akaike Information Criterion (AIC) [91], Baysian Information Criterion (BIC) [92], Minimum Description Length (MDL) [72] and Minimum Description Complexity (MDC) [93, 94].

2.1.1

Classical Approach

While this dissertation only concentrates on the Subspace approach of system identification which are used for SPC, here we provide a brief review on Classical approaches of system identification that are historically before Subspace approach. Classical system identification theory has been pioneered by Astrom and Bohlin [81] in 1960's by introducing the PEM. PEM is based on parameter optimization which uses the maximum likelihood framework. In PEM prediction error is defined as difference between the measured output y (k ) and the one-step ahead predicted output y ^(k ). The prediction error is used to create a cost function, then the cost function is optimized with respect to the parameters of the model. Hence, selecting model is the most crucial part of the PEM. The PEM was further extended by Ljung [4] who defined different classes of transfer-functionbased models. In some of these models the prediction error is linear in the model parameters, which allows for simple optimal solution by using the Least Squares method. However, for most of the 9

CHAPTER 2. BACKGROUND cases the problem is nonlinear, so a nonlinear optimization needs to be utilized. Some of the crucial problems in PEM framework are selecting the model class, determining order of the polynomials and choosing starting value of the model parameters, which are troublesome procedures especially in multi-input multi-output (MIMO) case. Procedure of the PEM can be summarized in four main steps [19]: Model Structure Selection Consider the true system represented as follows, ¯ p (q -1 )u(k ) + d(k ) y (k ) = G (2.1)

¯ p (q -1 ) is the true system, where k is the k -th sampling time, u(k ) is the input, y (k ) is the output, G q -1 is the backward shift operator, d(k ) is the output disturbance. A general stable linear time invariant (LTI) system model has following structure, y (k ) = Gp (q -1 ; )u(k ) + Gw (q -1 ; )w(k ) (2.2)

where Gp (q -1 ; ) is the system model Gw (q -1 ; ) is the noise model, w(k ) is white noise with zero mean and variance  2 , and  is a set of all unknown parameters other than the noise variance. The problem is to fit the LTI model in (2.2) to a given I/O data set {[u(k ), y (k )], k = 1, ..., N }. The most commonly used class of model structures in the PEM can be described by following equation, A(q -1 )y (k ) = where A(q -1 ) = 1 + a1 q -1 + · · · + ana q -na B (q -1 ) = b1 q -1 + · · · + bnb q -nb C (q -1 ) = 1 + c1 q -1 + · · · + cnc q -nc D(q -1 ) = 1 + d1 q -1 + · · · + dnd q -nd F (q -1 ) = 1 + f1 q -1 + · · · + fnf q -nf (2.4) (2.5) (2.6) (2.7) (2.8) B (q -1 ) C (q -1 ) u ( k ) + w(k ) F (q -1 ) D(q -1 ) (2.3)

10

CHAPTER 2. BACKGROUND the parameter to be estimated is,  = [a1 , . . . , ana , b1 , . . . , bnb , c1 , . . . , cnc , d1 , . . . , dnd , f1 , . . . , fnf ]T (2.9)

Different model structures are obtained from the PEM model in (2.3) by considering some assumptions: - Auto-Regressive with Exogenous Input Model (ARX): By considering C (q -1 ) = D(q -1 ) = F (q -1 ) = 1 in the PEM model (2.3), the ARX model can be obtained, A(q -1 )y (k ) = B (q -1 )u(k ) + w(k ) (2.10)

ARX model is a commonly used structure in system identification due to its simple and easily estimated structure.

- Auto-Regressive Moving Average with Exogenous Input Model (ARMAX): By removing the restriction from the numerator of the disturbance model in the ARX model structure ARMAX model can be obtained, A(q -1 )y (k ) = B (q -1 )u(k ) + C (q -1 )w(k ) (2.11)

The ARMAX model is useful in the design of Kalman filter and Generalized Predictive Control (GPC).

- Box-Jenkins Model (BJ): In BJ the model structure different denominators are considered for plant and disturbance models, unlike the ARMAX model, y (k ) = B (q -1 ) C (q -1 ) u ( k ) + w (k ) F (q -1 ) D(q -1 ) (2.12)

The BJ model can be useful when the disturbance has a completely different model from the plant.

- Output Error Model (OE): OE model is a special case of PEM if the estimation of the disturbance is not of interest, y (k ) = B (q -1 ) u(k ) + w(k ) F (q -1 ) 11 (2.13)

CHAPTER 2. BACKGROUND The OE model is a good option of model in practice due to have fewer parameters.

- State-space Model : Generally, in PEM an innovation form of the state-space model is used, x(k + 1) = A()x(k ) + B ()u(k ) + K () (k ) y (k ) = C ()x(k ) + D()u(k ) +  (k ) (2.14) (2.15)

where A, B, C and D are system matrices, K is the Kalman predictor gain, and  (k ) is innovation sequence. This state-space model is more suitable for MIMO systems, and it is transferable to the general form of the model in (2.2) by using following equations, Gp (q -1 ; ) = C ()[qI - A()]-1 B () Gd (q -1 ; ) = I + C ()[qI - A()]-1 K () Optimal Predictor Determination In PEM the prediction error is defined as follows, (k ; ) = y (k ) - y ^(k ; ) (2.18) (2.16) (2.17)

where (k ; ) is the one-step ahead prediction error, y (k ) is the current output and y ^(k ; ) is the onestep predicted output. The optimal one-step ahead predictor y ^(k ; ) and the one-step prediction error (k ; ) are given as follows by considering (2.1) and (2.2),
1 -1 -1 -1 -1 y ^(k ; ) = G- d (q ;  )Gp (q ;  )u(k ) + [I - Gp (q ;  )]y (k ) 1 -1 -1 -1 -1 ¯ -1 ( k ;  ) = G- d (q ;  )[Gp (q ) - Gp (q ;  )u(k )] + Gd (q ;  )d(k )

(2.19) (2.20)

For a state-space model in equations (2.14) and (2.15) in innovation form, the optimal predictor and the prediction error (k ; ) are computed as follows, x ^(k + 1; ) = [A() - K ()C ()]^ x(k ; ) + B ()u(k ) + K ()y (k ) y ^(k ; ) = C ()^ x(k ; ) + D()y (k ) (k ; ) = -C ()^ x(k ; ) - D()u(k ) + y (k ) (2.21) (2.22) (2.23)

12

CHAPTER 2. BACKGROUND Prediction Error Collection From the given experimental I/O data {[u(k ), y (k )], k = 1, ..., N } the predictions and the corresponding sequence of prediction errors are obtained as, (k ; ) = y (k ) - y ^(k ; ), Cost Function Definition To minimize the prediction error { (k ; ), k = 1, ..., N } in PEM the trace of sample covariance ^, matrix of prediction errors is selected as a cost function, JN (), to estimate the optimal  1 N
N T

k = 1, ..., N

(2.24)

min


JN () = trace[

(k ; )
i=1

(k ; )]

(2.25)

which is generally a complicated function of system parameters, and needs to be solved by iterative descent methods, which may get stuck in local minima. In the case of using ARMAX models the PEM reduces to a nonlinear optimization problem. Block diagram of PEM is shown in Figure 2.1 [19].
w(k) noise + u(k) input

System

+

y(k) output

System Model
predicted parameters

 (k;) predicted output 

+ (k;) prediction error

min{J()}

Figure 2.1: General block diagram of the Prediction Error Method

13

CHAPTER 2. BACKGROUND

2.1.2

Subspace Approach

Foundation of the system identification based on state-space model started in 1960's with solving the deterministic state-space realization problem by Ho and Kalman [95] for the first time. They show that how a state-space model can be obtained from the impulse response coefficients by forming a certain Hankel matrix. The realization algorithm is based on the property that the Hankel matrix, which is build from impulse response parameters is equal to the product of the extended observability and controllability matrices. However, the approach is only allowed for the determination of a deterministic state-space model from impulse responses. Stochastic state-space realization was further introduced by Akaike [96]. Later in 1990's the Subspace Identification Method (SIM) algorithms were developed [20,97] via extracting the system matrices from I/O data by using modern linear algebra tools. In SIM framework the state vector x(k ) is estimated from I/O data matrices by utilizing a projection algorithm, and linear algebra tools such as, Singular Value Decomposition (SVD) and QR-factorization. By having the state x(k ), input u(k ) and output y (k ) vectors the state-space matrices A, B, C and D can be estimated using Least Squares technique. Some of the important features of SIM can be listed as follows: 1- SIM framework is based on computational tools such as, SVD and QR-factorization, which makes it intrinsically robust from a numerical point of view. 2- No need to optimize an explicit cost function allowed SIM to avoid local minima and convergence problems. 3- Theoretical and experimental evidence have shown that some SIM algorithms are close to optimal, in the sense that they approach the Cramer-Rao bound. 4- SIM is a non-iterative procedure and it is more reliable to deal with MIMO systems.

An overview of SIM algorithms is given in this section. System Description There are several state-space representation forms of a system. One of the most commonly used form in system identification is the innovation form. A discrete LTI system of order n can be

14

CHAPTER 2. BACKGROUND described in state-space innovation form as, x(k + 1) = Ax(k ) + Bu(k ) + K (k ) y (k ) = Cx(k ) + Du(k ) +  (k ) (2.26) (2.27)

where k is the k -th sampling time, u(k )  Rm is the input, y (k )  Rl is the output, x(k )  Rn is the state of the system, and  (k )  Rl is an unknown innovation sequence with E [ (k ) T (k )] = S  Rl×l as the innovation covariance matrix. The matrices A  Rn×n , B  Rn×m , C  Rl×n , D  Rl×m are state-space matrices that describe the system, and K  Rn×l is the Kalman gain of the stable system. {A, C } and {A, B } are assumed to be observable and controllable, respectively. There is also another form of state-space model which has been used in subspace identification literature as below, x(k + 1) = Ax(k ) + Bu(k ) + w(k ) y (k ) = Cx(k ) + Du(k ) + v (k ) with (2.28) (2.29)

 E 

w(k ) v (k )

  wT (l) v T (l)





=

Q ST

S R

  kl  0 (2.30)

where kl is the Kronecker delta, Q  Rn×n , S  Rn×l and R  Rl×l . The process noise w(k )  Rn and the measurement noise v (k )  Rl are considered as the unobserved, Gaussian distributed, zero mean, white noise vector sequences. Note that the process noise represents the accuracy and time-lag in the estimated value, and the measurement noise represents noise characteristics of the sensor. {A, C } and {A, [ B Q1/2 ]} are assumed to be observable and controllable, respectively. It can be shown that the model in equations (2.28) and (2.29) is also convertible to innovation form in (2.26) and (2.27) [98]. Subspace Equations Suppose that measurements data of input u(k ) and output y (k ) are available for k  {1, 2, . . . , N }, where N is large enough (N  ). In addition, to have a consistent identification we assume that, the input u(k ) is PE signal and it is uncorrelated with  (k ), [20, 98]. The concept of persistency of excitation is defined in [20, 98, 99] as follows,

15

CHAPTER 2. BACKGROUND Definition 2.1.1. The input signal u(k ) is PE of order i if the following limit exists, 1 N  N
N

ru ( ) = lim

u(k +  )uT (k )
k=1

(2.31)

and the following input covariance matrix is positive definite.  ru (0) ru (1) . . . ru (i - 1)        

   ru (-1) ru (0) . . . ru (i - 2) Ru (i) =   . . . . . . . .  . . . .  ru (1 - i) . . . . . . ru (0)

(2.32)

In order to obtain the subspace equations the state-space representation in equations (2.26) and (2.27) is reformulated. For time instant k we have, x(k + 1) = Ax(k ) + Bu(k ) + K (k ) for time instant k + 1 and by substituting (2.33) we have, x(k + 2) = Ax(k + 1) + Bu(k + 1) + K (k + 1) = A(Ax(k ) + Bu(k ) + K (k )) + Bu(k + 1) + K (k + 1)     u ( k )  ( k )  + AK K   = A2 x(k ) + AB B  u(k + 1)  (k + 1) by continuing this procedure for time instant k + M - 1 can be written,  x(k + M ) = AM x(k ) + AM -1 B AM -2 B . . . B        u(k ) u(k + 1) . . . u(k + M - 1)                 (2.37) (2.34) (2.35) (2.36) (2.33)

 AM -1 K AM -2 K . . . K       

 (k )

+

 (k + 1) . . .  (k + M - 1)

16

CHAPTER 2. BACKGROUND Therefore, by collecting the state variables in a vector as below, [x(k + M ) x(k + M + 1) . . . x(k + N - (M - 1))] following equation is obtained,
x(k + M ) x(k + M + 1) ... x(k + N - (M - 1)) = AM x(k ) x(k + 1) ... x(k + M - 1)

(2.38)

 AM - 1 B AM -2 B       

u(k ) u(k + 1) . . .

u(k + 1) u(k + 2) . . .

... ... . . .

u(k + N - (M - 1)) u(k + N - (M - 2)) . . . u(k + N )

       

+

...

B

u(k + M - 1) u(k + M ) . . .

 AM -1 K AM -2 K       

 (k )  (k + 1) . . .

 (k + 1)  (k + 2) . . .

... ... . . .

 (k + N - (M - 1))  (k + N - (M - 2)) . . .  (k + N )

       

+

...

K

 (k + M - 1)  (k + M ) . . .

(2.39) By applying the same recursive substitution for (2.27) following equations are obtained. For time instant k we have, y (k ) = Cx(k ) + Du(k ) +  (k ) for time instant k + 1 and by substituting (2.26) we have, y (k + 1) = Cx(k + 1) + Du(k + 1) +  (k + 1) = C (Ax(k ) + Bu(k ) + K (k )) + Du(k + 1) +  (k + 1)     u(k )  (k )  + CK I   = CAx(k ) + CB D  u(k + 1)  (k + 1) (2.41) (2.42) (2.43) (2.40)

17

CHAPTER 2. BACKGROUND by continuing this procedure for time instant k + M - 1 can be written,  y (k + M - 1) = CAM -1 x(k ) + CAM -2 B CAM -3 B . . . D        u(k ) u(k + 1) . . . u(k + M - 1)                 (2.44)

 CAM -2 K CAM -3 K . . . I       

 (k )  (k + 1) . . .  (k + M - 1)

+

by compiling the output equations for time instants from k to k + M - 1 following matrix equation is obtained,         y (k ) y (k + 1) . . . y (k + M - 1)     +      C CA . . . CAM -1   D CB . . . 0 D . . . ... ... . . . 0 0 . . .                 (2.45) By collecting the state variables in a vector as [x(k ) x(k + 1) . . . x(k + M - 1)] following block u(k ) u(k + 1) . . . u(k + M - 1)        

      =      

CAM -2 B CAM -3 B . . . D  I 0 ... 0  (k )    CK I ... 0  (k + 1)    . . . . . . . . . . . . . . . . .  .  CAM -2 K CAM -3 K . . . I  (k + M - 1)

       xk +       

18

CHAPTER 2. BACKGROUND matrix output equation is obtained,
        y (k ) y (k + 1) . . . y (k + M - 1) y (k + 1) y (k + 2) . . . y (M ) ... ... . . . ... y (k + N - (M - 1)) y (k + N - (M - 2)) . . . y (k + N )   C CA . . . CAM -1        

      =      

x(k ) x(k + 1)

...

x(k + M - 1)

    +   

D CB . . . CAM -2 B

0 D . . . CAM -3 B

... ... . . . ...

0 0 . . . D

       

u(k ) u(k + 1) . . . u(k + M - 1)

u(k + 1) u(k + 2) . . . u(M )

... ... . . . ...

u(k + N - (M - 1)) u(k + N - (M - 2)) . . . u(k + N )

       

    +   

I CK . . . CAM -2 K

0 I . . . CAM -3 K

... ... . . . ...

0 0 . . . I

       

 (k )  (k + 1) . . .  (k + M - 1)

 (k + 1)  (k + 2) . . .  (M )

... ... . . . ...

 (k + N - (M - 1))  (k + N - (M - 2)) . . .  (k + N )

       

(2.46) Therefore, the subspace I/O matrix equations are obtained from equations (2.39) and (2.46) as below,
d s Yp = M Xp + HM Up + HN Zp

(2.47) (2.48) (2.49)

Yf Xf

d s = M Xf + HM Uf + HN Zf s = AM Xp + d M Up +  N Z p

where the superscripts d and s stand for the deterministic and stochastic part of the system, and subscripts p and f stand for the past and future. The past and future input-output (I/O) data and innovation block-Hankel matrices are defined as [20, 100],  Up
u(1) u(2) u(3) . . . ... ... . . . ... u(N - 2M + 1) u(N - 2M + 2) . . . u(N - M )

     , Uf  



u(M + 1)

u(M + 2) u(M + 3) . . . u(2M + 1)

... ... . . . ...

u(N - M + 1) u(N - M + 2) . . . u(N )

      

  u(2)   .  .  .

  u(M + 2)   . .  . 
u(2M )

u(M ) u(M + 1)

(2.50)

19

CHAPTER 2. BACKGROUND

 Yp

y (1)

y (2) y (3) . . .

... ... . . . ...

y (N - 2M + 1) y (N - 2M + 2) . . . y (N - M )

     , Yf  



y (M + 1)

y (M + 2) y (M + 3) . . . y (2M + 1)

... ... . . . ...

y (N - M + 1) y (N - M + 2) . . . y (N )

      

  y (2)   .  .  .

  y (M + 2)   . .  . 
y (2M )

y (M ) y (M + 1)

(2.51)

 Zp

 (1)

 (2)  (3) . . .

... ... . . . ...

 (N - 2M + 1)  (N - 2M + 2) . . .  (N - M )

     , Zf  



 (M + 1)

 (M + 2)  (M + 3) . . .  (2M + 1)

... ... . . . ...

 (N - M + 1)  (N - M + 2) . . .  (N )

      

   (2)   .  .  .

   (M + 2)   . .  . 
 (2M )

 (M )  (M + 1)

(2.52)

where Up , Uf  RM m×N -2M +1 , Yp , Yf  RM l×N -2M +1 and Zp , Zf  RM l×N -2M +1 .

Note that, the data in the columns of the "future" block-Hankel matrices follows the data of the same column of the "past" block-Hankel matrices. In order to have an extra freedom in identification algorithm, the row dimension of the "past" and "future" block-Hankel matrices can be different. In SIM the data block-Hankel matrices should be very rectangular to minimize the noise sensitivity. M > n, N >> max{M m, M l}, N 

The "past" and "future" state matrices, Xp and Xf , are defined as, Xp x(1) x(2) x(3) . . . x(N - 2M + 1)  Rn×N -2M +1 (2.53)

Xf

x(M + 1) x(M + 2) x(M + 3) . . . x(N - M + 1)

 Rn×N -2M +1 (2.54)

The extended (M > n) observability matrix, M , and the reversed extended controllability

20

CHAPTER 2. BACKGROUND
s matrices, d M and M , are defined as,

      =    

C CA CA2 ... CAM -1

        RM l×n    

M

(2.55)

d M = s M =

AM -1 B AM -2 B . . . AB B AM -1 K AM -2 K . . . AK K

 Rn×M m  Rn×M l

(2.56) (2.57)

d and H s , are also defined as below, The lower triangular block-Toeplitz matrices, HM M

      =    

D CB

0 D

0 0

... ...

0



d HM

  0    CAB CB D . . . 0   RM l ×M m   . . . . . . . . . . . ... .   M - 2 M - 3 M - 4 CA B CA B CA B ... D         RM l×M l    

(2.58)

      =    

I CK CAK . . .

0 I CK . . .

0 0 I . . .

... 0 ... 0 ... 0 . ... . .

s HM

(2.59)

CAM -2 K CAM -3 K CAM -4 K . . . I

Subspace Identification Methods (SIM) Subspace identification algorithms basically consist of two main steps: First the system order, n, and the extended observability matrix of the system, M or the estimate of the state sequence Xf , are determined directly from the given I/O data. Then the system matrices (A, B, C, D, Q, R, S ) are determined by using one of the following mentioned subspace identification algorithms up to similarity transformation. Two most popular subspace identification algorithms are: Numerical Subspace State-Space System Identification (N4SID), and Multivariable Output Error State-Space (MOESP).

21

CHAPTER 2. BACKGROUND - Numerical Subspace State-Space System Identification (N4SID): The algorithm constructs the projection of future output data onto past I/O data and estimates matrices via SVD and Least Squares, the algorithm was presented and expanded by Overschee and De Moor in 1990's [19, 20, 97]. This method mainly uses oblique projection concept and SVD for identification. The N4SID algorithm is summarized into four main steps, Step1: Compute the oblique projection of future outputs onto the past I/O along the future inputs, OM = Yf /Uf Wp where Wp is defined as the past I/O data matrix as below,  Wp =  Yp Up    RM (m+l)×N -2M +1 (2.61) (2.60)

From (2.48) the oblique projection can be obtained as,
d s OM = M Xf /Uf Wp + HM Uf /Uf Wp + HN Zf /Uf Wp

(2.62)

From the definition of oblique projection we have Uf /Uf Wp = 0, and when the number of samples N and number of past data row M are large enough we have, lim ^f Xf /Uf Wp = X (2.63) (2.64)

M,N 

N 

lim Zf /Uf Wp = 0

Therefore, the oblique projection in (2.62) is simplified as, OM = M Xf (2.65)

Step2: Compute the SVD of the oblique projection, then obtain system order n = dim(1 ) and the estimate of the state vector Xf . The SVD of the oblique projection be given by,  OM = U1 U2  1 0 0 0   V1T V2T   = U1 1 V1T (2.66)

Since the oblique projection is OM = M Xf , the extended observability matrix and the estimate

22

CHAPTER 2. BACKGROUND of the state vector are derived from SVD as below, M = U1 1 T  RM l×n Xf = T -1 1 V1T  Rn×N -2M +1
1/2 1/2

(2.67) (2.68)

where T  Rn×n is an arbitrary non-singular matrix representing a similarity transformation. Step3: In previous step the estimate of state vector is obtained from (2.68) as, XM = (2.69)

x(M ) x(M + 1) . . . x(N - M - 1) x(N - M )

then the following matrices with N - 2M columns are defined as, ¯ M +1 = X ¯M = X ¯ M |M = U ¯M |M = Y (2.70) (2.71) (2.72) (2.73)

x(M + 1) x(M + 2) . . . x(N - M ) x(M ) x(M + 1) . . . x(N - M - 1) u(M ) u(M + 1) . . . u(N - M - 1) y (M ) y (M + 1) . . . y (N - M - 1)

¯ M |M , Y ¯M |M are block Hankel matrices with only one block row of inputs and outputs, where U respectively. Step4: Compute the matrices A, B , C and D by solving the following regression equation by using the Least Squares technique,   ¯ M +1 X ¯M |M Y   A B C D   ¯M X ¯ M |M U   (2.74)

=

 

^ B ^ A ^ D ^ C





= 

¯ M +1 X ¯M |M Y

 

¯M X ¯ M |M U

T    T -1 ¯ ¯ XM XM          ¯ ¯ U M |M UM | M

(2.75)

- Multivariable Output Error State-Space (MOESP): This method uses RQ decomposition of joint I/O data matrices and first calculates the extended observability matrix via SVD and obtain some of the state-space matrices, then uses Least Squares method to estimate other matrices. This 23

CHAPTER 2. BACKGROUND method was introduced by Verhaegen and Dewilde in 1990's [19, 101, 102]. The MOESP algorithm is summarized into four main steps, Step1: Suppose that the past I/O data Up and Yp were obtained. Compute the RQ decomposition of the past I/O data matrix as,  Wp =  Up Yp   R11 0   QT 1 QT 2   (2.76)

=

R21 R22

where R11 and R22 are lower triangular and Q1 and Q2 are orthogonal. This RQ decomposition is
T . The following equations computed by taking the transpose of the RQ decomposition of the Wp

are written as, Up = R11 QT 1
T Yp = R21 QT 1 + R22 Q2

(2.77) (2.78)

Suppose that the following conditions are satisfied, (i) the system is reachable or the state vector is sufficiently excited, (ii) the system is persistently excited by the input sequence and (iii) the I/O data are obtained from an open-loop experiment. Under these assumption R11 is nonsingular, so that (2.78) is written as follows,
-1 Up + R22 QT Yp = R21 R11 2

(2.79)

T since QT 1 , Q2 are orthogonal, the first term in the right-hand side of the equation (2.78) is orthogonal

to the second term. The orthogonal projection of the row space of Yp onto the row space of Up and
 is given by these two equations, its complement Up

-1 Yp /Up = R21 QT 1 = R21 R11 Up

(2.80) (2.81)

 Yp /Up = R22 QT 2

also, it follows from equations (2.47) and (2.76) that,
d T T M Xp + HM R11 QT 1 = R21 Q1 + R22 Q2

(2.82)

24

CHAPTER 2. BACKGROUND post-multiplying (2.82) by Q2 yields, M Xp Q2 = R22 (2.83)

now, the image of the extended observability matrix M and the dimension n obtain from SVD of R22 . Step2: Compute the SVD of R22 to obtain the system order, n, and the extended observability matrix, M . The SVD of R22 be given by,  R22 = U1 U2  1 0 0 0   V1T V2T   = U1 1 V1T (2.84)

then M Xp Q2 = U1 1 V1T and the extended observability matrix is defined as, M = U1 1 and the order is n = dim(1 ). Step3: Obtain the matrices C and A by using the extended observability matrix. The matrix C is given by, C = M (1 : l, 1 : n) and A is obtained by solving this equation, M (1 : l(M - 1), 1 : n)A = M (l + 1 : M l, 1 : n) (2.87) (2.86)
1/2

(2.85)

TR Step4: Estimate the matrices B and D by the Least Squares method. Since U2 22 = 0 and T T U2 M = 0, pre-multiplying equation (2.82) by U2 , then post-multiplying it by Q1 yields,

T d T T T d T U2 HM R11 QT 1 = U2 R21 Q1 - U2 HM R11 = U2 R21

(2.88)

25

CHAPTER 2. BACKGROUND Now, there is a linear equation with respect to B and D, it can be solved by Least Squares method,      T  U2      D CB CAB . . . 0 D CB . . . 0 0 D . . . ... ... ... ... 0 0 0 . . .       -1 T R21 R11  = U2    

(2.89)

CAM -2 B CAM -3 B CAM -4 B . . . D

by Least Squares solution of this overdetermined linear matrix equation B and D can be estimated. Figure 2.2 shows the overview of N4SID and MOESP methods. Through the N4SID and MOESP algorithms the state sequence Xf and the extended observability matrix M are determined. Then the system matrices A, B, C and D are extracted using the algorithms.

Input-Output Data N4SID MOESP

Xf

M

System Matrices

Figure 2.2: Comparing the N4SID and MOESP algorithms

2.2

Predictive Control

In last three decades Predictive Controllers became most widely used advanced controllers in the process control in industry [7, 10, 103]. In general, predictive control is a control strategy which obtains the control signal by using a predictive model of the process in a cost function minimization. The procedure is done over a fixed prediction horizon in the presence of constraints. There are 26

CHAPTER 2. BACKGROUND two different approaches for predictive control. First is the model-based approach, which is called Model-based Predictive Control (MBPC) or briefly Model Predictive Control (MPC) [7,12,15,104]. Second the model-free approach, which is called Data-driven Subspace-based Predictive Control (SBPC) or briefly Subspace Predictive Control (SPC) [18, 21, 100]. Process model is the basic requirement of the design in model-based approach. Then the predictor matrices are obtained from the process model, and the controller is design by using the predictor matrices. Therefore, in MPC the closed loop performance of the system heavily depends on the accuracy of the process model, which is utilized to design the predictor. Hence, modeling is considered as the most challenging and time consuming work in MPC design. On the other hand, in model-free approach the predictor matrices are determined directly from I/O data by using the data block-Hankel matrices that are defined in SIM with no need to obtain the parametric model of the process and the identification step. Hence, model-free approach is also called data-driven approach. In model-free approach the predictor is derived from the subspace I/O matrix equations of SIM in (2.47), (2.48) and (2.49), which is called Subspace Predictor. Therefore, the model-free approach is obtained based on combination of the Subspace Predictor and Predictive Control which is called SPC. Figure 2.3 illustrate the difference between model-based approach and data-driven approach for predictive control. Main advantage of SPC over MPC is its capability to on-line adapt the SPC gains by collecting new I/O data and updating the Subspace Predictor matrices in data-driven manner. This feature makes SPC much more appropriate than MPC to control time-varying and nonlinear systems.

2.2.1

Model-based Approach: Model Predictive Control (MPC)

Note that while the concentration of this dissertation is on SPC (Section 2.2.2), the MPC and SPC methods share the same cost function and SPC has been introduced after MPC. This section concentrates on formulation of MPC and the provided cost function, control law and constraint of this section are used in SPC approaches. There are different MPC techniques in the literature, and some of the most popular MPC algorithms are Dynamic Matrix Control (DMC) [13], Quadratic Dynamic Matrix Control (QDMC) [14], and Generalized Predictive Control (GPC) [15]. In general, all of the MPC algorithms have three basic elements:

27

CHAPTER 2. BACKGROUND

Model-based Approach Input-Output Data

Process Model

Data-driven Approach

Predictor Matrices

Controller Design

Figure 2.3: Comparing model-based approach and data-driven approach in MPC Prediction Models MPC is a model-based optimization technique, however, the models are not directly used to design the controller. Instead, they are used to design the predictors, and then the control strategy is obtained according to the predictor. In MPC the predictor is multi-step ahead not just one-step ahead, therefore, the system output follows a trajectory of the reference input. Moreover, in MPC future outputs are predicted by considering the future inputs, not only by past inputs and outputs. Some of the typical models used in MPC are [98]:

- Step response model, which is used in DMC algorithm [98].


y (k ) =
i=1

gi u(k - i)

(2.90)

where gi s are the sampled unit step values,  = 1 - q -1 with q -1 as the back shift operator, and u(k ) = u(k ) - u(k - 1). In step response modeling there is no need to a priori information about the system, and complex dynamics such as non-minimum phase or time-delays can be described easily. However, it requires large number of parameters, and it is limited to stable systems without

28

CHAPTER 2. BACKGROUND integrators. The step response model is used in DMC algorithm to obtain the prediction model. Assume that the system is stable and Ns is the settling time in terms of samples. The step response model in (2.90) can be written as,
Ns 

y (k ) =
i=1

gi u(k - i) +
i=Ns +1

gi u(k - i)

(2.91)

using the step response model in (2.91) the prediction of the output at the instant k until the instant k + Np under the effect of Nc control actions can be obtain as follows,
 y ^f = yf + Guf

(2.92)

 represents free response of where the index f represents future data, y ^f is the predicted output, yf

the system, uf is variation of the future control signal and G is called the dynamic matrix, which contains the system dynamics,  y ^(k + 1|k )   y  (k + 1)   u(k ) u(k + 1) . . . u(k + Nc - 1)        

   y ^(k + 2|k ) y ^f =   . .  .  y ^(k + Np |k )

       y (k + 2)  , y =  f   . .   .   y  (k + Np )

       , uf =       

(2.93)

  g1   g2   .  .  . =  g  Nc  .  .  .  gNp 0 g1 . . . ... ... . . . 0 0 . . . g1 . . .

              

GNp

(2.94)

gNc -1 . . . . . . . . .

gNp -1 . . . gNp -Nc +1

Free response of the system is defined as the response that would be expected if no future actions are taken. Thus y  (k + Np ) is calculated as,
Ns 

y  (k + Np ) =
i=Np +1

gi u(k + Np - i) +
i=Ns +1

gi u(k + Np - i)

(2.95)

29

CHAPTER 2. BACKGROUND

- Transfer function models, which are used in GPC design [98]. y (k ) = B (q -1 ) C (q -1 ) u ( k ) + w(k ) A(q -1 ) A( q - 1 ) (2.96)

These models needs fewer parameters, and applicable for unstable systems. However, structure selection (ARX, ARMAX, BJ, OE) and order of the polynomials is necessary for identification. Transfer function models lead to solve the Diophantine equation in optimization procedure. Also, parametric models with limited number of parameters may not be suitable for some processes and MIMO systems. GPC design starts with identifying an ARIMAX (integrated moving-average) model for the system. ARIMAX model is expressed as, y (k ) = B (q -1 ) C (q -1 ) w(k ) u ( k - 1) + A( q - 1 ) A(q -1 )  (2.97)

Optimal prediction is obtained by recursion of following Diophantine equations,
-1 C (q -1 ) -1 -1 F (q ) = G ( q ) + q di A(q -1 ) A(q -1 ) Gdi (q -1 )B (q -1 ) = Gi (q -1 )C (q -1 ) + q -1 i (q -1 )

(2.98) (2.99)

The Np step ahead prediction equation is written as, y ^(k + Np |k ) = y  (k + Np ) + GNp u(k + Np - 1) + GdNp d(k + Np ) where, the free response of the system, y  (k + Np ), is calculated as, y  (k + Np ) = FNp (q -1 ) TNp (q -1 ) y ( k ) + u(k ) C (q -1 ) C (q -1 ) (2.101) (2.100)

by ignoring the future disturbance term Gdi d(k + i), the multi-step predictor equation is expressed as,
 y ^f = yf + Guf

(2.102)

 represents free response of where the index f represents future data, y ^f is the predicted output, yf

30

CHAPTER 2. BACKGROUND the system, uf is variation of the future control signal,  y ^(k + 1|k )   y  (k + 1)   u(k ) u(k + 1) . . . u(k + Nc - 1)
B ( q -1 ) A(q -1 )

       

   y ^(k + 2|k ) y ^f =   . .  .  y ^(k + Np |k )

       y (k + 2)   , y = f   . .   .   y  (k + Np )

       , uf =       

(2.103)

and G is the dynamic matrix, which contains the step response coefficients of response coefficients of
B (q -1 ) A(q -1 )

or the impulse

.

- State-space models, which became more popular recently due to applicability for stable, unstable and MIMO systems [98], x(k + 1) = Ax(k ) + Bu(k ) y (k ) = Cx(k ) + Du(k ) (2.104) (2.105)

however, process modeling with limited number of parameters is one of the disadvantages of these modeling method. Also, using an observer may complicate the calculations if all of the states are not reachable. Assume state-space model of a MIMO system by considering the system noise and disturbance as below [12], ¯ (k ) + Bu ¯ (k ) + B ¯d d(k ) x ¯(k + 1) = Ax ¯x y (k ) = C ¯(k ) (2.106) (2.107)

¯  Rn1 ×n1 , B ¯  Rn1 ×m and C ¯  Rl×n1 are the state-space matrices for system model, where A x ¯(k )  Rn1 ×1 is the state variable vector, d(k ) is the input disturbance that is a sequence of integrated white noise. Note that from the principle of predictive control the u(k ) cannot affect ¯ = 0 in the system model. the y (k ) at the same time. Thus, D State-space based MPC design starts with deriving the Integrator Embedded State-space (IESS)

31

CHAPTER 2. BACKGROUND model as,   ¯ x(k + 1) y (k + 1)
x(k+1)





=

¯ A ¯A ¯ C

0T n1 ×1 I l ×l
A

 

¯ x(k ) y (k )
x( k )





+

¯ B ¯B ¯ C
B





 u(k ) + 

¯d B ¯B ¯d C
Bw

  w (k ) (2.108)

 y (k ) = 0n1 ×1
C

¯ x(k ) y (k )
x( k )

  (2.109)

Il×l



Therefore the IESS model is obtained as, x(k + 1) = Ax(k ) + B u(k ) + Bw w(k ) y (k ) = Cx(k ) (2.110) (2.111)

The triplet (A, B, C ) is called the augmented model which is used in MPC design. The dimensionality of the augmented state-space system is n = n1 + l. Assuming that the state variable vector x(k ) is available at the sampling time k , the Np step ahead state variables are predicted as, x ^(k + Np |k ) = ANp x(k ) + ANp -1 B u(k ) + · · · + ANp -Nc B u(k + Nc - 1) +ANp -1 Bw w(k ) + ANp -2 Bw w(k + 1|k ) + · · · + Bw w(k + Np - 1|k ) From the predicted state variables, the Np step a head predicted output variables are, y ^(k + Np |k ) = CANp x(k ) + CANp -1 B u(k ) + · · · + CANp -Nc B u(k + Nc - 1) +CANp -1 Bw w(k ) + · · · + CANp -Nc Bw w(k + Nc - 1) Note that, for zero-mean white noise sequence w(k ), the predicted value of w(k + i - 1|k ) at future sample i is assumed to be zero. Therefore, the predictor equation is defined as, y ^f = x(k ) + uf (2.114)

(2.112)

(2.113)

where the index f represents future data, y ^f is the predicted output, uf is the future control

32

CHAPTER 2. BACKGROUND signal variations,  y ^(k + 1|k )   u(k ) u(k + 1) . . . u(k + Nc - 1)        

   y ^(k + 2|k ) y ^f =   . .  .  y ^(k + Np |k ) and  and  are defined as,  CA       ,           =     CB CAB CA2 B . . .

       , uf =       

(2.115)

0 CB CAB . . .

0 0 CB . . .

... ... ... ...

0 0 0 . . .

          

   CA2    =  CA3   . .  .  CANp

(2.116)

CANp -1 B CANp -2 B CANp -3 B . . . CANp -Nc B

Cost Function and Tuning Parameters There are different cost functions for different MPC techniques, but all of them wants to minimize the output deviation from the reference input by optimizing the control signal or its variation. A typical form of such a cost function in MPC is [12, 68, 98],
Np Nc

J (k ) =
i=Nd

(^ y (k + i|k ) - r(k + i)) Q(^ y (k + i|k ) - r(k + i)) +
i=1

T

uT (k + i)Ru(k + i)(2.117)

where r(k ) is the reference signal at the current sample time k , Np is the Prediction Horizon and Nc is the Control Horizon. Nd is usually chosen as one or the system time-delay in terms of samples. Q  RNp l×Np l and R  RNc m×Nc m are the positive semi-definite and positive definite Weighting Matrices for penalizing the tracking error and the incremental control signal, respectively. The weighting matrices and horizons are considered as Tuning Parameters to ensure the stability and enhance performance of the closed-loop system in MPC design. The cost function in (2.117) can be written in vector form as, J = (^ yf - rf )T Q(^ yf - rf ) + uT f Ruf (2.118)

33

CHAPTER 2. BACKGROUND where rf is a vector containing the future reference inputs as,  r(k + 1)        

   r(k + 2) rf =   . .  .  r(k + Np )

(2.119)

In MPC algorithm, at the current sample time k , first the appropriate future input trajectory is calculated based on the past I/O data and the internal model of the system over the control horizon Nc , then the best future output is predicted using the model-based designed predictor over a fixed prediction horizon Np . The prediction horizon needs to be selected large enough to ensure the stability and tracking purposes. This can be achieved by applying following terminal equality constraint [43, 44], e(k + Np + i|k ) = 0, i  0 (2.120)

where e(k + i) = r(k + i) - y ^(k + i|k ) is the tracking error. The Figure 2.4 shows a typical MPC strategy at the time instant k . Note that, the input trajectory remains constant after the control

reference input

f
predicted output past output

yp

Np
k k+1 k+2 k+Ni sample time

up
past input

uf Nc

future input

Figure 2.4: A typical MPC strategy

34

CHAPTER 2. BACKGROUND horizon Nc , which is applied by adding following equality constraint to the cost function [43, 44], u(k + Nc + i) = 0, i  0 (2.121)

In MPC algorithms after calculating the future control trajectory, only its first element is applied to the system. At the next time step k + 1, the whole procedure is repeated by one time step shifting forward of the both horizons.

Control Law and Constraints In MPC the control law, u(k ) = u(k - 1) + u(k ) is calculated via minimizing the cost function and solving a typical Least Squares problem as [12, 43, 44, 98, 105], min
uf

J = (^ yf - rf )T Q(^ yf - rf ) + uT f Ruf y ^f from (2.92) or (2.102) or (2.114) e(k + Np + i|k ) = 0, u(k + Nc + i) = 0, i  0 i  0 (2.122)

subject to

where y ^f could be any of the predictor equations of DMC in (2.92), GPC in (2.102) or state-space based MPC in (2.114). The unconstrained MPC solution is found from, J =0  uf for DMC and GPC:
 uf = (GT QT G + R)-1 GT QT (rf - yf )

(2.123)

(2.124)

and for state-space MPC: uf = (T QT  + R)-1 T QT (rf - x(k )) (2.125)

However, in the presence of active inequality constraints on u(k ), u(k ), y (k ) or y (k ) using the numerical optimization is necessary. Capability to handle the inequality constraints is one of the

35

CHAPTER 2. BACKGROUND main advantages of MPC algorithms. The inequality constraints are specified in the form of, umin  u  umax (2.126)

umin  u  umax ymin ymin  y  ymax

 y  ymax

Therefore the constrained MPC problem is summarized as follows [12, 43, 44, 98], min
uf

J = (^ yf - rf )T Q(^ yf - rf ) + uT f Ruf y ^f from (2.92) or (2.102) or (2.114) e(k + Np + i|k ) = 0, u(k + Nc + i) = 0, vmin  v  vmax i  0 i  0 (2.127)

subject to

where v is any of the constrained variables in (2.126).

2.2.2

Data-Driven Approach: Subspace Predictive Control (SPC)

The data-driven SPC approach is derived via the combination of Subspace predictor and Predictive control technique. SPC has some features, which has made it as one of the most popular control strategies in industry over the past decade [21, 66, 68]. These features are: 1- No need of prior knowledge on the order and model structure of the system, 2- On-line adapting the controller in data-driven manner, 3- Same cost function and same tuning parameters as GPC, 4- No need to solve the Diophantine equation, 5- Based on the reliable linear algebra tools such as SVD and QR decomposition, 6- No need for iterative algorithm to obtain predictor matrices, 7- Easily applicable to MIMO systems. Subspace Predictor Consider the state-space innovation form of discrete LTI system from (2.26) and (2.27). Suppose that measurements of the I/O data, u(k ) and y (k ) are available for k  {1, 2, . . . , N }, and the data is persistently exciting. The I/O data and innovation block-Hankel matrices are constructed as 36

CHAPTER 2. BACKGROUND equations (2.50), (2.51) and (2.52). Note that, to minimize the unwanted effect of noise on the identification of the system, the number of columns in these data block-Hankel matrices should be much larger than the number of rows. Moreover, the input u(k ) is persistently exciting and it is uncorrelated with  (k ), and the number of measurements are large enough (N  ) [18, 21, 68]. Consider the subspace future output matrix equation in (2.48), by substituting equations (2.47) and (2.49) into (2.48) the future output matrix equation is obtained as below,
d M  d d s M  s s Yf = M AM  M Yp +M (M - A M HM )Up + HM Uf +M (M - A M HM )Zp + HM Zf (2.128)

where superscript  represent the Moore-Penrose pseudo-inverse. Since Zf is stationary white noise and for a large enough set of measurements, the optimal prediction of Yf from equation (2.128) is written as a linear combination of past I/O data and future input of the system [18, 21]. Therefore the Subspace Predictor equation is defined as, ^f Y = Lw Wp + Lu Uf (2.129)

where Lw  RM l×M (l+m) and Lu  RM l×M m are the subspace linear predictor coefficient matrices that corresponds to the past I/O data and the future input data, respectively. Wp is the past I/O data matrix which is defined in (2.61). The subspace predictor matrices Lw and Lu are calculated from the data block-Hankel matrices, by solving the following Least Squares problem, 
Lw ,Lu

min Yf - [Lw Lu ] 

Wp Uf

 
2 F

(2.130)

The Least Squares problem can be solved numerically by making the following RQ-decomposition,  Wp   R11 0 0  QT 1     = RT QT  (2.131)

   Uf  Yf

     =  R21 R22 0   R31 R32 R33

  T   Q2  QT 3

^f can be written where RT is a lower triangular matrix and Q is an orthogonal matrix. Therefore, Y

37

CHAPTER 2. BACKGROUND as below,  ^  Y f = [R31 R32 ] R11 0     Wp Uf   (2.132)

R21 R22

where superscript  represent the Moore-Penrose pseudo-inverse. From equations (2.129) and (2.132) the matrices Lw and Lu are obtained as below,  L = [Lw Lu ] = [R31 R32 ]  R11 0   (2.133)

R21 R22

where, Lw and Lu can be represented as [18, 21], Lw = L(: , 1 : M (m + l)) Lu = L(: , M (m + l) + 1 : end) (2.134) (2.135)

assuming that M is large enough (M  ) following results are obtained from subspace identification [18, 21], Lw Wp = M Xf
d Lu = HM

(2.136) (2.137)

Therefore, the linear subspace predictor can describe the system behavior by constructing the subspace matrices directly from I/O data without need to identify the system model. Unconstrained SPC The developed subspace predictor in (2.129) can be combined with MPC cost function in (2.118) to obtain the data-driven approach SPC [18, 21]. The SPC problem is realized by minimization of a cost function under certain constraints on the control signal and the system output. ^f is considered to predict the output. In implementation of SPC, only the leftmost column of Y Selecting the Prediction Horizon, Np , and the Control Horizon, Nc , truncates the subspace predictor coefficient matrices. Therefore, (2.129) is rewritten as, y ^f ~ w wp + L ~ u uf = L 38 (2.138)

CHAPTER 2. BACKGROUND ~ w and L ~ u are defined as follows, where L ~ w = Lw (1 : Np l, :) L ~ u = Lu (1 : Np l, 1 : Nc m) L ^f , Uf and Wp as, Here, y ^f , uf and wp are defined as the leftmost column of matrices Y  y (k - M + 1) . . .              (2.141)            (2.139) (2.140)



y ^(k + 1|k )

    ,   



u(k + 1)

    ,   

   y ^(k + 2|k ) y ^f =   . .  .  y ^(k + Np |k )

   u(k + 2) uf =   . .  .  u(k + Nc )

      y (k - 1)      y (k )   yp = wp =  - - --   up  u(k - M + 1)   .  . .     u(k - 1)  u(k )

In the case of no active constraints, a typical form of cost function can be considered as (2.117) or (2.118). Recall the cost function from (2.118), the SPC control law which is applied to the system will be obtained by replacing the (2.138) in the cost function J in (2.118), and minimizing the cost function [18, 21]. Therefore, the predicted output, in (2.138) needs to be written in terms of incremental input signal as below [21], ~ w wp + F2 L ~ u uf y ^f = F1 y (k ) + F2 L (2.142)

39

CHAPTER 2. BACKGROUND where incremental input signal uf , incremental I/O data wp , F1 and F2 are defined as,  y (k - M + 1) . . .                        



u(k + 1)

    ,   

   u(k + 2) uf =   . .  .  u(k + Nc )

      y (k - 1)      y (k )    yp =  wp =  - - --   up  u(k - M + 1)   .  . .     u(k - 1)  u(k )

(2.143)

    F1 =    

Il Il . . . Il

    ,   

    F2 =    

Il

0

...

0 0 . . .

       

Il Il . . . . . .. . . . . .

(2.144)

Il Il . . . Il

The unconstrained SPC problem is defined as follows, min
uf

J = (^ yf - rf )T Q(^ yf - rf ) + uT f Ruf (2.145) ~ w wp + F2 L ~ u uf y ^f = F1 y (k ) + F2 L

subject to

by replacing the (2.142) in the cost function J in (2.118), and minimizing the cost function, the incremental control law will be obtained as, J =0  uf

~ u )T Q(F2 L ~ u ))-1 (F2 L ~ u )T Q((F2 L ~ w )wp + F1 y (k ) - F1 r(k + 1)) uf = -(R + (F2 L

(2.146)

Remark 2.2.1. Different control signals can be obtained from (2.146) by tuning the SPC parameters Q, R, Np and Nc . Therefore, appropriate choice of these tuning parameters has a significant impact on the performance and stability of the closed-loop system.

40

CHAPTER 2. BACKGROUND The incremental control law in (2.146) can be summarized as below; uf ~ e (y (k ) - r(k + 1)) - K ~ wp wp = -K (2.147)

~ e and K ~ wp are defined as, where SPC gain matrices K ~ u )T QF1 ~ u ))-1 (F2 L ~ u )T Q(F2 L ~ e = (R + (F2 L K ~ wp K ~ u )T Q(F2 L ~ u ))-1 (F2 L ~ u )T Q(F2 L ~w) = (R + (F2 L (2.148) (2.149)

Since, at each time instant only the first element of uf is considered to calculate the next control input, then by truncating the first m rows in (2.146), the control law at time instant k + 1 is obtained as, ~e (y (k ) - r(k + 1)) - k ~w wp u(k + 1) = -k p u(k + 1) = u(k ) + u(k + 1) (2.150) (2.151)

~e and k ~w are defined as the first m rows of the gain matrices K ~ e and where the controller gains k p ~ wp , respectively. By using this control action zero steady-state tracking error is achievable, even K for systems with no integrator. Note that above-mentioned SPC control law in (2.147) is obtained directly from subspace matrices, with no need to any system modeling and identification. Figure 2.5 shows a typical block diagram of data-driven SPC approach. Constrained SPC One of the main features of MPC and SPC is their ability to include constraints in the control algorithm. In real processes, there are constraints that are raised due to physical limitation of the system and safety concerns. Generally, in MIMO systems, the constrained SPC problem is considered as a Quadratic Programming (QP) problem and solved by a QP function solver to

41

CHAPTER 2. BACKGROUND

Tuning parameters and Constraints
control signal

reference input

Optimization Procedure

System

output

u(k) y(k)

 (k)
predicted output

Predictor Matrices

Subspace Predictor

SPC
Figure 2.5: A Typical Subspace Predictive Control System obtain the most optimum control law [21, 68]. The constrained SPC problem is defined as, min
uf

J = (^ yf - rf )T Q(^ yf - rf ) + uT f Ruf ~ w wp + F2 L ~ u uf y ^f = F1 y (k ) + F2 L e(k + Np + i|k ) = 0, u(k + Nc + i) = 0, vmin  v  vmax i  0 i  0 (2.152)

subject to

where v is any of the constrained variables, which are defined in (2.126). To formulate the constrained SPC problem in QP form the SPC cost function needs to be revised as follows: The SPC cost function in (2.118) can be rewritten by using the predicted output in (2.142) and reference input in (2.119) as below, ~ w wp + F2 L ~ u uf + F1 (y (k ) - r(k + 1)))T Q(F2 L ~ w wp + F2 L ~ u uf + F1 (y (k ) - r(k + 1))) J = (F2 L (2.153) + uT R  u f f by expanding (2.153) and removing the terms that are not related to uf the QP at every instant

42

CHAPTER 2. BACKGROUND will be obtained as [21, 68], min J
uf T = uT f H uf + P uf

(2.154)

where H and P are defined as, ~ u) + R ~ u )T Q(F2 L H = (F2 L P ~ u )T Q(F2 Lw wp + F1 (y (k ) - r(k + 1))) = 2(F2 L (2.155) (2.156)

The constraints for SPC are rewritten as, F4 uf -F4 uf uf -uf ~ u uf F2 L ~ u uf -F2 L ~ u uf L ~ u uf -L      F3 umax - F3 u(k ) -F3 umin + F3 u(k ) F3 umax -F3 umin ~ w  wp F1 ymax - F1 y (k ) - F2 L ~ w  wp F1 ymax - L ~ w  wp -F1 ymax + L (2.157)

~ w  wp  -F1 ymax + F1 y (k ) + F2 L  

where ufmax = F3 umax , ufmin = F3 umin and yfmax = F1 ymax , yfmin = F1 ymin , and ufmax , ufmin , yfmax and yfmin are defined in a similar way. F3 and F4 are defined as,     F3 =     Im Im . . . Im      ,        F4 =     Im 0 ... 0 0 . . .        

Im Im . . . . . .. . . . . .

(2.158)

Im Im . . . Im

43

CHAPTER 2. BACKGROUND These constraints in (2.157) are written in a single inequality as follows [21, 68],  F4   F3 umax - F3 u(k )                    

   -F4    Im    -Im   ~u  F2 L   ~u  -F2 L   ~u  L  ~u -L

    -F3 umin + F3 u(k )       F3 umax         -F3 umin  uf     ~ w  wp   F1 ymax - F1 y (k ) - F2 L     ~ w  wp   -F1 ymax + F1 y (k ) + F2 L     ~ w  wp   F1 ymax - L   ~ w  wp -F1 ymax + L

(2.159)

AQP uf  BQP

(2.160)

Therefore, the constrained SPC problem is solved by considering the following QP formulation [21, 68], min
uf T J = uT f H uf + P uf

(2.161) AQP uf  BQP

subject to

The optimization can be done by using any of the standard commercially available QP optimization code at each sample time. Then the calculated control signal u(k + 1) is applied to the system. Constrained SPC has more computational load than unconstrained SPC. However, capability of constraint handling is one of the most attractive features of the SPC in industrial applications, since all of the real processes has some practical limitations on their parameters.

2.3

Subspace-based Approach for a Priori Knowledge Extraction

SIM is a data-driven black-box approach, but it has the capability to determine or estimate some of the system characteristics, such as system order, impulse response, step response, static gain and time-delay. These characteristics are derived directly from I/O data and can be used as a priori knowledge when they are required.

44

CHAPTER 2. BACKGROUND Impulse/Step Response Estimation Recall the Subspace Predictor equation in (2.129), ^f Y = Lw Wp + Lu Uf (2.162)

it has been shown that assuming that the number of row blocks, M in data block-Hankel matrices is large enough (M  ) following result is obtained [18, 21],
d Lu = HM

(2.163)

where Lu is the subspace predictor coefficient matrix which is calculated in (2.133) from the I/O
d is a lower data block-Hankel matrices by solving the Least Square problem in (2.130), and HM d from (2.58), it can be seen triangular block Toeplitz matrix in (2.58). Considering the matrix HM d contains the true impulse response coefficients (IRCs) (Markov parameters) that first column of HM

of the system for M horizon.       d = HM (:, 1) =      D CB CAB . . . CAM -2 B           

¯M h

(2.164)

Moreover, true step response coefficients (SRCs) for M horizon are obtained as,       d = F2 HM (:, 1) =      D D + CB D + CB + CAB . . . D + CB + CAB + · · · + CAM -2 B           

g ¯M

(2.165)

45

CHAPTER 2. BACKGROUND where F2 is defined as (2.144). Therefore, first M samples of IRCs and SRCs of the system can be estimated from the first column of matrix Lu as below, ^M h g ^M = Lu (:, 1) = F2 Lu (:, 1) (2.166) (2.167)

Time-delay and Static Gain Estimation By estimating the IRCs and SRCs from (2.166) and (2.167), if there is a time-delay of Td in I/O data ^ M and g (or Nd in terms of samples) the first Nd elements of the h ^M must be approximately zero. Therefore, the time-delay can be estimated from (2.166) and (2.167) by applying an appropriate threshold. Moreover, static gain of the system and its sign can be determined from the estimated SRCs in (2.167). Order Estimation Consider the Subspace Predictor equation in (2.129). In this equation Lu and Lw are subspace predictor matrices, which are calculated in (2.133) from the I/O data block-Hankel matrices by solving the Least Square problem in (2.130) via QR decomposition in (2.131). From the equation (2.133), Lw can be obtained as,
-1 -1 R21 )R11 Lw = (R31 - R32 R22

(2.168)

In SIM framework, order of system can be estimated from the Singular Value Decomposition (SVD) of subspace predictor coefficient matrix Lw [62],  Lw = [U1 U2 ]  1 0 0 2     V1T V2T   (2.169)

In practical applications Lw is not a rank deficient matrix, but it can be approximated as,
T Lw  U1 T 1 V1

(2.170)

which is a rank deficient matrix. Consequently, order of the system is estimated by applying an appropriate threshold and determining the number of dominant singular values, 1 in (2.169) [62].

46

CHAPTER 2. BACKGROUND Moreover, it was shown that the following SVD can give better estimation of the system order than (2.168) [20] [106], 
-1 R31 - R32 R22 R21 = [U1 U2 ] 

S1 0

0 S2

   

V1T V2T

  (2.171)

the order n will be the number of dominant singular values S1 in (2.171).

2.4

Fuzzy Gain-Scheduling Procedure (FGS)

Most industrial processes are nonlinear and time-varying systems, their dynamic behavior changes with the operating conditions of the process. A commonly used scheme to deal with this issue is applying a Gain Scheduling (GS) technique [99]. In GS technique the controller parameters are updated by monitoring different operating conditions of the process. GS technique was first introduced in about 1950s with application on flight control systems [99], and it is very useful technique to reduce the effects of parameter variation in different processes, such as pH control, fuel-air control in car engine and ship steering [99]. Since the parameter updating procedure is done based on open-loop or pre-programmed way, it is controversial to consider the GS technique as an adaptive system. However, by utilizing and evaluating some auxiliary scheduling variables, which are correlated with the changes in system dynamics the GS technique can be considered as an adaptive controller [99]. In this case there is no need to estimate and identify the system parameters, because the controller parameters can be updated quickly by using the auxiliary scheduling variable values, as soon as a changes are observed in system dynamics. Figure 2.6 shows a typical block diagram of a system with GS technique. Generally, a conventional GS procedure has following designing steps [99]: 1- Determine the suitable auxiliary scheduling variables based on the physical system knowledge.

2- Linearized the non-linear system at about different operating points. These linearized models are utilized to compute the local controller parameters for each of the operating conditions.

3- Apply the GS technique based on measurements of the auxiliary scheduling variables, which is done by monitoring different operating conditions.

47

CHAPTER 2. BACKGROUND

Scheduling variables

Gain Scheduling Algorithm

Control parameters

+ reference input -

Controller

u(k) control signal

System

y(k) output

Figure 2.6: Typical structure of GS technique The main issue in design of GS system is finding suitable auxiliary scheduling variables, which is done in model-based approach. Another drawback of conventional GS technique is that it is based on open-loop compensation method, and if the linearization is not accurate enough, the error can result to closed-loop instability. Moreover, the controller parameters must be determined for many operating points, which is very time consuming. Stability and local performance assessment of each linearized controller must be done for each operating point, and non-local performance evaluation is checked by simulations. On the other hand, there is a fuzzy logic based approach to GS technique which can overcome the disadvantages of conventional GS technique. The approach is called Fuzzy Gain-Scheduling (FGS) technique. There are many successful implementation of FGS in the literature to control the nonlinear and time-varying systems [107­110], and FGS-PID, which is one of the most popular FGS techniques in industry [111­114]. In general, FGS procedure consist of three main parts Fuzzifier, FGS Rules and Defuzzifier as shown in Figure 2.7. In FGS technique, first, the crisp input variables are converted to fuzzy numbers in Fuzzifier, then they are utilized to determine updated controller gains by considering FGS Rules. Finally, the resultant fuzzy numbers representing the controller gains are converted to crisp values by Defuzzifier. A well-known issue in FGS technique is predefined the prescribed ranges of controller parameters. Since the controller parameters in FGS algorithm is considered in normalized format in range

48

CHAPTER 2. BACKGROUND

Fuzzifier
input variables input fuzzy numbers

FGS rules
output fuzzy numbers

Defuzzifier
output variables

Figure 2.7: Typical structure of FGS of zero to one, the output of defuzzifier block should be in the normalized form. Therefore, without having these controller gain ranges (CGRs) recovering the actual values of controller parameters is impossible. Currently, these CGRs are calculated in an ad hoc manner by the designer, which is a complex procedure especially for time-varying and nonlinear systems. The issue was addressed in the literature, and several methods were presented to determine the CGRs in FGS-PID controller [112, 115­118]. For example, in [117] CGRs are determined in FGS-PID by rule of thumb based on extensive simulation studies and by using the gain and period of oscillation at the stability limit under proportional control. The CGRs replaced by proper adjustment rates in [112] to generate the degrees of gain difference. However, the rates still need to be selected by the user according to the several simulation results of FGS-PID. In [116] CGRs were considered as scaling factors for a PID-type fuzzy controller, and a self-tuning method was presented to tune them via tracking error based functions. However, the simulation results show a considerable overshoot at the transient response of the controlled system. An optimal FGS-PID control is also presented in [115] to enhance the performance of conventional PID controller. The method uses Bee Colony Optimization technique to automatically tuning the scaling factors, membership functions and control rules of the FGS-PID controller automatically. However, on-line tuning of all membership functions and control rules is a complex task and demands high computational load. In [118] a Particle Swarm Optimization (PSO) based strategy was provided for tuning scaling factors in a PID-type fuzzy logic controller, by considering a cost function to minimize the maximum overshoot and the integral of absolute error. The approach was compared with standard Genetic Algorithm (GA) approach which shows computational efficiency and converges superiority of the PSO-based algorithm.

49

CHAPTER 2. BACKGROUND

2.5

Particle Swarm Optimization Technique (PSO)

PSO is an intelligent based optimization technique, and belongs to meta-heuristic class of optimization algorithms [119­121], which is inspired by social behavior of animals like ants, fish and birds. It has superiority to other meta-heuristic optimization methods such as GA in terms of complexity and computational load [118, 122]. There are many successful engineering applications of PSO in the literature [123,124]. PSO is a population based technique which contains a swarm of candidate solutions called particles. Each particle has a position in a multidimensional search space of the optimization problem. The search space contains all possible solutions for the optimization problem. PSO searches for the optimum solution among these possible solutions. Mathematical model for motion of particles and updating their position in PSO is described as [121], vi (t + 1) = w(t)vi (t) + r1 c1 (pbi (t) - pi (t)) + r2 c2 (pg (t) - pi (t)) pi (t + 1) = pi (t) + vi (t + 1) (2.172) (2.173)

where, i is the index of the particle, t is the discrete time step and shows the iteration of the algorithm, pi (t) and vi (t) are position and velocity of ith particle in the search space at time step t, respectively. w(t) is the inertia coefficient. r1 and r2 are uniformly distributed random numbers between zero and one. c1 and c2 are positive acceleration coefficients to scale the cognitive component (pbi (t) - pi (t)) and the social component (pg (t) - pi (t)), respectively. pbi (t) is the personal best position for ith particle and pg (t) is the global best position among the members of swarm at iteration t. The personal best position and the global best position, at the next iteration t + 1 are updated as follows [121],   pbi (t) if J (pi (t + 1))  J (pbi (t)) if J (pi (t + 1)) < J (pbi (t)) if J (pbi (t + 1))  J (pg (t)) if J (pbi (t + 1)) < J (pg (t))

pbi (t + 1) =

(2.174)

pg (t + 1) =

 p (t + 1) i   pg (t)  pb (t + 1) i

(2.175)

where J (pi (t + 1)) is the PSO fitness function evaluation for particle i at the iteration t + 1. Suitable selection of inertia coefficient, w(t), is essential to ensure convergence behavior of

50

CHAPTER 2. BACKGROUND optimization problem. The inertia coefficient has effect on narrowing the search area gradually by changing the exploratory mode to an exploitative mode. In practical applications, w normally decreases from wmax = 0.9 to wmin = 0.4 from the beginning to the end of the optimization problem [121]. In each iteration the inertia coefficient is calculated as follows, w(t) = (wmin - wmax ) IT (t) + wmax ITmax (2.176)

where, IT (t) and ITmax are current iteration number and maximum number of iterations, respectively. Since, PSO is an iterative optimization procedure a convergence criteria is required to terminate the iteration. Some of these terminating conditions are such as [125]: Exceeding the maximum number of iteration, finding an acceptable solution, observing no improvement over a number of iteration, achieving to almost zero normalized swarm radius or achieving approximately zero slope in objective function. General principle of PSO algorithm is shown in Figure 2.8.

Initialize PSO parameters and particles

Evaluate the fitness of each particle

Calculate the global best and the personal best

Update particles using global best, personal best and specific formulas

Converge?

No

Yes
End

Figure 2.8: Basic principle of PSO A more generalized PSO algorithm was suggested in [120], which can improve the convergence quality by decreasing the amplitude of the particle movements as they approach to the solution.

51

CHAPTER 2. BACKGROUND This method is also called Constriction PSO, which is formulated as follows, vi (t + 1) = [vi (t) + r1 1 (pbi (t) - pi (t)) + r2 2 (pg (t) - pi (t))] pi (t + 1) = pi (t) + vi (t + 1) where 1 = 2 = 2.05 and  is calculated as, = 2 |2 -  - 2 - 4| (2.179) (2.177) (2.178)

where  = 1 + 2 and  = 1. Parameter  guaranties a decreasing velocity for each particle by increasing the number of iterations [120, 126].

2.6

Summary

In this chapter, an introduction to system identification methods and predictive control were provided. Focusing on open-loop system identification, the Prediction Error Method (PEM) and the Subspace Identification Methods (SIM) were reviewed. A review of model-based approach and data-driven approach of predictive control were presented, and formulation of Model Predictive Control (MPC) and Subspace Predictive Control (SPC) were provided for both unconstrained and constrained cases. A review of subspace-based methods to estimate the impulse response and step response, static gain, time-delay and system order were also included. Moreover, a review of Fuzzy Gain Scheduling (FGS) technique and Particle Swarm Optimization (PSO) procedure were presented.

52

Chapter 3

Stability and Performance of SPC
3.1 Problem Statement and Chapter Summary

In SPC implementation, the SPC gain matrices and control signal are functions of SPC tuning parameters. Therefore, appropriate choice of SPC tuning parameters, horizons and weighting matrices, have a significant impact on stability and performance of SPC closed-loop system. Moreover, the SPC gains are calculated directly by using I/O data, and computational complexity of the optimization algorithm depends on the dimension of the system, which is also affected by the length of SPC horizons. Therefore, it is required to select the SPC horizons in an optimal way to guarantee the closed-loop stability and performance and to minimize the computational load. There exist tuning strategies and stability analysis for MPC, which are based on model-based techniques [48]. These methods need some prior information from the system model and parameters, such as openloop settling time, rise time, delay time or system matrices and only some of them can guarantee the closed-loop stability. However, SPC is a model-free approach, and these model-based techniques are not applicable for SPC. In the best of our knowledge, there is no model-free approach for tuning the SPC horizons. In SPC approach the controller is designed based on subspace predictor matrices Lu and Lw , which are driven directly from I/O data. Therefore, it is necessary to provide an optimal model-free tuning technique for SPC based on subspace predictor matrices that can also guarantee the closed-loop stability. Here, we concentrate on open-loop stable systems, but the systems can be non-minimum phase. In this chapter, SPC closed-loop stability conditions are provided for open-loop stable systems based on SPC gains and SPC cost function tuning parameters. Consequently, SPC stability graph is introduced and feasibility range of prediction horizon is determined to ensure the closed-loop 53

CHAPTER 3. STABILITY AND PERFORMANCE OF SPC stability. Moreover, a technique is provided to calculate the efficient prediction and control horizons to guarantee the closed-loop stability and performance and to minimize the computational load.

3.2

SPC Stability Analysis

Considering the SPC incremental control law from (2.147) and SPC gain matrices in (2.148) and (2.149), this section provides SPC closed-loop stability conditions based on the SPC gains and SPC cost function tuning parameters.

3.2.1

SPC Closed-loop Transfer Function

Assume that a SISO open-loop stable system is described by the following discrete time transfer function, Gol (q -1 ) = y (k ) N (q -1 ) = u(k ) D (q -1 ) (3.1)

where N (q -1 ) and D(q -1 ) are polynomials with unknown coefficients. Note that the system can be non-minimum phase with time-delay. On the other hand, by considering wp in (2.143) the SPC control law in (2.150) can be rewritten as below, ~e (y (k ) - r(k + 1)) - k ~y (q -1 )y (k ) - k ~u (q -1 )u(k ) u(k + 1) = -k p p ~y (q -1 ) and k ~u (q -1 ) are defined as, where k p p ~y (q -1 ) = k ~y q -M +1 + · · · + k ~y q -1 + k ~y k p pM p2 p1 ~u (q -1 ) = k ~u q -M +1 + · · · + k ~u q -1 + k ~u k p pM p2 p1 See Appendix A for calculations. By multiplying both sides of (3.1) by  = 1 - q -1 and using (3.2) the SPC closed-loop transfer function is obtained as follows, Gcl (q -1 ) = y (k ) q -1 N (q -1 ) = r(k + 1) Dcl (q -1 ) (3.5) (3.3) (3.4) (3.2)

54

CHAPTER 3. STABILITY AND PERFORMANCE OF SPC where Dcl (q -1 ) is the closed-loop characteristics polynomial as below, ~-1 D(q -1 ) + q -1 [k ~-1 N (q -1 )k ~y (q -1 ) + k ~-1 D(q -1 )k ~u (q -1 ) + N (q -1 )] (3.6) Dcl (q -1 ) = k p p e e e See Appendix B for calculations. Remark 3.2.1. In MIMO systems with m inputs and l outputs, the transfer function matrix can be considered as follows, y (k ) = Gol (q -1 )u(k ) where y (k )  Rl , u(k )  Rm and Gol (q -1 ) is defined as below,  G11 G12 . . . G1m . . . G2m . .. . . . ... Glm         (3.7)

   G21 G22 -1 Gol (q ) =   . . .  . . .  Gl 1 Gl 2

(3.8)

where Gij is transfer function from j th input to ith output.

3.2.2

SPC Closed-loop Stability

The SPC closed-loop characteristics equation in (3.6) can be rewritten as follows, ~-1 D(q -1 ) + N  (q -1 ) = 0 k e where N  (q -1 ) is defined as, ~-1 N (q -1 )k ~y (q -1 ) + k ~-1 D(q -1 )k ~u (q -1 ) + N (q -1 )] N  (q -1 ) = q -1 [k p p e e and the closed-loop poles are also satisfying the following equation, 1+ ~e N  (q -1 ) k =0  D(q -1 ) (3.11) (3.10) (3.9)

55

CHAPTER 3. STABILITY AND PERFORMANCE OF SPC Structure of (3.11) indicates that the SPC closed-loop characteristics equation can be defined as a negative feedback control system, which includes a compound system with transfer function of, N  (q -1 ) D (q -1 ) and an integral type regulator with transfer function of, ~e k  (3.13) (3.12)

By considering that (1) = 0, DC-gain of the compound system in (3.12), can be derived as, Kco = N (1) N  (1) = = Kol D(1) D(1) (3.14)

where Kco is DC-gain of the compound system in (3.12) and Kol is DC-gain of the open-loop system in (3.1). Theorem 3.2.1. The necessary and sufficient condition for small-gain stability of SPC closed-loop system in (3.5) is, ~e Kol > 0 k ~e and Kol are defined in (2.150) and (3.14), respectively. where k Proof. Applying similar small-gain root-locus analysis as in [127] and [128] to (3.11) proves stability of the SPC closed-loop system [129]. See Appendix C for proof. (3.15)

Remark 3.2.2. Note that in (3.15), only sign of the open-loop DC-gain, sgn[Kol ], is required, which can be determined by the method in Section 2.3. ~e is defined as the first m rows of K ~e is propor~ e , recall (2.148) shows that k Remark 3.2.3. Since, k tional to the weighting matrix Q and inversely proportional to the weighting matrix R. Therefore, ~e can be interpreted as sufficiently large value of R and sufficiently small the concept of small-gain k value of Q. Remark 3.2.4. Given sufficiently small Q  0 and sufficiently large R > 0, the SPC closed-loop system defined in (3.5) is stable, if Np and Nc (Np > Nc ) are selected such that the SPC closed-loop 56

CHAPTER 3. STABILITY AND PERFORMANCE OF SPC stability condition in (3.15) is satisfied. Definition 3.2.1. SPC gains that satisfy (2.148) and (2.149) are called achievable SPC gains, and the achievable SPC gains which ensure SPC closed-loop stability in (3.15) are called feasible SPC gains. Similar definitions are used for MPC in [49].

3.3

SPC Tuning Parameters Selection

The SPC cost function in (2.117) has four tuning parameters: Weighting matrices Q, R, Prediction horizon Np and Control horizon Nc . Appropriate selection of them has significant effect on stability and performance of the SPC controller. Weighting matrices have similar roles and characteristics as in Linear Quadratic (LQ) controllers [48]. However, prediction horizon Np and control horizon Nc are the specific parameters for SPC, as LQ controllers use infinite horizon in the optimization algorithm. This section analyzes the effect of selecting SPC tunning parameters on stability and performance of the closed-loop system.

3.3.1

Weighting Matrices (Q and R)

In SPC cost function weighting matrices Q and R are considered as positive semidefinite and positive definite matrices to penalize tracking error and control signal, respectively. Weights on the tracking error, Q, are used to direct more control effort towards more dominant output signal. Setting larger weight for a particular output leads to a faster, but aggressive response for that particular output. Weighting matrix R is utilized to penalize the control signal. Setting a large penalty for a specific control signal yields a more robust, but slow controller. On the other hand, smaller weights result in more aggressive, but less robust controllers [48].

3.3.2

Prediction Horizon (Np )

Prediction horizon is considered as the number of future samples needed to be computed to minimize tracking error. Np is chosen long enough to ensure that the controlled system output reaches steady state. For practical applications, Np needs to be larger than the control horizon and is chosen at least equal to the open-loop settling time [48, 54]. Good stability properties has been observed by using a large prediction horizon, which leads to a control law that approaches LQ control [61, 130]. However, the use of large Np does not always guarantee the closed-loop stability and may cause numerical instability, and also drastically increases the computational load especially for systems 57

CHAPTER 3. STABILITY AND PERFORMANCE OF SPC with fast dynamics. On the other hand, selecting very short prediction horizon can jeopardize the robustness and stability of the SPC closed-loop system. In SPC approach selecting the prediction horizon Np truncates the rows of subspace predictor matrices Lw  RM l×M (l+m) and Lu  RM l×M m , as shown in (2.139) and (2.140). Therefore, Np cannot be considered larger than M . By considering Nc < Np , the achievable prediction horizon range for the SPC approach is defined as, 1  Nc < Np  M (3.16)

However, not all of the achievable prediction horizons can guarantee the SPC closed-loop stability. Therefore, the feasible range of prediction horizon has to be defined based on SPC closed-loop stability condition in (3.15). Definition 3.3.1. The achievable range of Np that guarantees the SPC closed-loop stability condition in (3.15) is called the Feasible Range (FR) of prediction horizon. Definition 3.3.2. The smallest prediction horizon that ensures the SPC closed-loop stability condition in (3.15) is called Shortest-Feasible-Prediction-horizon (SFP) and is denoted by NSF P . Determine NSFP and FR of Prediction Horizon Table 3.1 provides a technique to obtain the feasible SPC gains based on the SPC stability condition in (3.15) and Remark 3.2.4 for given weighting matrices, Q  0 and R > 0, then to determine the NSF P and FR of Np [129].

Table 3.1: Technique of determining the feasible SPC gains, NSF P and FR of Np Step 1: Determine sgn[Kol ] using open-loop I/O data ~e , for 1  Nc < Np  M and Nc = Np - 1 Step 2: Given Q and R calculate achievable SPC gains k ~e by using SPC closed-loop stability condition in (3.15), Step 3: Obtain the feasible SPC gains k and determine NSF P and FR of Np

58

CHAPTER 3. STABILITY AND PERFORMANCE OF SPC Remark 3.3.1. According to the SPC stability condition in (3.15) and the technique in Table 3.1, the Efficient-Prediction-horizon, NEP must be selected as, NSF P  NEP  M (3.17)

Moreover, choice of NEP depends on the desired closed-loop performance for each system. Here, we propose optimal selection of NEP based on the SPC cost function optimization. The strategy is provided in Table 3.2.

3.3.3

Control Horizon (Nc )

Control horizon determines behavior of the control signal. Selecting a very small control horizon can be computationally more efficient, but deteriorates performance of the closed-loop system [48]. In addition, increasing the control horizon from a certain value has no significant effect on the closed-loop performance, but increases the computational load of the optimization algorithm and closed-loop system [48, 56]. In SPC approach selecting the control horizon, Nc , truncates the columns of subspace predictor matrix Lu  RM l×M m , which is shown in (2.140). The achievable control horizon range for SPC approach is defined as, 1  Nc < Np (3.18)

It can be shown that for sufficiently small Q  0 and sufficiently large R > 0, control horizon does not affect the FR of Np and SPC closed-loop stability. However, Nc has an impact on the performance of the closed-loop system. Selection of Efficient Control Horizon and Prediction Horizon (NEC and NEP ) In the subspace identification literature, by assuming that the number of row blocks, M , in the data block Hankel matrices is large enough (M  ), the subspace predictor coefficient matrix,
d [18, 21]. Recall the matrix H d in (2.58), Lu in (2.135) is considered as the estimate of matrix HM M d is a lower triangular block Toeplitz matrix which contains the impulse it can be seen that HM

response coefficients (Markov parameters) of deterministic inputs. In (2.140) it has been shown that selecting the control horizon truncates the columns of subspace predictor coefficient matrix Lu .
d . To represent an Truncating the columns of matrix Lu is equivalent to rank reduction of matrix HM

59

CHAPTER 3. STABILITY AND PERFORMANCE OF SPC nth order system properly by its Markov parameters, the number of parameters has to be selected large enough, and it is necessary to have at least n Markov parameters to represent the system. Therefore, for an observable and reachable nth order system it needs to have,
d min{ rank (HM )} = n + 1

(3.19)

Therefore, in truncating the columns of matrix Lu , control horizon has to be chosen greater than or equal to n + 1, (Nc  n + 1). Choice of Nc < n + 1 can deteriorate the closed-loop performance. Moreover, it can be shown that increasing the Nc greater than NSF P does not have significant ~e and the performance. However, it reduces efficiency of the effect on the feasible SPC gain k control algorithm by increasing the computational load. Proposition 3.3.1. In SPC approach for closed-loop system in (3.5), to avoid performance deterioration, and for computational efficiency, the Efficient-Control-horizon, NEC , is suggested to be selected as follows by efficiently minimizing dimension of the subspace predictor matrix Lu [129],   NEC = n + 1  N EC = NSF P if NSF P  n + 1 if NSF P > n + 1

(3.20)

Remark 3.3.2. Since, SPC is a model-free approach, the system order, n, can be estimated by applying Subspace-based order estimation method in Section 2.3. Full discussion is provided in Chapter 5. Table 3.2 provides an efficient methodology to select the SPC horizons based on the technique in Table 3.1, and definitions of NEC in (3.20) and NEP in (3.17).

60

CHAPTER 3. STABILITY AND PERFORMANCE OF SPC

Table 3.2: Technique of determining the efficient SPC horizons, NEC and NEP Step 1: Given Q and R determine NSF P and FR of Np by using the technique in Table 3.1 Step 2: Determine NEC based on the proposed criteria in (3.20). Step 3: Given Q and R, determine NEP by optimizing the SPC cost function in (2.118) as below, min
Np

J = (^ yf - rf )T Q(^ yf - rf ) + uT f Ruf given Q  0, R > 0 Nc = NEC NSF P  Np  M vmin  v  vmax

subject to

where v is any of the constrained variables in (2.126).

3.4

Simulation Results

In this section, five examples are presented to demonstrate the proposed SPC stability condition by applying the technique in Table 3.1. Simulation results show the effectiveness of the proposed technique in Table 3.2 to select the efficient SPC horizons and stable SPC design. The selected systems have been extensively used in simulations to validate the controller performance [20,21,40, 45, 49, 53, 55, 131, 132].

3.4.1

Systems Description

System I: Non-minimum Phase Fourth-order System For the first system an open-loop I/O dataset (N = 2000, M = 60 and Ts = 0.5sec) is collected from the following forth-order (n = 4) SISO non-minimum phase stable system [20],  0.603 0.603 0 0   0.9238        

   -0.603 0.603 0 0 A=   0 0 -0.603 -0.603  0 0 0.603 -0.603

     2.7577   , B=     4.3171   -2.6436

(3.21)

61

CHAPTER 3. STABILITY AND PERFORMANCE OF SPC C= -0.5749 1.0751 -0.5225 0.1830 , D= -0.7139 (3.22)

System II: Non-minimum Phase Second-order System Here, open-loop I/O dataset with N = 2000, M = 60 and Ts = 0.5sec is collected from the following second-order (n = 2) SISO high stable non-minimum phase system [45],  A= 1 -0.25 1 0   1 0   , C= -1 1.2 , D= 0 (3.23)

 , B=

System III: Non-minimum Phase Fourth-order System In this case a fourth-order SISO non-minimum phase stable system is selected to collect open-loop I/O dataset for N = 2000, M = 60 and Ts = 0.1sec [40],     A=    1.5310 -0.6914 0.0995 -0.0256 1 0 0 0 1 0 0 0 1 0 0 0   1 

        0   , B=       0     0

(3.24)

C=

-0.0876 -0.1897 0.2947 0.0758

, D=

0

(3.25)

System IV: SISO time-delayed system Consider following second-order plus time-delay system from [55], Gol (s) = e-50s (150s + 1)(25s + 1) (3.26)

Sampling time of Ts = 5sec is selected for System IV, and discretized transfer function of the system, by using zero order holder, is obtained as, Gol (z -1 ) = z -11 0.003087 + 0.002856z -1 1 - 1.786z -1 + 0.7919-2 (3.27)

Open-loop I/O dataset is collected from the discretized system (N = 2000 and M = 150).

62

CHAPTER 3. STABILITY AND PERFORMANCE OF SPC System V (Distillation System): MIMO time-delayed system The Wood-Berry distillation column system is a Methanol-Water distillation column, which is a continuous time two-input two-output process with large time-delay. Transfer function of this MIMO model is [133],   Y1 (s) 

12.8e-s

-18.9e-3s 16.7s+1 14.4s+1

 (3.28)

    =  16.7s+1   6.6e-7s Y2 (s)

    U1 (s)    - 3 s  U2 (s) -19.4e

10.9s+1

In this system, y1 , y2 are two outputs, which are the overhead product compositions and the bottom product composition, respectively. The outputs are controlled by manipulating the flux flow rate and steam flow rate as inputs, u1 and u2 [133]. Sampling time of Ts = 1min is selected, and discretized transfer function matrix of the system, by using zero order holder, is obtained as follows,   Gp (z -1 ) =  

z z

-2 -8

0.744 1-0.9419z -1 0.5796 1-0.9123z -1

z z

-4 -4

-0.8789 1-0.9535z -1   -1.302 1-0.9329z -1

 (3.29) 

Open-loop I/O data for all inputs and outputs are collected for this discretized MIMO system (N = 2000, M = 80). Note that open-loop I/O data for all of the systems are obtained by using PRBS of magnitude 2 for the inputs u(k ).

3.4.2

Stability Analysis and Determination of NSFP and FR of Np

FR of prediction horizon and NSF P for each system are calculated by applying the technique in Table 3.1 as follows: Step1: Sign of DC-gain, sgn[Kol ], for each open-loop system is determine by using I/O data. The DC-gain signs are provided in Table 3.3 for SISO systems, and Table 3.4 shows the sign of static gains for each subsystem of the MIMO Distillation system. Note that, as shown in (3.15), only sign of the DC-gain is used not the exact value.

~e , are calculated for each Step2: Given weighting matrices Q and R, the achievable SPC gains, k

63

CHAPTER 3. STABILITY AND PERFORMANCE OF SPC

Table 3.3: Sign of DC-gain for open-loop SISO systems System sgn[Kol ] I II + III + IV +

Table 3.4: Sign of DC-gain for each subsystem of Distillation System Subsystem sgn[DC-gain] Gp11 + Gp21 + Gp12 Gp22 -

system for achievable prediction horizon range, 1  Nc < Np  M , and Nc = Np - 1. ~e , in terms of achievable preFigures 3.1, 3.2, 3.3 and 3.4 show the achievable SPC gains, k diction horizon range for each SISO system. We denoted these graphs as SPC stability graphs. The SPC stability graphs for each subsystem of MIMO Distillation System are shown in Figure 3.5.

SPC Stability Graph -0.01 NSFP = 2 ke = - 0.0156

Achievable SPC gain, ke

-0.03

-0.05

-0.07

-0.09

0

10

20 30 40 Prediction Horizon, Np

50

60

Figure 3.1: System I: SPC stability graph for Q = I , R = 100I and Nc = Np - 1

Remark 3.4.1. Note that, for simplicity and without loss of generality, for all simulation the weighting matrices are selected as, Q = I and R = 100I for SISO systems, and Q = 0.01I and 64

CHAPTER 3. STABILITY AND PERFORMANCE OF SPC R = 400I for Distillation System, otherwise they can be chosen any positive semi-definite and positive definite matrices.
SPC Stability Graph 0.1

0.08

Achievable SPC gain, ke

0.06

0.04

0.02 NSFP = 9 k = 0.0026
e

0

-0.02

0

10

20 30 40 Prediction Horizon, Np

50

60

Figure 3.2: System II: SPC stability graph for Q = I , R = 100I and Nc = Np - 1

SPC Stability Graph 0.1

0.08

Achievable SPC gain, ke

0.06

0.04

0.02 NSFP = 11 ke = 0.0017 0

-0.02

0

10

20 30 40 Prediction Horizon, Np

50

60

Figure 3.3: System III: SPC stability graph for Q = I , R = 100I and Nc = Np - 1 Step3: From the plotted SPC stability graphs in Figures 3.1, 3.2, 3.3 and 3.4, the NSF P and FR of ~e ], for each Np are determined by comparing the sgn[Kol ], and sign of achievable SPC gains, sgn[k 65

CHAPTER 3. STABILITY AND PERFORMANCE OF SPC
SPC Stability Graph 0.1

0.08

Achievable SPC Gain, ke

0.06 8 6 4 2 0 -2

x 10

-4

NSFP=12

0.04

0.02

2

4

6

8

10

12

0 Nd=11 -0.02 0 50 Prediction Horizon, N
p

100

150

Figure 3.4: System IV: SPC stability graph for Q = I , R = 100I and Nc = Np - 1 SISO system, to satisfy the SPC closed-loop stability condition in (3.15). For MIMO Distillation System the sign of DC-gain of each subsystem must be considered. The NSF P for each system and subsystems are shown in Figures 3.1, 3.2, 3.3, 3.4 and 3.5. For System I, from Table 3.3 the DC-gain is negative, Kol < 0, and according to the stability ~e < 0. Therefore, from the stability graph in Figure 3.1 all achievable SPC gains are negative, k ~e Kol > 0, all achievable SPC gains are feasible. As a result, FR of Np for condition in (3.15), k System I is equal to the achievable range of Np , and NSF P = 2. However, for Systems II, III and IV, from Table 3.3, the DC-gains are positive, Kol > 0, and the SPC stability graphs in Figures 3.2, 3.3 and 3.4 indicate that not all of the achievable SPC gains can satisfy the stability condition in (3.15). According to the stability graphs in Figures 3.2 and 3.3 the value of NSF P for Systems II and III are NSF P = 9 and NSF P = 11, respectively. Therefore, the FR of Np for Systems II and III can be obtained as 9  Np  M and 11  Np  M , respectively. For System IV the time-delay can also be determined from SPC stability graph in noise-free cases. From SPC stability graph in Figure 3.4 we obtain Nd = 11, NSF P = 12 and FR of Np is 12  Np  M . The results for each SISO system have been shown in Table 3.5. For MIMO Distillation System, from the SPC stability graphs in Figure 3.5 and sign of the DC-gains in Table 3.4, NSF P and FR of Np are determined as Table 3.6 for each subsystem. Note that from Figure 3.5 it can be seen that in subsystem Gp21 for Np > 38 the SPC gain ke becomes negative. Therefore, FR of Np for subsystem Gp21 will be 9  Np  38. Moreover, time-delay of

66

CHAPTER 3. STABILITY AND PERFORMANCE OF SPC
SPC Stability Graphs 5 x 10
-3

Gp11 0.5

x 10

-3

Gp12 x 10 0 -2 -4 2 Nd=4 N
SFP
-5

Achievable SPC Gain, ke

4 3 x 10 2 1 0 4 2 0 0 0
-3 -5

Achievable SPC Gain, ke

0 -0.5 -1 -1.5 -2 -2.5

Nd=2

=5 4 5

NSFP=3

3

20

1 40 N
p

2 60

3 80

0
-3

20

40 N

60

80

p

1

x 10

Gp21 5

x 10 0

-5

1

x 10

Gp22 x 10 0 -2 -4 2 Nd=4 NSFP=5 3 4 5
-5

Achievable SPC Gain, ke

0.5 0 -0.5 -1 -1.5 4 2 0 2 0 4 20 6 40 Np x 10
-5

-5 38
d

Achievable SPC Gain, ke
80

0 -1 -2 -3 -4 -5

40 N =8

NSFP=9

8 60

0

20

Figure 3.5: Distillation System: SPC stability graphs for each subsystem of System V for Q = 0.01I , R = 400I and Nc = Np - 1 Table 3.5: NSF P and FR of prediction Horizon for each system System NSF P FR of Np I 2 2  Np  M II 9 9  Np  M III 11 11  Np  M IV 12 12  Np  M

40 Np

60

80

each subsystem is also detectable from the SPC stability graphs.

Figure 3.6 shows output of the System III for different feasible and unfeasible Np values. From Table 3.5 the feasible range of prediction horizon for System III is 11  Np  M . Therefore, Figure 3.6 indicates that by selecting the Np from the feasible range of prediction horizon an stable

67

CHAPTER 3. STABILITY AND PERFORMANCE OF SPC

Table 3.6: NSF P and FR of Np for Distillation System (M = 80) Subsystem NSF P FR of Np Gp11 3 3  Np  M Gp21 9 9  Np  38 Gp12 5 5  Np  M Gp22 5 5  Np  M

closed-loop system is achievable. However, for unfeasible prediction horizon values, Np = 5 and Np = 10, the SPC closed-loop system is unstable.
Np = 5 4 2 0
Reference signal & output

N = 10
p

4 2 0
Reference signal & output

-2 -4

-2 -4

0

500

1000 samples Np = 13

1500

2000

0

500

1000 samples N = 20
p

1500

2000

3 2 1 0 -1

3 2 1 0 -1

0

500

1000 samples

1500

2000

0

500

1000 samples

1500

2000

Figure 3.6: System III: Reference signal and output for Q = I , R = 100I , Nc = Np - 1 and different unfeasible and feasible Np values

3.4.3

Choice of Efficient SPC Horizons NEC and NEP

By determining the FR of Np the optimum SPC horizons need to be determined from the feasible values. In this part the efficient SPC horizons, NEC and NEP , are determined for each system 68

CHAPTER 3. STABILITY AND PERFORMANCE OF SPC based on the technique in Table 3.2 as follows:

Step 1: The NSF P and FR of Np are determined for each system by using the technique in Table 3.1. The results are shown in Tables 3.5 and 3.6.

Step 2: The efficient-control-horizon, NEC , can be determined for each system from the proposed criteria in (3.20). For this purpose order of each system needs to be compared with its NSF P . The results are shown in Table 3.7 for each system. Remark 3.4.2. Note that the system order, n, can be estimated using the Subspace-based approach in Section 2.3 or other existing order estimation methods in the literature. More details are provided in Chapter 5.

Table 3.7: System order, NSF P and NEC for each system System order (n) NSF P NEC I 4 2 n+1 II 2 9 NSF P III 4 11 NSF P IV 2 12 NSF P

For System I according to Table 3.7, we have NSF P < n. Therefore, from the criteria in (3.20) we select NEC = n + 1. However, for Systems II, III and IV we have NSF P > n, as a result, we consider NEC = NSF P for these systems. For Distillation System, the overall MIMO system order can be estimated from Subspace-based approach in Section 2.3 by considering the dominant singular values of subspace predictor matrix Lw in (2.169). Figure 3.7 shows the first 20 dominant singular values of Lw for the MIMO Distillation system. Here, the system order can be estimated as n = 1 or n = 2 and in both cases n < NSF P . As a result, from (3.20) we have NEC = NSF P for all subsystems. Therefore, from the technique in Table 3.2 the efficient values of control horizons are obtained for each subsystem as Table 3.8, To verify the efficiency of proposed control horizon selection criteria in (3.20) the stability graphs of Systems I, II and III are plotted in Figures 3.8, 3.9 and 3.10 for 1  Nc  14. The results show that, since Q = I is small enough and R = 100I is large enough, Nc has no effect on the SPC closed-loop stability. 69

CHAPTER 3. STABILITY AND PERFORMANCE OF SPC

Distillation System 35

30

25 Singular values of Lw

20

15

10

5

0

0

5

10 Order

15

20

Figure 3.7: Distillation System: Singular values of Lw for Distillation System Table 3.8: NEC for each subsystem of MIMO Distillation System Subsystem NEC Gp11 3 Gp21 9 Gp12 5 Gp22 5

Moreover, the SPC stability graphs indicate that there is no significant change on the SPC gain, ~e , for Nc > NEC . Therefore, the achievable performance of the SPC closed-loop system remains k the same for Nc > NEC , which verifies the proposed computationally efficient selection criteria of control horizon in (3.20).

Step 3: In this step, the NEP is selected based on the SPC cost function optimization procedure in (3.2). Given Q = I , R = 100I and Nc = NEC the SPC cost function in (2.117) has been evaluated for feasibility range of each system, NSF P  Np  M . From the evaluation result NEP is selected for each system according to the Table 3.9. From the proposed technique in Table 3.2 the efficient values of prediction horizons are obtained 70

CHAPTER 3. STABILITY AND PERFORMANCE OF SPC
SPC Stability Graph 0
Nc=1

-0.05

Nc=2 Nc=3 Nc=4 NEC=5

Achievable SPC gain, ke

-0.1

Nc=6 Nc=7 Nc=8 Nc=9

-0.15

Nc=10 Nc=11 Nc=12 Nc=13 Nc=14

-0.2

-0.25

0

10

20 30 40 Prediction Horizon, N

50

60

p

Figure 3.8: System I: SPC stability graphs for Q = I , R = 100I and 1  Nc  14
SPC Stability Graph 0.35
Nc=1

0.3 0.25

Nc=2 Nc=3 Nc=4 Nc=5 Nc=6 Nc=7 Nc=8 NEC=9

Achievable SPC gain, ke

0.2 0.15 0.1 0.05 0 -0.05

Nc=10 Nc=11 Nc=12 Nc=13 Nc=14

0

10

20 30 40 Prediction Horizon, N
p

50

60

Figure 3.9: System II: SPC stability graphs for Q = I , R = 100I and 1  Nc  14

71

CHAPTER 3. STABILITY AND PERFORMANCE OF SPC
SPC Stability Graph 0.35
Nc=1

0.3 0.25

Nc=2 Nc=3 Nc=4 Nc=5 Nc=6 Nc=7

Achievable SPC gain, ke

0.2 0.15 0.1 0.05 0 -0.05

Nc=8 Nc=9 Nc=10 NEC=11 Nc=12 Nc=13 Nc=14

0

10

20 30 40 Prediction Horizon, Np

50

60

Figure 3.10: System III: SPC stability graphs for Q = I , R = 100I and 1  Nc  14 Table 3.9: Selected NEP by SPC Cost Function Evaluation System NEP I 15 II 30 III 32 IV 50

for each subsystem of the MIMO Distillation System by minimizing the cost function of each subsystem. The results are shown in Table 3.10, Table 3.10: NEP for each subsystem of MIMO Distillation System Subsystem NEP Gp11 32 Gp21 30 Gp12 24 Gp22 16

Figures 3.11 and 3.12 illustrate the SPC cost function evaluation and selection of NEP for Systems III and IV. Moreover, the SPC stability graphs in Figures 3.8, 3.9 and 3.10 show that for Nc = NEC 72

CHAPTER 3. STABILITY AND PERFORMANCE OF SPC
0.04

0.035

SPC Cost Function

0.0157 0.03 0.0157 0.0157 0.025 0.0156 25 0.02 30

NEP = 32

35

40

0.015 10

15

20

25

30

35 N

40

45

50

55

60

p

Figure 3.11: System III: SPC cost function evaluation for Q = I , R = 100I and Nc = NEC = 11
0.045

0.04

0.035 0.0124

SPC Cost Function

0.0124 0.03 0.0124 0.0124 0.025 0.0124 0.0124 0.02 48 50

NEP = 50

52

54

0.015

0.01

0

10

20

30

40

50

60

70 80 Np

90 100 110 120 130 140 150

Figure 3.12: System IV: SPC cost function evaluation for Q = I , R = 100I and Nc = NEC = 12

73

CHAPTER 3. STABILITY AND PERFORMANCE OF SPC increasing the prediction horizon from NEP for each system has no significant effect on the SPC ~e . As a result, it can not enhance the maximum achievable SPC closed-loop performance, gain k but will increase the computational load of the SPC algorithm.

3.4.4

Effect of Weighting Matrices R and Q

According to Theorem 3.2.1 and Remark 3.2.3 to ensure the SPC closed-loop stability the weighting matrices Q and R need to be selected sufficiently small and sufficiently large, respectively. Here, the effect of selecting different weighting matrices R and Q on NSF P and FR of Np is studied, and the results are provided for System III. Effect of Weighting Matrix R In this part effects of selecting different weighting matrices, R, on the SPC closed loop stability and robustness is being studied for System III. In order to study the effects of selecting weighting matrix, R on NSF P and FR of Np , the SPC stability graphs of System III are plotted for different R values. The results are shown in Figures 3.13 and 3.14 for Q = I and Nc = Np - 1 for different weighting matrices R.
SPC Stability Graph 0.3 0.25 0.2

Achievable SPC gain, ke

0.15 0.1 0.05 0 -0.05 -0.1 -0.15 R=1000I R=100I R=10I 0 10 20 30 40 Prediction Horizon, Np 50 60

Figure 3.13: System III: SPC stability graphs for Q = I and Nc = Np - 1

74

CHAPTER 3. STABILITY AND PERFORMANCE OF SPC
SPC Stability Graph 4 3 2

Achievable SPC gain, ke

1 0 -1 -2 -3 -4 R=1I R=0.1I R=0.01I 0 10 20 30 40 Prediction Horizon, Np 50 60

Figure 3.14: System III: SPC stability graphs for Q = I and Nc = Np - 1 Table 3.11 shows NSF P and FR of Np for each R in System III.

Table 3.11: System III: FR of Np for different R, Q = I and Nc = Np - 1 R NSF P FR of Np 1000I 11 11  Np  M 100I 11 11  Np  M 10I 11 11  Np  M I 14 14  Np  M 0.1I 18 18  Np  M 0.01I 22 22  Np  M

The results show that selecting very large values for R increases robustness of the closed-loop system, but does not affect the NSF P and FR of Np . However, for very small values of R, NSF P increases drastically and robustness decreases. For example, Figure 3.14 shows that for R = 0.01I ~e , which is not acceptable even when closedthere is a significant fluctuation in the SPC gain value, k loop system is stable. As a result, robustness and stability of the closed-loop system decreases by decreasing the values of R, which needs to be selected large enough to ensure the stability and performance, however selecting very large values of R may result a slow response. Note that this behavior was expected based on our SPC stability analysis in Theorem 3.2.1 and similar results 75

CHAPTER 3. STABILITY AND PERFORMANCE OF SPC can be obtained by analyzing other systems. Effect of Weighting Matrix Q To investigate the effect of different weighting matrices Q on NSF P and FR of Np , the SPC stability graphs are considered for R = 100INc ×Nc and Nc = Np - 1. Figures 3.15 and 3.16 show the SPC stability graphs of System III for different values of matrix Q.
SPC Stability Graph 0.1 Q=1I Q=0.1I Q=0.01I

0.08

Achievable SPC gain, ke

0.06

0.04

0.02

0

-0.02

0

10

20 30 40 Prediction Horizon, Np

50

60

Figure 3.15: System III: SPC stability graphs for R = 100I and Nc = Np - 1 The results show that selecting very large values for Q decreases both robustness and FR of Np and increases the NSF P . Therefore, the closed-loop stability can be jeopardized by selecting large Q weighting matrices. On the other hand, for small value of matrix Q, robustness increases, but NSF P and FR of Np remain same. Table 3.12 shows NSF P and FR of Np for each value of matrix Q for System III.

3.4.5

Performance Evaluation

In order to verify the effectiveness of the proposed technique in Table 3.2, the designed SPC by proposed technique of SPC horizons selection is applied to System IV and MIMO Distillation System. In this simulation, the following SPC tuning parameters and constraints are selected for 76

CHAPTER 3. STABILITY AND PERFORMANCE OF SPC
SPC Stability Graph 1.5

1

0.5
Achievable SPC gain, ke

0

-0.5

-1 Q=10I Q=100I Q=1000I 0 10 20 30 40 Prediction Horizon, Np 50 60

-1.5

-2

Figure 3.16: System III: SPC stability graphs for R = 100I and Nc = Np - 1 Table 3.12: System III: FR of Np for different Q, R = 100I and Nc = Np - 1 Q NSF P FR of Np 1000I 18 18  Np  M 14 14  Np  M 100I 10I 11 11  Np  M I 11 11  Np  M 0.1I 11 11  Np  M 11 11  Np  M 0.01I

each system, System IV: System V: Q = I, R = 100I, Nc = NEC = 12, Np = NEP = 50, -0.1  u  0.1, -3  u  3

Q = 0.01I, R = 400I, Nc = NEC = 9, Np = NEP = 32, -0.1  u  0.1, -1  u  1

Figures 3.17, 3.18, 3.19 and 3.20 illustrate the SPC closed-loop system response of designed SPC and the control signal for System IV and MIMO Distillation System. The figures show the effectiveness of the proposed SPC design method in improving stability and performance of the both SISO and MIMO systems, and the fact that control signals remain in the prescribed constraints limit.

77

CHAPTER 3. STABILITY AND PERFORMANCE OF SPC

3

Reference signal & output

2.5 2 1.5 1 0.5 0 reference input output 0 500 1000 samples 1500 2000

Figure 3.17: System IV: Reference input and output for Q = I , R = 100I , Nc = NEC = 12 and Np = NEP = 50

3

Control signal

2 1 0

u umax

0

500

1000 samples

1500

2000

Incremental control signal

0.1 0.05 0 -0.05 -0.1 0 500 1000 samples 1500 2000 u u u
min max

Figure 3.18: System IV: Control signal u, its variation u and the constraints for Q = I , R = 100I , Nc = NEC = 12 and Np = NEP = 50

3.4.6

Effect of Prediction Horizon

In order to verify the effectiveness of the proposed technique in Table 3.2, the designed SPC by proposed technique of SPC horizons selection is applied to SISO Systems II and III. In this 78

CHAPTER 3. STABILITY AND PERFORMANCE OF SPC

1.5

1

0.5

Reference signals & outputs

0

refrence input1 y1 0 200 400 600 800 1000 samples 1200 1400 1600 1800 2000

-0.5

1.5 reference input2 y2

1

0.5

0

-0.5

0

200

400

600

800

1000 samples

1200

1400

1600

1800

2000

Figure 3.19: Distillation System: Reference inputs and outputs y1 and y2 for Q = 0.01I , R = 400I , Nc = NEC = 9 and Np = NEP = 32 simulation, the following SPC tuning parameters and constraints are selected for each system, System II: System III: Q = I, R = 100I, Nc = NEC = 9, Np = NEP = 30, -0.1  u  0.1, -1  u  1 Q = I, R = 100I, Nc = NEC = 11, Np = NEP = 32, -0.1  u  0.1, -1  u  1

Figures 3.21 3.22, 3.23 and 3.24 show the output of the SPC closed-loop system for a constant reference signal and SPC control signal and its variation for Systems II and III. Here, different values of feasible Np s are considered to compare the performance. The figures confirm the efficiency of the proposed method in selecting the SPC horizons. According to the Figures 3.23 and 3.21 for Np < NEP performance is deteriorated, but for Np > NEP there is no significant changes in output of the systems. Moreover, Figures 3.22 and 3.24 show that the control signals and their variation do not violate the determined constraint values.

3.4.7

Effect of Disturbance and Noisy Data

In order to investigate performance of the proposed SPC design method in the presence of noisy data, noisy version of the System IV and MIMO Distillation System are considered as follows,

79

CHAPTER 3. STABILITY AND PERFORMANCE OF SPC

0.2 0.15 0.1

Control signals 0.06 0.04 0.02

u1

2

0 -0.02

0.05 0 -0.05 -0.04 0
-3

u
500 1000 samples 1500 2000

-0.06

0
-3

500

Control signals variation 2 1 0 x 10

1000 samples

1500

2000

4 3 2

x 10

 u1

1 0 -1 -2 0 500 1000 samples 1500 2000

 u2

-1 -2 -3 -4 -5 0 500 1000 samples 1500 2000

Figure 3.20: Distillation System: Control signals u1 , u2 and their variations u1 , u2 for Q = 0.01I , R = 400I , Nc = NEC = 9 and Np = NEP = 32 System IV: SISO time-delayed noisy system The transfer function of System IV in Section 3.4.1 with additive noise [55], Y (s) = e-50s U (s) + Ew (s) (150s + 1)(25s + 1) (3.30)

where Ew is a Gaussian noise with zero mean and variance of  2 . Appropriate variances are selected for simulation to obtain the desired SNRs. Figure 3.25 shows the SPC stability graphs of System IV for noisy data with SN R = 0db and SN R = 10db for Q = I and R = 100I . From the SPC stability graph in Figure 3.25 the NSF P and FR of Np are determined based on the SPC stability condition in (3.15) to ensure the closed-loop stability. Efficient values of control and prediction horizons are determined by applying the technique in Table 3.2. For SN R = 0db

80

CHAPTER 3. STABILITY AND PERFORMANCE OF SPC

0.6 0.5 0.4 0.3 0.2 0.1 0 -0.1 -0.2 input N =16
P p

Reference signal & output

N =20 NEP=30 Np=40 Np=50 Np=60 0 50 100 150 200 samples 250 300 350 400

Figure 3.21: System II: Reference signal and output for Q = I , R = 100I , Nc = NEC = 9 and different values of feasible Np we have n + 1 < NSF P , then NEC = 16, and for SN R = 10db we have n + 1 > NSF P , therefore, NEC = 3. The results are shown in Table 3.13 for each SNR. Table 3.13: NSF P , FR of Np , NEC and NEP for System IV from Noisy Data (M = 150) SNR NSF P FR of Np NEC NEP 0db 16 16  Np  M 16 35 10db 2 2  Np  M 3 37

Figures 3.26 and 3.27 show the SPC closed-loop system responses for this noisy system with the following SPC parameters and constraints for each SNR case, SN R = 0db : SN R = 10db : NEC = 16, NEC = 3, NEP = 35, NEP = 37, Q = I, Q = I, R = 100I, R = 100I, -1  u(k )  1, -0.1  u  0.1 -1  u(k )  1, -0.1  u  0.1

The results show the effectiveness of the proposed SPC design method even in the presence of noisy data. 81

CHAPTER 3. STABILITY AND PERFORMANCE OF SPC

0.8
Control signal

0.6 0.4 0.2 0 0 50 100 150 200 samples 250 300 350 400 N =16
P

Np=20 NEP=30 Np=40 Np=50 Np=60

Incremental control signal

0.1 0.05 0 -0.05

0

50

100

150

200 samples

250

300

350

400

Figure 3.22: System II: SPC control signal and its variation for Q = I , R = 100I , Nc = NEC = 9 and different values of feasible Np

0.5

Reference signal & output

0.4

0.3

0.2 input Np=16 Np=20 NEP=32 Np=40 Np=50 Np=60 0 50 100 150 200 samples 250 300 350 400

0.1

0

-0.1

Figure 3.23: System III: Reference signal and output for Q = I , R = 100I , Nc = NEC = 11 and different values of feasible Np

82

CHAPTER 3. STABILITY AND PERFORMANCE OF SPC

0.5 0.4
Control signal

0.3 0.2 0.1 0 NP=16 NP=20 0 50 100 150 200 samples 250 300 350 400 NEP=32 NP=40 NP=50 NP=60

0.06
Incremental control signal

0.04 0.02 0 -0.02 -0.04

0

50

100

150

200 samples

250

300

350

400

Figure 3.24: System III: SPC control signal and its variation for Q = I , R = 100I , Nc = NEC = 11 and different values of feasible Np Distillation System: MIMO time-delayed system Disturbance added Distillation System model in Section 3.4.1 [53],    Y1 (s) 

12.8e-s

-18.9e-3s 16.7s+1 14.4s+1

  



3.8e-8s

    Ed (s)   (3.31)

   =  16.7s+1   6.6e-7s Y2 (s)

   U1 (s)    +  10.9s+1    4.9e-3s -19.4e-3s  U2 (s)

10.9s+1

13.2s+1

83

CHAPTER 3. STABILITY AND PERFORMANCE OF SPC

SPC Stability Graph, SNR = 0db

0.1 Achievable SPC Gain, ke 0.08 x 10 0.06 6 0.04 0.02 0 -0.02 4 2 0 -2 5 0 50 10 15 20 150 100 Prediction Horizon, Np NSFP=16
-3

SPC Stability Graph, SNR = 10db 0.12 Achievable SPC Gain, ke 0.1 0.08 0.06 0.04 0.02 0 0 50 10 5 0 2 4 6 8 10 150 x 10
-3

NSFP=2

100 Prediction Horizon, Np

Figure 3.25: System IV: SPC stability graphs from noisy I/O data for SNR=0db and SNR=10db where Ed is an independent white noise with covariance of diag {4.4582, 4.9898}. The discretized disturbance model using zero order holder with Ts = 1min is obtained as,   Gd (z -1 ) =  

z

-9

z

-4

0.331 1-0.9123z -1   0.3575 1-0.927z -1

 (3.32) 

To determine the FR of prediction horizon, which guarantees the SPC closed-loop stability, 84

CHAPTER 3. STABILITY AND PERFORMANCE OF SPC

SNR = 0db 3 Reference signal & output

2

1

0

Reference input output 0 500 1000 samples SNR = 10db 1500 2000

-1

3 Reference signal & output 2.5 2 1.5 1 0.5 0 -0.5 0 500

Reference input output 1000 samples 1500 2000

Figure 3.26: System IV: Reference input and output for noisy system for SNR=0db and SNR=10db the SPC stability graphs need to be plotted for each subsystem. Figure 3.28 shows the SPC stability graphs from I/O data of System V with disturbance for each subsystem with R = 400I and Q = 0.01I . The NSF P and FR of Np are determined based on the SPC stability condition in (3.15) by using the SPC stability graphs to ensure the closed-loop stability. The results are shown in Table 3.14. Note that from Figure (3.28) it can be seen that in subsystem Gp12 for Np > 66 the SPC gain ke becomes positive. Therefore, FR of Np for subsystem Gp12 will be 8  Np  66.

Therefore, by applying the proposed technique in Table 3.2 the efficient values of control and 85

CHAPTER 3. STABILITY AND PERFORMANCE OF SPC

SNR = 0db 3.5 3 2.5 2 1.5 1 0.5 0 0 500 1000 samples SNR = 0db 0.15 0.1 0.05 0 -0.05 -0.1 0 500 1000 samples 1500 1500

Control Signal 3.5 3 2.5 2 1.5 1 0.5 2000 0 0 500

SNR = 10db

1000 1500 samples SNR = 10db

2000

Incremental Control Signal 0.15 0.1 0.05 0 -0.05 -0.1 2000 0 500 1000 1500 samples 2000

Figure 3.27: System IV: Control signal u and its variations u for noisy system for SNR=0db and SNR=10db Table 3.14: NSF P and FR of Np for Distillation System with Disturbance (M = 80) Subsystem NSF P FR of Np Gp11 5 5  Np  M Gp21 2 2  Np  M 8 8  Np  66 Gp12 Gp22 2 2  Np  M

prediction horizons are determined as NEC = 8 and NEP = 30. Figures 3.29 and 3.30 show the SPC closed-loop system responses for this noisy MIMO system with the following SPC parameters

86

CHAPTER 3. STABILITY AND PERFORMANCE OF SPC

SPC Stability Graphs x 10 20 Achievable SPC Gain, k 15 10 5 0 2 0 -2 2 0 20 40 Np G
p21 e
-4

Gp11
e

x 10 Achievable SPC Gain, k 5

-4

x 10 6 4 2 0 -2 -4

-5

Gp12

N 2

SFP

=8 6 8 x 10 2 1 0 -1 66
-5

0

x 10

-5

4

NSFP=5

-5

3

4 60

5

-10 20 40 Np G x 10

67 60

68 80

1 Achievable SPC Gain, ke 0.8 0.6

x 10

-3

0 Achievable SPC Gain, ke -1 -2 -3 -4 -5 80

x 10

-3

p22

-5

0 -2 -4 2 3 4 5 NSFP=2

x 10 0.4 0.2 0 4 2 0

-5

NSFP=2 2 4 40 Np 6 60 8

0

20

0

20

40 Np

60

80

Figure 3.28: Distillation System: SPC stability graphs for each subsystem of System V with disturbance and constraints, NEC = 8, NEP = 30, Q = 0.01I, R = 400I, -1  u(k )  1, -0.1  u  0.1

Remark 3.4.3. Comparing the results of noise-free and noisy scenario shows that different SPC stability graphs and different FR of Np are obtained in those scenarios. This observation proves generality and effectiveness of the proposed data-driven SPC stability analysis.

87

CHAPTER 3. STABILITY AND PERFORMANCE OF SPC

2 1 Reference signals & outputs 0 -1 Reference input1 y1 0 200 400 600 800 1000 1200 samples 1400 1600 1800 2000

1.5 1 0.5 0 Reference input2 y2

-0.5

0

200

400

600

800

1000 1200 samples

1400

1600

1800

2000

Figure 3.29: Distillation System: Reference inputs and outputs y1 and y2 for System V with disturbance

3.5

Summary

In this chapter the necessary and sufficient condition to assure the SPC closed-loop stability was provided for open-loop stable systems. Based on the derived SPC stability condition, a model-free technique (Table 3.1) was provided to determine the shortest-feasible-prediction-horizon (NSF P ) and the feasibility range (FR) of prediction horizon that guarantees the SPC closed-loop stability. Moreover, the criteria to optimally determine the efficient control horizon (NEC ) and efficient prediction horizon (NEP ) were suggested based on efficiently minimizing the dimension of the subspace predictor and optimizing the SPC cost function. Consequently, the technique in Table 3.2 was proposed for tuning SPC horizons based on the technique in Table 3.1, and provided NEC and NEP . The proposed techniques were evaluated in simulation on three SISO non-minimum phase stable systems, a SISO time-delayed noisy system and a MIMO time-delayed system with disturbance. For each of the systems our stability criteria successfully provided the necessary and sufficient FR of Np and NSF P . Consequently, applying the proposed model-free technique on each system determined the efficient SPC horizons among the feasibility range by minimizing the desired SPC cost function as well. The simulation results show effectiveness of the proposed SPC design 88

CHAPTER 3. STABILITY AND PERFORMANCE OF SPC

0.3 0.2 u1 0.1 0 -0.1 0 x 10 500
-3

Control signals 0.1 0.05
2

0 -0.05

4 2  u1

-0.1 1000 1500 2000 0 500 1000 1500 samples samples Control signals variation -3 x 10 5

u

2000

 u2 0 -2 0 500 1000 1500 samples 2000

0

-5

0

500

1000 1500 samples

2000

Figure 3.30: Distillation System: Control signals u1 , u2 and their variations u1 , u2 for System V with disturbance method.

89

Chapter 4

SPC Gains Updating: PSO-based FGS-SPC
4.1 Problem Statement and Chapter Summary

Main advantage of SPC is its capability to on-line adapting the SPC gains directly from I/O data, which makes SPC appropriate to control time-varying and nonlinear systems. In this SPC gain updating procedure, the subspace linear predictor matrices, Lu and Lw , and subsequently, the ~ e and K ~ wp , must be calculated off-line by using equations (2.133), controller gain matrices, K (2.148) and (2.149) before the controller implementation. In order to adapt the controller with any parameter uncertainties in the time-varying or nonlinear systems, the SPC matrices must be updated by applying randomly generated persistent excitation (PE) signals to the system and capturing the new I/O data from the system during the closed-loop control. After that, to obtain the ~ e and K ~ wp , have new incremental control law from equation (2.150) the controller gain matrices, K to be recalculated by using the equations (2.148), (2.149) and by considering the new updated Lu and Lw matrices. However, this procedure is very time consuming and demands high computational load, which increases by system dimension [68]. On the other hand, applying the PE input signals to the system can interrupt the system operation at the steady-state mode and may cause a biased estimate when applying to the closed-loop system [100, 134]. It will be advantageous for SPC to ~ e and K ~ wp without the mentioned drawbacks. update the controller gains K In this chapter, a Particle Swarm Optimization-based Fuzzy Gain-Scheduling (PSO-based FGS) method is proposed to update the SPC gains for time-varying systems in the presence of constraints and noise. The method is denoted by PSO-based FGS-SPC. In the proposed method, instead of up90

CHAPTER 4. SPC GAINS UPDATING: PSO-BASED FGS-SPC dating the predictor matrices by applying the PE signals, only SPC gains are re-tunned by applying PSO-based FGS technique. Consequently, the decrease in computational load makes the proposed method more suitable and time efficient for industrial applications. Moreover, the proposed PSObased FGS technique overcomes the problem of controller gain ranges (CGRs) calculation in FGS algorithm. In PSO-based FGS-SPC the CRGs are determined optimally by minimizing the SPC cost function.

4.2

Re-arranging SPC Gain Formula

Considering the incremental control law for unconstrained SPC in equation (2.150), this equation can be rewritten as below [135],  ~e (y (k ) - r(k + 1)) - k ~w  u(k + 1) = -k p  ~e e(k ) - [k ~y k ~u ]  = -k p p yp up  yp up     (4.1)

(4.2)

where e(k ) is the tracking error between the piecewise constant reference input and the output, ~y and k ~u are defined as [135], and k p p ~y k p ~u k p ~w (:, 1 : M l) = k p ~w (:, M l + 1 : M (m + l)). = k p (4.3) (4.4)

~w has been defined in Section 2.2.2 as the first m rows of the matrix K ~ wp in (2.149). Here, and k p by considering a piecewise constant reference input, variation of the tracking error, e(k ), can be obtain as follows, e(k ) = e(k ) - e(k - 1) = (y (k ) - r(k + 1)) - (y (k - 1) - r(k )) = y (k ) - y (k - 1) =  y (k ) (4.5) (4.6) (4.7) (4.8)

91

CHAPTER 4. SPC GAINS UPDATING: PSO-BASED FGS-SPC Then, the vector of incremental past I/O data, wp in (2.143) can be modified as 4.9. This rearranging of the standard SPC control signal formula enables us to illustrate flexibility of SPC technique in automatically updating the controller gains [135].  y (k - M + 1) . . .   e(k - M + 1) . . .               ep =    up         

      y (k - 1)      y (k )   yp =  wp =  - - --   up  u(k - M + 1)   .  . .     u(k - 1)  u(k )

            e(k - 1)       e(k )     = - - --       u(k - M + 1)     .   . .         u(k - 1)   u(k )

(4.9)

Here, the control law in equation (2.151) is obtained when the constraints are not active, but it can be applied to the constrained SPC by using the formulation in Section 2.2.2. By considering the incremental control law in (4.2) and (4.9), it can be seen that Ke is the ~y is the proportional gain of its variations, proportional gain of the tracking error, e(k ), and k p ep . Therefore, this part of the controller has similar structure as the Proportional-Derivative (PD) controller. Remark 4.2.1. Note that, the vector ep is not only the variation of error at the current time instant, but also it consists of the variation error for past output data. This feature makes the controller much more sophisticated than a simple PD control algorithm. ~e and k ~y can be easily updated in realMotivated by this observation the controller gains k p time by applying one of the existing advanced model-free GS techniques [136­139] with no need to apply PE signals and update the subspace linear predictor matrices. Here, a Particle Swarm Optimization-Based Fuzzy Gain-Scheduling (PSO-based FGS) method is proposed to update the ~e and k ~y . The method is denoted by PSO-based FGS-SPC. SPC gains k p ~u , in (4.2), is the proportional gain of current and past variaRemark 4.2.2. The SPC gain, k p ~u can also be updated tions of controller signal, up , in closed-loop system. Therefore, the gain k p according to e(k ) and wp at each time instant by considering the aforementioned PSO-based FGSSPC algorithm. 92

CHAPTER 4. SPC GAINS UPDATING: PSO-BASED FGS-SPC

4.3

PSO-based FGS-SPC

~e and k ~w in (2.150) by applying Particle Swarm OptimizationUpdating the SPC controller gains k p Based Fuzzy Gain-Scheduling (PSO-based FGS) method is proposed in this section. Unlike the existing SPC methods, the proposed algorithm does not need applying PE signals to update the ~e and subspace linear predictor matrices, Lu and Lw . Instead, it re-calculates the controller gains k ~w based on the PSO-based FGS technique. The proposed technique can optimally calculate k p the controller gain ranges (CGRs) for FGS-SPC by minimizing the SPC cost function via the PSO algorithm. As a result, the PSO-based FGS-SPC can eliminate the time consuming and disruptive procedure of applying PE signals to the system, and updating the SPC gains optimally by improving the time efficiency. These features of PSO-based FGS-SPC method make it a suitable approach for industrial applications.

4.3.1

FGS-SPC Procedure

~e , k ~y and k ~u can be updated In the proposed PSO-based FGS-SPC method, the SPC gains k p p real-time by applying a fuzzy-logic based GS algorithm, [117], [112]. In this scheme, it is assumed that the controller gains are in the following prescribed ranges [135], ~e  [k ~e,min , k ~y k p ~u k p ~y ,min ,  [k p ~u ,min ,  [k p ~e,max ] k ~y ,max ] k p ~u ,max ] k p (4.10) (4.11) (4.12)

~e,min , k ~e,max , k ~y ,min , k ~y ,max , k ~u ,min and k ~u ,max are called the controller gain ranges where k p p p p (CGRSs). In FGS-SPC the CGRs are vectors with compatible dimensions as the corresponding gains. ~e , k ~y and k ~u , are normalized into the range between zero For convenience, the SPC gains, k p p and one by considering the following linear transformations, ¯e = k ¯y k p ¯u k p = = ~e - k ~e,min k ~e,max - k ~e,min k ~y - k ~y k
p

(4.13) (4.14) (4.15)

p,min

~y ~ k p,max - kyp,min ~u - k ~u k
p p,min

~u ~ k p,max - kup,min 93

CHAPTER 4. SPC GAINS UPDATING: PSO-BASED FGS-SPC ¯e , k ¯y and k ¯u , are the normalized SPC gains. where k p p In FGS-SPC strategy, the normalized SPC gains are determined based on the tracking error, e(k ), and the variation of the past I/O data vector, wp , by a set of rules of the form, ¯e is CCi , k ¯y is DDi and wp (j ) is BBi , then k p ¯u is EEi and k p

if e(k ) is AAi

where, AAi , BBi , CCi , DDi and EEi are Fuzzy Subsets for input and output variables. The fuzzy subsets are also called Membership Functions, and the Membership Degree is denoted by µAAi (X ) for each variable like X . In general, FGS procedure consists of three main parts: Fuzzifier, FGS Rules and Defuzzifier as shown in Figure 2.7. First, the crisp input variables are converted to fuzzy numbers in fuzzifier, then they are used to determine updated controller gains by considering GS rules. Finally, the resultant fuzzy numbers representing the controller gains are converted to crisp values. Fuzzification The procedure of converting crisp input variables to fuzzy numbers is called fuzzification. Here, the fuzzy subsets are in triangular shape with seven different fuzzy subsets representing linguistic fuzzy variables negative big (NB), negative medium (NM), negative small (NS), zero (ZZ), positive big (PB), positive medium (PM), and positive small (PS). The shape of the fuzzy subsets are all the same for e(k ), yp , and up . However, the minimum and the maximum ranges are different for them, and selected according to the process characteristics [135, 140]. The fuzzy subsets of input variables, e(k ), yp and up are shown in Figure 4.1.
NB NM NS ZZ PS PM PB

emin epmin upmin

0 0 0

emax epmax upmax

Figure 4.1: Fuzzy subsets for e(k ), ep and up

94

CHAPTER 4. SPC GAINS UPDATING: PSO-BASED FGS-SPC These subsets for each input variable like X can be represented as below,                      1 if X  Xmin

µ N B (X ) =

3 Xmin X

2 - 2 if Xmin < X  3 Xmin

(4.16)

0

if

otherwise

µN M ( X ) =

                    

-3 Xmin X

+ 3 if

2 Xmin < X  3 Xmin

3 Xmin X

- 1 if

2 3 Xmin

<X 1 3 Xmin

(4.17)

0

if

otherwise

µN S ( X ) =

                    

-3 Xmin X

+ 2 if

2 3 Xmin

<X 1 3 Xmin <X0 (4.18)

3 Xmin X

if

1 3 Xmin

0

if

otherwise

µZZ (X ) =

                    

-3 Xmin X

+ 1 if

1 3 Xmin

<X0

-3 Xmax X

+ 1 if 0 < X  1 3 Xmax

(4.19)

0

if

otherwise

95

CHAPTER 4. SPC GAINS UPDATING: PSO-BASED FGS-SPC                     
3 Xmax X

if

0<X 1 3 Xmax
1 3 Xmax

µ P S (X ) =

-3 Xmax X

+ 2 if

<X 2 3 Xmax

(4.20)

0

if

otherwise

µ P M (X ) =

                    

3 Xmax X

- 1 if

1 3 Xmax

2 <X 3 Xmax

-3 Xmax X

+ 3 if

2 3 Xmax

< X  Xmax

(4.21)

0

if

otherwise

µP B ( X ) =

                    

1

if

Xmax  X

3 Xmax X

- 2 if

2 3 Xmax

< X  Xmax

(4.22)

0

if

otherwise

where X is representing one of the input variables e(k ), yp , and up . Note that, here Xmin = -Xmax and the input variables yp , and up are considered in a single vector form of wp = [yp up ]T . FGS Rules After the fuzzy numbers representing each input variable are obtained, the fuzzy numbers of the ¯e , k ¯y and k ¯u are determined using the rule decision tables. The normalized output variables k p p rule decision table defines the rules to update each of the normalized SPC gains. Generally, the membership degree of normalized output variable Y is obtained by the product of membership degrees of input variables X1 and X2 as below, µi (Y ) = µAAi (X1 ) · µBBi (X2 ) (4.23)

96

CHAPTER 4. SPC GAINS UPDATING: PSO-BASED FGS-SPC ¯e , k ¯y and Tables 4.1, 4.2 and 4.3 show the FGS rules to update the normalized SPC gains, k p ¯u , respectively [135, 140]. Note that, the rules are derived experimentally based on a piecewise k p constant reference input. ¯e Table 4.1: Rule decision table to update k  wp NB NM NS ZZ PS PM PB NB B B B B B B B NM M B B B B B M e(k ) NS S M M M M M S ZZ S S S S S S S PS S M M M M M S PM M B B B B B B PB B B B B B B B

Table 4.2: Rule decision table to  wp NB NM NS ZZ NB S S S S NM M S S S e(k ) NS B M S S ZZ B B M M PS B M S S PM M S S S PB S S S S

¯y update k p PS S S S M S S S PM S S M B M S S PB S M B B B M S

Table 4.3: Rule decision table to  wp NB NM NS ZZ NB B B B M NM B B M M e(k ) NS B M M S ZZ M M S S PS M M S S PM M S S M PB S S M M

¯u update k p PS M S S S M M B PM S S M M M B B PB S M M M B B B

97

CHAPTER 4. SPC GAINS UPDATING: PSO-BASED FGS-SPC Defuzzification In defuzzifier the fuzzy number of normalized SPC gains which are obtained based on FGS rules are converted to the crisp gain values. Here, three different triangular-shape fuzzy subsets with linguistic fuzzy variables big (B), medium (M) and small (S) are defined for the normalized output ¯e , k ¯y and k ¯u . The membership functions of normalized output variables are shown variables, k p p in Figure 4.2 [135, 140].

Degree of membership function

S

M

B

0

0.5 Fuzzy variable

1

¯u ¯e , k ¯y and k Figure 4.2: Fuzzy subsets for k p p In general, the normalized output variable Y corresponding to µi (Y ) is obtained from the following equation, µi (Y )Yi Y =
i

µi (Y )
i

,

i = 1, 2, . . . , number of rules

(4.24)

where Yi is the value of the normalized output variable Y corresponding to the µi (Y ) for ith rule.

In the proposed FGS-SPC method, the output variables can be determined using the following defuzzification formula,
49

¯e )k ¯ei µi ( k (4.25) ¯e ) µi (k

¯e = k

i=1 49 i=1

98

CHAPTER 4. SPC GAINS UPDATING: PSO-BASED FGS-SPC
49

¯w (j ) = k p

i=1

¯w (j ) )k ¯w (j )i µi ( k p p
49 i=1

(4.26) ¯w (j ) ) µi (k p

where wp (j ) = [yp (j ) up (j )]T . ¯e , k ¯y and k ¯u , are obtained, the actual Once the crisp values of the normalized SPC gains, k p p ~e , k ~y and k ~u are calculated from the following equations by having the CGRs, SPC gains k p p ~e = (k ~e,max - k ~e,min )k ¯e + k ~e,min k ~y k p ~u k p ~y ~ ¯ ~ = (k p,max - kyp,min )kyp + kyp,min ~u ~ ¯ ~ = (k p,max - kup,min )kup + kup,min (4.27) (4.28) (4.29)

Figure 4.3 shows block diagram of the PSO-based FGS-SPC system.

PSO-based Fuzzy Gain-Scheduler

Tuning parameters and Constraints
reference input

Optimization Procedure

u(k)
control signal

System

y(k)
output

 (k)
predicted output Predictor Matrices

Subspace Predictor

SPC

Figure 4.3: SPC system with PSO-based FGS

99

CHAPTER 4. SPC GAINS UPDATING: PSO-BASED FGS-SPC

4.3.2

PSO-based FGS Technique

In FGS procedure, the CGRs in (4.10), (4.11) and (4.12) completely depend on the process and may vary by variation of the process parameters [112]. In conventional FGS, the CGRs are usually determined by rule of thumb and the designer's experience at the beginning of the FGS procedure [117], which is a time consuming procedure, especially for time-varying and nonlinear systems. Here, we proposed a PSO-based FGS-SPC technique to optimally-automatically calculate the CGRs and ~e , k ~y and k ~u [140]. update the SPC gains k p p By utilizing the aforementioned Constriction PSO algorithm in (2.177) and (2.178), the CGRs in (4.10), (4.11) and (4.12) can be selected as PSO particles. All PSO particles and parameters ~e , k ~y are initialized according to PSO commendations. At each iteration, the controller gains k p ~u are calculated based on particle information and defuzzification results from the FGS and k p algorithm. Next, the calculated controller gains are used to evaluate the PSO fitness function. Here, SPC cost function in (2.117) is selected as the PSO fitness function. This selection enables us to optimize the CGRs in FGS and SPC cost function, simultaneously. Finally, the global best is updated based on the evaluation results. The algorithm continues until the convergence condition is met. Here, an iteration limit is considered to terminate the PSO algorithm. Moreover, to avoid the increase of the computational time, a criterion is set to terminate the algorithm when there is no significant changes in particle movements. Algorithm 4.3.1 presents the PSO-based FGS-SPC procedure [140]. Remark 4.3.1. Unlike the existing FGS algorithms in PSO-based FGS technique, the CGRs in (4.10), (4.11) and (4.12) are not manually selected as constant values, but they are calculated and updated optimally-automatically at each iteration according to the PSO fitness function. Therefore, the proposed PSO-based FGS technique by itself is a valuable package, which can be applied to any control algorithm that requires CGRs tuning for FGS.

Remark 4.3.2. Note that since the re-arranging of SPC control law in Section 4.2 has been done by considering a piecewise constant reference input; therefore, to met the PSO convergence condition the input constant intervals need to be not smaller than the PSO convergence time.

100

CHAPTER 4. SPC GAINS UPDATING: PSO-BASED FGS-SPC Algorithm 4.3.1. PSO-based FGS-SPC Method Phase I: Calculation of normalized SPC gains via FGS procedure Inputs: Tracking error, e(k ), variation of past output data yp and variation of past control signal up ¯e , k ¯y and k ¯u Outputs: Normalized SPC gains, k p p Step 1: Fuzzification using input membership functions in Figure 4.1 Step 2: Applying fuzzy rules from Tables 4.1, 4.2, 4.3 Step 3: Defuzzification using output membership functions in Figure 4.2 to obtain the normalized ¯e , k ¯y and k ¯u SPC gains k
p p

Phase II: Calculate the optimum CGRs via PSO Procedure ¯e , k ¯y and k ¯u from Phase I, and the presumed CGRs, k ~e,min , Inputs: Normalized SPC gains, k p p ~e,max , k ~y ,min , k ~y ,max , k ~u ,min and k ~u ,max k
p p p p

Outputs: The optimum CGRs Step 1: Initialize PSO parameters and set CGRs as particles Step 2: Calculate SPC gains based on CGRs info and the normalized SPC gains from Phase I using (4.27), (4.28), (4.29) Step 3: Evaluate SPC cost function and update the personal best and the global best Step 4: Check the convergence (the iteration limit or particle movement threshold). If YES goto Phase III. If NO update the CGRs using the new global best and personal best then goto Step 2 Phase III: Calculate actual SPC gains ¯e , k ¯y and k ¯u and the optimum CGRs from Inputs: Normalized SPC gains from Phase I, k p p Phase II ~e , k ~y , k ~u Outputs: The SPC gains k p p Step 1: Calculate SPC gains based on optimally selected CGRs from Phase II by using (4.27), (4.28), (4.29)

4.4

Simulation Results

In this section, we demonstrate the performance and efficiency of proposed PSO-based FGS-SPC technique by applying it to the SISO Dryer System [4, 141].

4.4.1

System Description

Dryer System: SISO Minimum phase fourth-order system The proposed PSO-based FGS-SPC method was tested on data set collected from a SISO laboratory scale "hairdryer" system [4, 141]. In this system, air is fanned through a tube and heated at the inlet. Input of the system is the voltage over the heating device, which is a mesh of resistor wires. Output of the system is the air temperature, which is measured by a thermocouple (or rather the voltage from the thermocouple). The sampling time was set to Ts = 0.08sec throughout the 101

CHAPTER 4. SPC GAINS UPDATING: PSO-BASED FGS-SPC simulation. Figure 4.4 shows the collected open-loop I/O data for this system from [20].
7 6

input

5 4 3

0

200

400 Samples

600

800

1000

7 6

output

5 4 3

0

200

400 Samples

600

800

1000

Figure 4.4: Dryer System: Collected input-output data from open-loop system The open-loop system can be identified as a forth order system by applying subspace identification method [20]. Eigenvalues of the system are located at, 1,2 = 0.4735 ± j 0.2037, 3 = 0.7439, 4 = 0.9475 Remark 4.4.1. In PSO-based FGS-SPC algorithm there is no need to identify the open-loop system. Here, the eigenvalues are only provided to test the robustness of the proposed method in simulation.

4.4.2

Stability Analysis

First, the technique in Table 3.1 is applied to determine the FR of Np and NSF P . The DC-gain is determined from the open-loop I/O data as Kol = 0.97802 > 0. Figure 4.5 shows the SPC stability graph for this system. According to the SPC stability graph and the DC-gain, we have NSF P = 4 and FR of Np is 4  Np  M . In this simulation, the number of row blocks in data Hankel matrices has been chosen as M = 30. Then, the efficient values of prediction horizon and control horizon are determined by using the technique in Table 3.2 as NEC = 5 and NEP = 14.

102

CHAPTER 4. SPC GAINS UPDATING: PSO-BASED FGS-SPC
SPC Stability Graph 0.18 0.16 0.14
Achievable SPC Gain, ke

0.12 0.1 0.08 0.06 0.04 0.02 0 -0.02 0 5 10 15 20 Prediction Horizon, Np 25 30

Figure 4.5: Dryer System: SPC stability graph for Q = I , R = 30I and Nc = Np - 1

4.4.3

Simulation Initialization

The appropriate choice of the initialization parameters has a significant impact on the stability and performance of the designed control system. In this simulation, the controller parameters are considered as follows: - SPC parameters and constraints: Np = NEP = 14 , Nc = NEC = 5 , Q = I , R = 30I , -10  u(k )  10 , -1  u(k )  1 - FGS parameters: emax = -emin = 50 , ypmax = -ypmin = 1 , upmax = -upmin = 0.5 - PSO parameters:  = 1 , 1 = 2 = 2.05 , population size = 100 , iteration limit = 100 In order to eliminate the adverse effect of fluctuations at the output signal, specifically for SPC method, a user-defined tolerance, = 0.1, is applied to the tracking error.

103

CHAPTER 4. SPC GAINS UPDATING: PSO-BASED FGS-SPC

4.4.4

Robustness and Adaptiveness Test

To study the adaptability of the controllers in the presence of the time-varying systems, we assume that there are some changes in the eigenvalues of the system at sampling instants k = 200 and k = 800, respectively, as follows: 1,2 = 0.9238 ± j 0.2441, 3 = 0.2939, 4 = 0.9468 1,2 = 0.9486 ± j 0.2042, 3 = -0.6184, 4 = 0.6096 Figure 4.6 shows the input and output of the closed-loop system for PSO-based FGS-SPC, FGS-SPC and SPC methods with previously mentioned constraints on control signal, u(k ), and its variation u(k ).

4 3.5 reference input PSO-based FGS-SPC FGS-SPC SPC

Reference input and outputs

3 2.5 2 1.5 1 0.5 0

0

500 sample

1000

1500

Figure 4.6: Dryer System: Reference input and outputs for PSO-based FGS-SPC, FGS-SPC and SPC According to the results, by changing the system dynamics at k = 200 and k = 800, all three aforementioned methods start to adapt their controllers with the new conditions. However, Figure 4.6 shows that SPC starts applying PE input signals to identify the new model of the system to update the subspace predictor matrices Lu and Lw , and then calculates the new controller gains ~e and k ~w . It can be seen that this process makes large fluctuations in the output of the system k p 104

CHAPTER 4. SPC GAINS UPDATING: PSO-BASED FGS-SPC and takes a lot of time to capture the new model. Also, overshoots and undesired fluctuations at the step response indicates that SPC also needs to update and re-tune the weighting matrices Q and R to obtain a smooth response for new systems.

4 control signal 3 2 1 0 PSO-based FGS-SPC FGS-SPC SPC

0

500 sample

1000

1500

0.3 control signal variation 0.2 0.1 0 -0.1

0

500 sample

1000

1500

Figure 4.7: System I: Control signal u, and its variation u for PSO-based FGS-SPC, FGS-SPC and SPC Control signal, u(k ), and its variation, u(k ), are shown in Figure 4.7 for each of the control algorithms. The results in Figures 4.6 and 4.7 show that both PSO-based FGS-SPS and FGS-SPC ~e , k ~y and k ~u , much faster than the SPC with less methods can adapt the controller gains, k p p fluctuation, and manage to comply with the specified constraints on u(k ) and u(k ). Moreover, PSO-based FGS-SPC has faster response than the FGS-SPC method due to optimally tunning of the CGRs by using the PSO algorithm.

~e , k ~y and k ~u were adjusted Figures 4.8, 4.9 and 4.10 demonstrates how the controller gains k p p for each of the above-mentioned methods. The fluctuations in tuning of the controller gains for the SPC method is observable in Figure 4.10, which is because of applying PE signals in SPC. Figure 4.9 shows that FGS-SPC technique arranged the controller gains inside a small boundary to ensure the stability of the algorithm

105

CHAPTER 4. SPC GAINS UPDATING: PSO-BASED FGS-SPC

PSO-based FGS-SPC 0.2 0.15 0.1 0.05 0 0.1 0.05 0 -0.05 -0.1 Ky 0 500 sample 1000
p

Ke

0

500 sample 0.15 0.1 0.05 0 1500-0.050

1000

1500

Ku

p

500 sample

1000

1500

Figure 4.8: Dryer System: Updating the controller gains for PSO-based FGS-SPC method
FGS-SPC 0.2 0.15 Ke 0.1 0.05 0

0

500 sample 0.15 0.1

1000

1500

0.1 0.05 0 -0.05 -0.1 Ky 0 500 sample 1000

Ku 0.05 0
p

p

1500

-0.05

0

500 sample

1000

1500

Figure 4.9: Dryer System: Updating the controller gains for FGS-SPC method

106

CHAPTER 4. SPC GAINS UPDATING: PSO-BASED FGS-SPC

SPC 0.22 0.2 K 0.18 0.16 0.14 0.12 0 500 sample 0.3 0.2 0.1 0 -0.1 -0.2 0 500 sample 1000 1500 Ky 0.2 0.15
p

e

1000

1500

Ku

p

0.1 0.05 0 -0.05 0 500 sample 1000 1500

Figure 4.10: Dryer System: Updating the controller gains for SPC method without considering its optimality. Figure 4.8 shows flexibility of the proposed PSO-based FGSSPC method in optimally tuning the controller gains compared to the FGS-SPC method in Figure 4.9. Consequently, PSO-based FGS-SPC can provide both optimal and stable closed-loop system by utilizing the PSO technique. To investigate the robustness of the proposed PSO-based FGS-SPC algorithm in the presence of noisy data, a Gaussian noise with SN R = 25db is added to the I/O data in simulations. Also, the user-defined tolerance is changed to = 0.4 to eliminate the adverse effect of fluctuations in

the presence of noisy data. Figure 4.11 illustrates the input and output of the closed-loop noisy system for each aforementioned method. Although all three methods can adapt the controller gains according to their control algorithm, but having a large overshoot in the response of the SPC method shows that it needs to re-tune the weighting matrices in the SPC cost function. PSObased FGS-SPC has faster response compared to FGS-SPC, and less fluctuations compared to SPC method that shows its superiority with respect to other methods.

107

CHAPTER 4. SPC GAINS UPDATING: PSO-BASED FGS-SPC

PSO-based FGS-SPC 4 3 2 1 0

0

500 FGS-SPC

1000

1500

4 3 2 1 0

0

500 SPC

1000

1500

4 3 2 1 0

0

500 samples

1000

1500

Figure 4.11: Dryer System: Reference input (dashed) and outputs for PSO-based FGS-SPC, FGSSPC and SPC methods for noisy system

4.4.5

Performance Evaluation

In order to compare the performance of each method, performance points are depicted in Figure 4.12. The results show that PSO-based FGS-SPC has the best performance among these three methods. PSO-based FGS-SPC has smaller tracking error variance and variation of control signal variance than SPC method which shows superiority of the proposed method. In comparison with FGS-SPC, the PSO-based FGS-SPC can significantly decrease the variance of the tracking error, 108

CHAPTER 4. SPC GAINS UPDATING: PSO-BASED FGS-SPC but it has a small amount of increase in the variance of the incremental control signal.
0.22 0.21 0.2 0.19 var(r - y) 0.18 0.17 0.16 0.15 0.14 0.13 0.01 0.012 0.014 0.016 0.018 0.02 0.022 var( u) 0.024 0.026 0.028 0.03 PSO-based FGS-SPC FGS-SPC SPC

Figure 4.12: Dryer System: Performance points for each method To show the optimality of proposed PSO-based FGS-SPC method, cost function evaluation, J of the methods for each sample time is depicted in Figure 4.13 as the performance index. Convergence results of the performance index in Part(a) and Part(b) of the Figure 4.13 are shown in Table 4.4. The results show effectiveness and superiority of the proposed PSO-based FGS method with respect to SPC and FGS-SPC methods, and superiority of the proposed technique in terms of optimality. Although SPC method shows faster response than the other two methods, it has large deviation at the beginning and slow convergence pace due to the fluctuation. These fluctuations are generated by applying PE signals, and SPC needs to re-tune its parameters, such as weighting matrices and horizons in the cost function. In some cases, one of which has been shown in Figure 4.14, the fluctuations can result a non-convergence situation for the SPC method. Table 4.4: Dryer System: Performance index value for each method Performance Index PSO-based FGS-SPC FGS-SPC SPC - 16 - 12 Figure 4.13(a) 5.87 × 10 4.33 × 10 5.21 × 10-7 Figure 4.13(b) 1.82 × 10-16 1.06 × 10-4 1.35 × 10-16

To demonstrate the computational time saving feature of the proposed PSO-based FGS-SPC 109

CHAPTER 4. SPC GAINS UPDATING: PSO-BASED FGS-SPC

12 PSO-based FGS-SPC FGS-SPC SPC

10

8 Performance index

6

4

Part (a) 2

Part (b)

0

0

500 sample

1000

1500

Figure 4.13: Dryer System: Performance index value for each of the methods algorithm, Monte-Carlo simulations consisting of 30 runs were conducted for all methods with different row dimensions of data Hankel matrices, M . Figure 4.15 shows the average computational time that is required to complete the updating algorithm through the MATLAB programming. The results show that the computational load increases by increasing the row dimension of the past and future data Hankel matrices. However, the PSO-based FGS-SPC algorithm needs much less computational time than SPC method. In addition, it shows that time efficiency of the PSObased FGS-SPC is much less sensitive to the increasing of Hankel matrix orders. Moreover, Figure 4.15 shows PSO-based FGS-SPC needs more computational time than the FGS-SPC method, which is because of applying the PSO procedure. However, Figure 4.15 also shows that by increasing the M , the gap between two graphs decreases. As a result, for large values of M , the PSO algorithm is computationally more efficient. 110

CHAPTER 4. SPC GAINS UPDATING: PSO-BASED FGS-SPC

4 Reference signal and output 3.5 3 2.5 2 1.5 1 0.5 0 0 500 sample 1000 1500 Reference signal SPC

Figure 4.14: Dryer System: Reference input and outputs for SPC method in non-convergence case
3

Monte-Carlo (30 runs) for algorithms PSO-based FGS-SPC FGS-SPC SPC

10

10

2

computation time

10

1

10

0

10

-1

10

-2

10

-3

0

10

20 30 40 50 M (order of future hankel matrix)

60

70

Figure 4.15: Dryer System: Monte-Carlo simulations consisting of 30 runs for PSO-based FGS-SPC and SPC

111

CHAPTER 4. SPC GAINS UPDATING: PSO-BASED FGS-SPC

4.5

Summary

In this chapter, an algorithm has been provided to eliminate the requirement of disruptive and time consuming process of applying PE signals in SPC. The proposed method, denoted by PSObased FGS-SPC, is based on optimally-automatically tuning of SPC controller gains by applying the proposed PSO-based FGS technique. In this technique, the CGRs of the FGS algorithm are calculated and updated optimally by applying the PSO technique. In PSO-based FGS-SPC, by considering the SPC cost function as PSO fitness function, the SPC controller gains and the CGRs of FGS algorithm are simultaneously tuned. Moreover, the proposed PSO-based FGS technique by itself is a valuable package, which can be applied to any control algorithm that requires CGRs tuning for FGS. Robustness and efficiency of the method have been illustrated by simulation on a time-varying SISO system in the presence of constraints and noisy data. Simulation results indicate the superiority of the proposed PSO-based FGS-SPC method over FGS-SPC and SPC in achieving an optimal solution and in providing a fast and robust tracking performance. In addition, the main advantage of PSO-based FGS-SPC over SPC is that it does not require applying disruptive PE signals; therefore, it provides a time-efficient control strategy.

112

Chapter 5

Time-delay and Order Selection for SIM and SPC
5.1 Problem Statement and Chapter Summary

Since SPC is a black-box data-driven approach, it has no need to collect prior knowledge about the system for designing the controller, making it appropriate for automatic control purposes. However, prediction results and the closed-loop performance can be deteriorated in the presence of noisy I/O data. In this scenario, integrating a priori information about the system characteristics, such as time-delay and system order can help to appropriately select the SPC parameters and enhance the performance. Moreover, order selection and time-delay estimation are still open problems in the SIM framework. There are some threshold-based techniques in the SIM framework to estimate these parameters, which are discussed in Section 2.3. However, the estimation of these parameters from noisy I/O data by threshold-based techniques is a tricky task, and sometimes utilizing more powerful statistic methods would be helpful. In this chapter, we show that using the new approach, denoted by RE-based TDE [142], is a robust method of time-delay estimation that outperforms the existing approaches of time-delay estimation. In addition, we propose to utilize a new method denoted by MSEE [143] for order estimation and show the advantages of this method over the conventional thresholding approaches. It is shown that MSEE also outperforms existing non-thresholding methods.

113

CHAPTER 5. TIME-DELAY AND ORDER SELECTION FOR SIM AND SPC

5.2

Data-driven Approaches for Time-delay Estimation and Order Selection

In this section Reconstruction Error based (RE-based) method and Mean Squared Eigenvalue Error Method (MSEE) are presented for time-delay estimation and order selection for SPC design in the presence of noisy I/O data.

5.2.1

Reconstruction Error based (RE-based) Method

The Reconstruction Error (RE) has been introduced in [144] as below, eRE = 1 ||y ¯-y ^||2 2 N (5.1)

which is the error between the N samples of true unavailable noise-free output data, y ¯, and the estimated output, y ^. The RE-based time-delay estimation (RE-based TDE) method is an statistical data-driven method proposed to estimate the RE for a possible range of time-delays (0  Nd  N ) [142]. Then it determines the optimum time-delay as the one that minimizes the upper bound of the RE in (5.1). The RE has been used in finding the optimum length of the estimated impulse response coefficients (IRCs) in [144]. In the RE-based TDE, the same approach is utilized to estimate the optimum time-delay simultaneously. Therefore, in the RE-based TDE, the optimum IRCs length and the time-delay can be estimated by solving the following minimization problem [142],
 (Nd , L h ) = arg min eRE Nd ,Lh

(5.2)

 is the optimum time-delay estimation, L is the optimum length of IRCs and e where Nd RE is the h

upper bound of the RE in (5.1), which is defined in [142, 144]. Application of RE-based TDE Method for Time-delay Estimation in SIM In Section 2.3, the Subspace-based approach for time-delay estimation has been shown by using the estimated impulse/step response coefficients (IRCs and SRCs) in (2.166) and (2.167) by selecting an appropriate threshold. However, in noisy scenarios the estimated IRCs and SRCs are corrupted by noise; therefore, estimating the time-delay is a tricky task. This issue can be resolved by 114

CHAPTER 5. TIME-DELAY AND ORDER SELECTION FOR SIM AND SPC applying the presented RE-based TDE method to the noisy IRCs of the system. Therefore, correct estimation of the system time-delay by the RE-based TDE method can improve the performance of the designed SPC for time-delay systems.

5.2.2

Mean Squared Eigenvalue Error (MSEE) Method

MSEE method is a statistical data-driven approach to denoise the eigenvalues, which are derived from noisy data [143, 145]. The optimum order selection is based on the calculation of eigenvalue estimates using the available noisy data, then sorting these estimated eigenvalues and thresholding them with a consistent criterion. The proposed criterion is obtained by minimizing the following reconstruction error (RE) [143], eRE = 1 ¯ ^ 2 || - m || N (5.3)

¯ is the vector of the true unavailable sorted noise-free eigenvalues and  ^ m is a vector with where  the first m largest estimated eigenvalues and zero for the rest of the vector. The MSEE method determines the optimum order of the system as the one that minimizes the upper bound of the RE in (5.3). The RE has been first introduced in [144] to determine the optimum length of the estimated IRCs. Therefore, in the MSEE method the optimum order of system can be estimated by solving the following minimization problem [143, 145], m = arg min eRE
m

(5.4)

where m is the optimum order selection and eRE is the upper bound of the RE in (5.3). Application of MSEE for Order Estimation in SIM According to Section 2.3, in SIM framework the system order is defined as the number of dominant singular values of the subspace predictor matrix Lw in (2.168). However, in the presence of noisy data, Lw is not a rank deficient matrix. Since noise magnifies the small singular values of Lw , estimating the system order by selecting an appropriate threshold is not straightforward. In these scenarios, motivated by the MSEE method, the system order can be estimated with sufficiently accuracy for SPC design by applying the presented MSEE method to the noisy singular values of matrix Lw . 115

CHAPTER 5. TIME-DELAY AND ORDER SELECTION FOR SIM AND SPC

5.3

Simulation Results

In this section, the effectiveness of applying the proposed RE-based TDE and MSEE methods in the SIM framework are presented on time-delayed SISO and MIMO processes with noisy data. Moreover, the superiority of presented RE-based TDE and MSEE methods are illustrated over other existing techniques by utilizing them to estimate the time-delay and system order in noisy scenarios.

5.3.1

Systems Description

System IV: SISO time-delayed noisy system Consider the following transfer function of System IV in Section 3.4.1 with additive noise [55], Y (s) = e-50s U (s) + Ew (s) (150s + 1)(25s + 1) (5.5)

where Ew is a Gaussian noise with zero mean and variance of  2 . Appropriate variances are selected for simulation to obtain the desired SNRs. Distillation System: MIMO time-delayed system with disturbance Following disturbance model is added to the Distillation System model in Section 3.4.1 [53],    Y1 (s) 

12.8e-s

-18.9e-3s 16.7s+1 14.4s+1

  



3.8e-8s

    Ed (s)   (5.6)

   =  16.7s+1   6.6e-7s Y2 (s)

   U1 (s)    +  10.9s+1    4.9e-3s -19.4e-3s  U2 (s)

10.9s+1

13.2s+1

where Ed is an independent white noise with covariance of diag {4.4582, 4.9898}. The discretized model of the disturbance channel (by using zero order holder) with Ts = 1min is obtained as,   Gd ( z ) =  

z

-9

z

-4

0.331 1-0.9123z -1   0.3575 1-0.927z -1

 (5.7) 

116

CHAPTER 5. TIME-DELAY AND ORDER SELECTION FOR SIM AND SPC

5.3.2

Impulse/Step Response Estimation from Noisy Data

Estimated IRCs and SRCs for both systems are obtained by using the Subspace-based approach in (2.166) and (2.167). System IV: SISO time-delayed noisy system Figures 5.1 and 5.2 show the estimated IRCs and SRCs for System IV from noisy data for SN R = 0db and SN R = 10db, respectively.

Estimated impulse response , SNR = 0db 0.2
IRCs

0.1 0 -0.1 -0.2 0 100 samples Estimated step response, , SNR = 0db 50 150

1.5 1
SRCs

0.5 0 -0.5 0 50 samples 100 150

Figure 5.1: System IV: Estimated IRCs and SRCs from noisy I/O data for SNR=0db

Distillation System: MIMO time-delayed system with disturbance Figures 5.3 and 5.4 show the estimated IRCs and SRCs for each subsystem of the Distillation System with disturbance by using the Subspace-based approach in (2.166) and (2.167). Here, each subsystem of the MIMO system, Gpij is denoted according to the description in Remark 3.2.1.

117

CHAPTER 5. TIME-DELAY AND ORDER SELECTION FOR SIM AND SPC

Estimated impulse response , SNR = 10db 0.1 0.05
IRCs

0 -0.05 -0.1

0

100 samples Estimated step response, , SNR = 10db

50

150

1.5 1
SRCs

0.5 0 -0.5

0

50 samples

100

150

Figure 5.2: System IV: Estimated IRCs and SRCs from noisy I/O data for SNR=10db

5.3.3

Time-delay Estimation

In the presence of noisy data, the first Nd terms of estimated IRCs and SRCs are corrupted by noise. Therefore, selecting an appropriate threshold to estimate the time-delay is not straightforward, and threshold-based methods may not be applicable. Here, the RE-base TDE method presented in Section 5.2.1 is applied to estimate the IRCs and the time-delay. System IV: SISO time-delayed noisy system Figures 5.5 and 5.6 show the RE-based estimated IRCs for System IV. True IRCs and Subspacebased estimated IRCs are also plotted to compare. It can be seen that the presented RE-based method provides better estimation of IRCs than
 = 11 for Subspace-based approach in Section 2.3, and it can estimate the correct time-delay of Nd

both SNR cases. Moreover, Table 5.1 shows the estimated time-delay for noisy System IV in (5.5) via different 118

CHAPTER 5. TIME-DELAY AND ORDER SELECTION FOR SIM AND SPC

Gp11 1

Estimated Impulse Responses 0.5

Gp12

0.5 IRCs IRCs 0

0

-0.5

-0.5

0

20

40 samples G
p21

60

80

-1

0

20

40 samples G
p22

60

80

1

0.5 0

0.5 IRCs IRCs 0 -1 -0.5 -1.5 0 20 40 samples 60 80 -0.5

0

20

40 samples

60

80

Figure 5.3: Distillation System: Estimated IRCs for each subsystem by Subspace-based method existing time-delay estimation methods, such as Cumulative Sum (CUSUM) method [74], Separating Frequency Method [75] and Subspace based method [76]. The results show that CUSUM overestimates the time-delay, but Freq.-based method underestimates the time-delay drastically. Time-delay is not detectable (N/D) by using the Subspace-based method. Therefore, the results indicate outstanding performance of the presented RE-based TDE method over the other compared methods.

119

CHAPTER 5. TIME-DELAY AND ORDER SELECTION FOR SIM AND SPC

Gp11 15 10 SRCs 5 0 -5

Estimated Step Responses 5 0 SRCs -5 -10 -15

Gp12

0

20

10 8 SRCs

40 samples Gp21

60

80

0

20

40 samples Gp22

60

80

0 -5 SRCs -10 -15 -20

6 4 2 0 0 20 40 samples 60 80

-25

0

20

40 samples

60

80

Figure 5.4: Distillation System: Estimated SRCs for each subsystem by Subspace-based method Table 5.1: Time-delay estimation for System IV from noisy I/O data via different methods SNR RE-based CUSUM Freq.-based Subspace-based 0db 11 17 3 N/D 10db 11 15 6 N/D

Distillation System: MIMO time-delayed noisy system Figure 5.3 and 5.4 show that estimated IRCs and SRCs are highly corrupted due to the additive noise, therefore, time-delay estimation via threshold methods is not a straightforward task. However, by applying the proposed RE-based TDE method the time-delays can be estimated accurately

120

CHAPTER 5. TIME-DELAY AND ORDER SELECTION FOR SIM AND SPC
SNR = 0db, Nd = 11, Lh = 63 0.2 0.15 0.1 0.05 IRCs 0 -0.05 -0.1 -0.15 -0.2 Subspace-based RE-based True IRCs 0 50 N 100 150
* *

Figure 5.5: System IV: True noise-free IRCs and estimated noisy IRCs for SNR=0db
SNR = 10db, H* = 11, L* = 106 d h

0.06

0.04

0.02 IRCs

0

-0.02 Subspace-based RE-based True IRCs 0 50 N 100 150

-0.04

-0.06

Figure 5.6: System IV: True noise-free IRCs and estimated noisy IRCs for SNR=10db

121

CHAPTER 5. TIME-DELAY AND ORDER SELECTION FOR SIM AND SPC for each subsystem. Figure 5.7 shows application of RE-based TDE method for this noisy system. RE-based TDE method estimates the time-delay correctly for each subsystem with no need for applying any threshold. Table 5.2 shows estimated sample-delays for each subsystem from noisy data using RE-based TDE method.
Gp11 1
Subspace-based RE-based

Estimated impulse responses 0.5 0 IRCs -0.5 -1

Gp21

0.5 IRCs 0
Subspace-based RE-based

-0.5

0

20

40 samples Gp12

60

80

-1.5

0

20

40 samples Gp22

60

80

1
Subspace-based RE-based

0.5 0

0.5 IRCs IRCs 0 -1 -0.5 -1.5
Subspace-based RE-based

-0.5

0

20

40 samples

60

80

0

20

40 samples

60

80

Figure 5.7: Distillation System: Estimated IRCs from noisy I/O data for each subsystem (RE-based TDE method and Subspace-based method)

5.3.4

Order Selection

From the literature review in Section 2.3, in Subspace identification framework the order of the system can be estimated from the dominant singular values of the matrix Lw . Here, drawback of this method is shown in the presence of noisy data, and the application of the presented new MSEE

122

CHAPTER 5. TIME-DELAY AND ORDER SELECTION FOR SIM AND SPC

Table 5.2: Estimated Time-delay for Distillation System using RE-based TDE method Subsystem Sample-delay (Nd ) Gp11 2 Gp21 8 Gp12 4 Gp22 4

method is illustrated for order estimation. System IV: SISO time-delayed noisy system Figure 5.8 shows the first 20 singular values of Lw for noisy I/O data with SN R = 0db and SN R = 10db, respectively. It can be seen that in noisy data the small singular values of the matrix Lw are magnified, which makes the order estimation difficult by using threshold-based methods.
SNR = 0db 8 Singular values of Lw 6 4 2 0

0

5

10 Order SNR = 10db

15

20

2 Singular values of Lw 1.5 1 0.5 0

0

5

10 Order

15

20

Figure 5.8: System IV: Singular values of Lw from noisy I/O data for SNR=0db and SNR=10db In these noisy scenarios the order of the system can be estimated accurately by applying the 123

CHAPTER 5. TIME-DELAY AND ORDER SELECTION FOR SIM AND SPC presented MSSE method in Section 5.2.2. Figure 5.9 shows the RE for System IV in SN R = 0db. Order selection is performed based on RE minimization by averaging on 100 trials. It is observed that the MSEE method estimates the order precisely and Figure 5.9 indicates m = 2 for this system for SN R = 0db and SN R = 10db. Therefore, we have n = 2 for System IV.

SNR = 0db 70 60 50 40 30 RE upper bound 20 0 2 4 6 8 10 m SNR = 10db 30 Reconstruction error 12 14 16 18 20

Reconstruction error

m*=2

25 m*=2 20

15

RE upper bound

10

0

2

4

6

8

10 m

12

14

16

18

20

Figure 5.9: System IV: RE upper bound for order selection noisy data with SNR=0db and SNR=10db In addition, Table 5.3 shows the estimated order for System IV in (5.5) from noisy data using two well-known existing order estimation methods, Akaike's Information Criterion (AIC) [71] and Minimum Description Length (MDL) [72]. The results show that MDL underestimates the order for both SNR values. AIC overestimates for SN R = 10db, but by decreasing the SNR value it 124

CHAPTER 5. TIME-DELAY AND ORDER SELECTION FOR SIM AND SPC tends to underestimate the order. The results indicate superiority of presented MSEE method to estimate the system order compared to AIC and MDL. Table 5.3: Time-delay estimation for System IV from noisy I/O data via different methods SNR MSEE AIC MDL 0db 2 1.92 1 10db 2 2.67 1.05

Distillation System: MIMO time-delayed noisy system To estimate the overall order of this MIMO system from noisy data the Subspace-based approach in Section 2.3 can be applied. The dominant singular values of Lw are depicted in Figure 5.10.
14

However, selecting an appropriate threshold is difficult due to the noisy data.
12

10 Singular values of Lw

8

6

4

2

0

0

5

10 Order

15

20

Figure 5.10: Distillation System: Singular values of Lw from noisy data In noisy cases utilizing the presented MSEE method in Section 5.2.2 as a powerful order selection technique would be helpful. Figure 5.11 shows the order estimation from MSEE method. The results show that the MSEE method can estimate the MIMO system order correctly as m = 1 from both available noisy outputs y1 and y2 .

125

Reconstruction error for y1

1.4 1.3

x 10

7

CHAPTER 5. TIME-DELAY AND ORDER SELECTION FOR SIM AND SPC
m =1 1.2 1.1 1
*

RE upper bound

0 x 10
7

2

4

6

8

10 m

12

14

16

18

20

Reconstruction error for y2

1.8 1.6

RE upper bound m =1 1.4 1.2 1
*

0

2

4

6

8

10 m

12

14

16

18

20

Figure 5.11: Distillation System: RE upper bound for order selection of noisy system

5.4

Summary

In this chapter, a statistical approach was utilized to estimate the time-delay, based on Reconstruction Error (RE) in subspace identification framework. The method is denoted by RE-based TDE. Moreover, application of a new statistical approach was presented to estimate the system order. The method is denoted by MSEE. Comparison of the proposed methods with other classical timedelay estimation and order selection methods illustrates advantages of the proposed methods in different SNR scenarios. The results show efficient performance and robustness of RE-based-TDE and MSEE methods even for very low SNR cases.

126

Chapter 6

Conclusion and Future Work
Focus of this dissertation was on analysis and improvement of the Subspace Predictive Control (SPC). By closely analyzing the SPC in Chapter 3, its closed-loop transfer function was derived in general form. This enabled us to obtain the necessary and sufficient condition of small-gain SPC closed-loop stability for open-loop stable systems. As a result, SPC stability graph was introduced to find the shortest-prediction-horizon and feasible range (FR) of prediction horizon, which guarantee the closed-loop stability. Next, the efficient control horizon and prediction horizon were derived based on shortest-feasible-prediction-horizon and by minimizing the SPC cost function. Simulation results illustrated the efficiency of the proposed techniques in SPC parameter selection. On-line adaption of SPC gains was studied in Chapter 4 and the problem of applying persistently exciting (PE) signals in SPC was addressed. This Chapter introduced PSO-based FGS-SPC method to update the SPC gains using FSG rules with no need for applying PE signals. In addition, the proposed PSO-based FGS technique overcame the issue of determining controller gain ranges (CGRs) in FGS method by optimally selecting the CGRs via minimizing the SPC cost function. Simulation results were provided to show the robustness and efficiency of the proposed method. Chapter 5 discussed black-box data-driven SPC approach in the presence of noisy data. Drawbacks of Subspace-based thresholding approaches for order selection and time-delay estimation were shown in this Chapter. Mean Squared Eigenvalue Error (MSEE) method and Reconstruction Error based time-delay estimation (RE-based TDE) method were implemented for order selection and time-delay estimation, respectively. Simulation results show the superiority of presented MSEE and RE-based TDE methods to other existing approaches. This dissertation was taking the initiatives to improve the SPC and provide theoretic and systematic requirement for SPC parameter design. There is a great potential in continuing this 127

CHAPTER 6. CONCLUSION AND FUTURE WORK research direction. For example, extending the SPC stability conditions to open-loop unstable systems, and SPC design by using closed-loop I/O data are interesting possible future research. In addition, it is well worthed to study the design of other parameters of SPC, Q and R, through an optimization approach that is proposed in this dissertation. Lastly, extending the application of PSO-based FGS-SPC method in systems with non-piecewise-constant inputs might be of interest.

128

Appendix A

Re-configuration of SPC Control Law
Recall SPC control law in (2.150), ~e (y (k ) - r(k + 1)) - k ~w wp u(k + 1) = -k p and by considering wp in (2.143)   wp =  yp up   (A.2) (A.1)

the second part of (A.1) can be rewritten as below,  ~w wp = k p ~y k ~u k p p   yp up   (A.3)

~y and k ~u are defined as, where k p p ~y k p ~u k p ~w (:, 1 : M l) = k p ~w (:, M l + 1 : M (m + l)) = k p (A.4) (A.5)

129

APPENDIX A. RE-CONFIGURATION OF SPC CONTROL LAW wp in (2.143) can be rewritten as follows,  y (k - M + 1) . . .   q -M +1 y (k ) . . .                        

      y (k - 1)      y (k )   yp =  wp =  - - --   up  u(k - M + 1)   .  . .     u(k - 1)  u(k ) Therefore, (A.3) is obtained as,

            q -1 y (k )       y (k )     = - - --       q -M +1 u(k )     .   . .         q -1 u(k )   u(k )

(A.6)



~w wp = k p

~y ~y k ~y | k ~u ~u k ~u k ... k ... k pM p2 p1 pM p2 p1

      q -1 y (k )    y (k )    - - --    q -M +1 u(k )   .  . .     q -1 u(k )  u(k )

q -M +1 y (k ) . . .

             (A.7)            (A.8)

~y (q -1 )y (k ) + k ~u (q -1 )u(k ) = k p p ~y (q -1 ) and k ~u (q -1 ) are defined as, where k p p ~y q -M +1 + · · · + k ~y q -1 + k ~y ~y (q -1 ) = k k p pM p2 p1 ~u q -1 + k ~u ~u (q -1 ) = k ~u q -M +1 + · · · + k k p pM p2 p1 As a result, the SPC control law in (A.1) can be rewritten as below, ~e (y (k ) - r(k + 1)) - k ~y (q -1 )y (k ) - k ~u (q -1 )u(k ) u(k + 1) = -k p p

(A.9) (A.10)

(A.11)

130

Appendix B

SPC Closed-loop Transfer Function
Recall the open-loop transfer function in (3.1) Gol (q -1 ) = and the SPC control law in (3.2) ~e (y (k ) - r(k + 1)) - k ~y (q -1 )y (k ) - k ~u (q -1 )u(k ) u(k + 1) = -k p p multiplying both sides of (B.1) by  = 1 - q -1 we have, y (k )D(q -1 ) = u(k )N (q -1 ) y (k )D(q -1 ) = u(k )N (q -1 ) y (k )D(q -1 ) = q -1 u(k + 1)N (q -1 ) therefore, u(k + 1) is obtained as follows, u(k + 1) = y (k ) By replacing (B.6) in (B.2) we have, y (k )
-1 D(q -1 ) ~e (y (k ) - r(k + 1)) - k ~y (q -1 )y (k ) - k ~u (q -1 )y (k ) D(q ) = - k p p q -1 N (q -1 ) N (q -1 )

N (q -1 ) y (k ) = u(k ) D (q -1 )

(B.1)

(B.2)

(B.3) (B.4) (B.5)

D(q -1 ) q -1 N (q -1 )

(B.6)

(B.7)

131

APPENDIX B. SPC CLOSED-LOOP TRANSFER FUNCTION after simplifying, the closed-loop system transfer function is derived as below, Gcl (q -1 ) = y (k ) q -1 N (q -1 ) = r(k + 1) Dcl (q -1 ) (B.8)

where Dcl (q -1 ) is the closed-loop characteristics equation as below, ~y (q -1 ) + k ~-1 D(q -1 )k ~u (q -1 ) + N (q -1 )] (B.9) ~-1 D(q -1 ) + q -1 [k ~-1 N (q -1 )k Dcl (q -1 ) = k p p e e e

132

Appendix C

Proof of Theorem 3.2.1
Theorem 3.2.1 is proven by applying small-gain root-locus analysis to (3.11). Recall (3.11) as follows, 1+ ~e N  (z -1 ) k =0  D(z -1 ) (C.1)

Considering its second part without integrator as below,
 -1 ~e N (z ) k D(z -1 )

(C.2)

by having (3.14), DC-gain of the system in (C.2) can be obtained as,
 ~e N (1) = k ~e Kol ~e N (1) = k k D(1) D(1)

(C.3)

~e and Kol . Therefore, to utilize the root-locus analysis, it is necessary to know the sign of both k The system in (C.2) can be rewritten as,
 -1 ~e (z - z1 )(z - z2 ) . . . (z - zn ) ~e N (z ) = k k D(z -1 ) (z - p1 )(z - p2 ) . . . (z - pn ) N  (z -1 ) . D(z -1 )

(C.4)

where pi s and zi s are poles and zeros of the compound system

From (C.2) we have,

 ~e N (1) = k ~e (1 - z1 )(1 - z2 ) . . . (1 - zn ) k D(1) (1 - p1 )(1 - p2 ) . . . (1 - pn )

(C.5)

133

APPENDIX C. PROOF OF THEOREM 3.2.1 Since, open-loop system is stable we have D(1) > 0, therefore, the denominator of (C.5) is positive. However, sign of the numerator needs to be determined. For complex conjugated zi s we have,
 (1 - zi )(1 - zi ) > 0,

if

zi = 1

(C.6)

for real zi s we have,   1 - zi > 0,  1 - z < 0, i if if zi < 1 zi > 1

(C.7)

Therefore, sign of the numerator in (C.5) can be determine as, ~e (1 - z1 )(1 - z2 ) . . . (1 - zn )] = sgn[k ~e (-1)Nz ] sgn[k (C.8)

where Nz is the number of real zi s that are greater than one. Therefore, from (C.3) and (C.8) we have,   Kol > 0,  K < 0, ol Rewrite the system with integrator in (3.11) as follows, ~e z (z - z1 )(z - z2 ) . . . (z - zn ) k =0 z - 1 (z - p1 )(z - p2 ) . . . (z - pn ) if if Nz even Nz odd

sgn[Kol ] = sgn[(-1)Nz ] 

(C.9)

1+

(C.10)

~e the roots of (C.10) are close to the open-loop poles {p1 , p2 , . . . , pn }. Since, the For small-gain k ~e = 0. open-loop poles are stable, therefore, roots of the system in (C.10) are stable for small-gain k Here, z = 1 is a simple pole and corresponding root-locus follows the real axis at the beginning. Since, the open-loop system is stable, total number of poles and zeros to the right side of z = 1 is equal to Nz . Therefore, according to (C.9): -If Nz is even, then Kol > 0 and positive-gain root-locus is plotted, therefore, roots of the SPC ~e > 0. close-loop system in (3.11) are inside the unit circle if k -If Nz is odd, then Kol < 0 and negative-gain root locus is plotted, therefore, roots of the system ~e < 0. in (3.11) are stable if k Therefore, the necessary and sufficient condition for small-gain stability of SPC closed-loop 134

APPENDIX C. PROOF OF THEOREM 3.2.1 system is, ~e Kol > 0 k (C.11)

135

Bibliography
[1] D.C. Karnopp and D.L. Margolis. Engineering Applications of Dynamics. John Wiley & Sons, 2008. [2] B.T. Kulakowski, J.F. Gardner, and J.L. Shearer. Dynamic Modeling and Control of Engineering Systems. Cambridge University Press, 2007. [3] M.J. Moran and H.N. Shapiro. Fundamentals of Engineering Thermodynamics. Wiley, 2008. [4] L. Ljung. System Identification: Theory for User. Prentice Hall, NJ, 1999. [5] T.P. Bohlin. Practical Grey-Box Process Identification: Theory and Application. Springer, 2006. [6] C.E. Garcia, D.M. Prett, and M. Morari. Model predictive control: Theory and practice; a survey. Automatica, 25:335­348, 1989. [7] J.H. Lee. Model predictive control: Review of the three decades of development. International Journal of Control, Automation and Systems, 9(3):415­424, 2011. [8] D.Q. Mayne. Model predictive control: Recent developments and future promise. Automatica, 50:2967­2986, 2014. [9] M. Morari and J.H. Lee. Model predictive control: Past, present and future. Computers and Chemical Engineering, 23(4):667­682, 1999. [10] S.J. Qin and T.A. Badgwell. A survey of industrial model predictive control technology. Control Engineering Practice, 11:733­764, 2003. [11] J.M. Maciejowski. Predictive Control with Constraints. Pearson Education Limited, 2002.

136

BIBLIOGRAPHY [12] L. Wang. Model Predictive Control System Design and Implementation using MATLAB. Springer-Verlag, 2009. [13] C.R. Cutler and B.L. Ramaker. Dynamic matrix control - a computer control algorithm. AIChE 86th National Meeting, Texas, 1979. [14] C.E. Garcia and A.M. Morshedi. Quadratic programming solution of dynamic matrix control (QDMC). Chemical Engineering Community, 46:073­087, 1986. [15] D.W. Clarke, C. Mohtadi, and P.S. Tuffs. Generalized predictive control - Part I. the basic algorithm. Automatica, 23(2):137­148, 1987. [16] D.W. Clarke, C. Mohtadi, and P.S. Tuffs. Generalized predictive control - Part II. extensions and interpretations. Automatica, 23(2):149­160, 1987. [17] J.B. Rawlings. Tutorial overview of model predictive control. IEEE Control Systems Magazine, 20:38­52, 2000. [18] W. Favoreel and B. De Moor. SPC: Subspace predictive control. Proceeding of the 14th IFAC World Congress [CD-ROM], H:235­240, 1999. [19] T. Katayama. Subspace Methods for System Identification. Springer-Verlag, London, 2005. [20] P.V. Overschee and B. De Moor. Subspace Identification of Linear Systems: Theory, Implementation, Application. Kluwer Academic Publisher, 1996. [21] R. Kadali, B. Huang, and A. Rossiter. A data driven subspace approach to predictive controller design. Control Engineering Practice, 11(3):535­561, 2003. [22] S.T. Navalkar, E.V. Solingen, and J.W.V. Wingerden. Wind tunnel testing of subspace predictive repetitive control for variable pitch wind turbines. IEEE Transactions on Control Systems Technology, 23(6):2101­2116, 2015. [23] J. Richalet. Industrial applications of model predictive based predictive control. Automatica, 29(5):1251­1274, 1993. [24] A.S. Kumar and Z. Ahmad. Model predictive control and its current issues in chemical engineering. Chemical Engineering Communication, 199(4):472­511, 2012.

137

BIBLIOGRAPHY [25] J. Zeng, C. Gao, and H. Su. Data-driven predictive control for blast furnace ironmaking process. Computers and Chemical Engineering, 34:1854­1862, 2010. [26] S. Jing, R. Errouissi, A. Al-Durra, and I. Boiko. A data-driven subspace predictive controller design for artificial gas-lift process. IEEE Conference on Control Application (CCA), pages 1179­1184, 2015. [27] G. Valverde and T. Van Cutsem. Model predictive control of voltages in active distribution networks. IEEE Transactions on Smart Grid, 4(4):2152­2161, 2013. [28] X. Wu, J. Shen, Y. Li, and K.Y. Lee. Data-driven modeling and predictive control for boilerturbine unit. IEEE Transaction on Energy Conversion, 28(3):470­481, 2013. [29] X. Luo. Data-driven predictive control for continuous-time linear parameters varying systems with application to wind turbine. International Journal of Control, Automation and Systems, 15(2):619­626, 2017. [30] J. Hu and P. Karava. Model predictive control strategies for buildings with mixed-mode cooling. Building and Environment, 71:233­244, 2014. [31] S.E. Shafiei, T. Knudsen, R. Wisniewski, and P. Andersen. Data-driven predictive direct load control of refrigeration systems. IET Control Theory and Applications, 9(7):1022­1033, 2015. [32] Y. Xia, W. Xie, B. Liu, and X. Wang. Data-driven predictive control for network control systems. Information Science, 235:45­54, 2013. [33] P. Wang, D. Zhu, and X. Lu. Active queue management algorithm based on data-driven predictive control. Telecommunication Systems, 64(1):103­111, 2017. [34] X. Lu, H. Chen, B. Gao, Z. Zhang, and W. Jin. Data-driven predictive gearshift control for dual-clutch transmissions and FPGA implementation. IEEE Transactions on Industrial Electronics, 62(1):599­610, 2015. [35] B. Kulcsar, J.W. wan Wingerden, J. Dong, and M. Verhaegen. Closed-loop subspace predictive control for hammerstein systems. Joint 48th IEEE Conference on Decision and Control and 28th Chinese Control Conference, pages 2604­2609, 2009.

138

BIBLIOGRAPHY [36] W. Lu, L. Ning, and L. Shao-Yuan. Performance monitoring of the data-driven subspace predictive control systems based on historical objective function benchmark. ACTA Automatica SINICA, 39(5):542­547, 2013. [37] D. Mayne and P. Falugi. Generalized stabilizing conditions for model predictive control. Journal of Optimization Theory and Application, 169:719­734, 2016. [38] C. Panjapornpon and M. Soroush. Shortest-prediction-horizon non-linear model-predictive control with guaranteed asymptotic stability. International Journal of Control, 80(10):1533­ 1543, 2007. [39] D.Q. Mayne, J.B. Rawlings, C.V. Rao, and P.O.M. Scokaert. Constrained model predictive control: Stability and optimality. Automatica, 30:789­814, 2000. [40] D. Viudez-Morerias. Extended predictive control: Stability and performance. Journal of Dynamic Systems, Measurement and Control, 137, 2015. [41] R. Estrada, A. Favela, A. Raimondi, and A. Nevado. Stable predictive control horizons. International Journal of Control, 85(4):361­372, 2012. [42] J.A. Rossiter, J.R. Gossner, and B. Kouvaritakis. Infinite horizon stable predictive control. IEEE Transactions on Automatic Control, 41(10), 1996. [43] P.O.M. Scokaert and D.W. Clarke. Stabilizing properties of constrained predictive control. IEE Proceeding on Control Theory and Application, 141(5):295­304, 1994. [44] K. Patan. Neural network-based model predictive control: Fault tolerance and stability. IEEE Transactions on Control Systems Technology, 23(3):1147­1155, 2015. [45] R. Scattolini and S. Bittanti. On the choice of the horizon in long-range predictive controlsome simple criteria. Automatica, 26(5):915­917, 1990. [46] J.O. Trierweiler and L.A. Farina. RPN tuning strategy for model predictive control. Journal of Process Control, 13:591­598, 2003. [47] A. Georgiou, C. Georgakis, and W.L. Luyben. Nonlinear dynamic matrix control for highpurity distillation columns. AIChE Journal, 34(8):1287­1298, 1988. [48] J.L. Garriga and M. Soroush. Model predictive controller tuning methods: A review. Industrial Engineering Chemical Research, 49:3505­3515, 2010. 139

BIBLIOGRAPHY [49] P. Bagheri and A. Khaki-Sedigh. An analytical tuning approach to multivariable model predictive controllers. Journal of Process Control, 24:41­54, 2014. [50] J. Yu and S.J. Qin. Statistical MIMO controller performance monitoring. Part I: data-driven covariance benchmark. Journal of Process Control, 18:277­296, 2008. [51] J. Yu and S.J. Qin. Statistical MIMO controller performance monitoring. Part II: performance diagnosis. Journal of Process Control, 18:297­319, 2008. [52] Q. Zhang and S. Li. Enhanced performance assessment of subspace model-based predictive controller with parameters tuning. The Canadian Journal of Chemical Engineering, 85:537­ 548, Aug. 2007. [53] X. Tian, G. Chen, and S. Chen. A data-based approach for multivariate model predictive control performance monitoring. Neurocomputing, 74:588­597, 2011. [54] R. Shridhar and D.J. Cooper. A tuning strategy for unconstrained multi-variable model predictive control. Industrial Engineering Chemical Res., 37(10):4003­4016, 1998. [55] R. Shridhar and D.J. Cooper. A tuning strategy for unconstrained SISO model predictive control. Industrial Engineering Chemical Res., 36(3):729­746, 1997. [56] J.L. Garriga and M. Soroush. Model predictive controller tuning via eigenvalue placement. American Control Conference, pages 429­434, 2008. [57] A.S. Yamashita, A.C. Zanin, and D. Odloak. Tuning the model predictive control of a crude distillation unit. ISA Transactions, 60:178­190, 2016. [58] K.Y. Rani and H. Unbehauen. Study of predictive controller tuning methods. Journal of Process Control, 33(12):2243­2248, 1997. [59] C. Muller, D.E. Quevedo, and G.C. Goodwin. How good is quantized model predictive control with horizon one? IEEE Transaction on Automatic Control, 56(11):2623­2638, 2011. [60] S. Valluri, M. Soroush, and M. Nikravesh. Shortest-prediction-horizon non-linear modelpredictive control. Chemical Engineering Science, 53(2):273­292, 1998. [61] J. Dong and M. Verhaegen. On the equivalence of closed-loop subspace predictive control with LQG. Proceeding of the 47th IEEE Conference on Decision and Control, pages 4085­4090, 2008. 140

BIBLIOGRAPHY [62] W. Favoreel, B. De Moor, M. Gevers, and P. Van Overschee. Closed-loop model-free subspacebased LQG-design. Proceeding of the 7th Mediterranean Conf. on Control and Automation (MED99), pages 1926­1939, June, 1999. [63] E. Zacokova, S. Privara, and M. Pcolka. Persistent excitation condition within the dual control framework. Journal of Process Control, 23:1270­1280, 2013. [64] E. Aggelogiannaki and H. Sarimveis. Multiobjective constrained MPC with simultaneous closed-loop identification. International Journal of Adaptive Control and Signal Processing, 20(4):145­173, 2006. [65] G. Marafioti, R.R. Bitmead, and M. Hovd. Persistently exciting model predictive control. International Journal of Adaptive Control and Signal Processing, 28(6):536­552, 2014. [66] R. Hallouzi and M. Verhaegen. Fault-tolerant subspace predictive control applied to a boeing 747 model. Journal of Guidance, Control and Dynamics, 31(4):873­883, 2008. [67] R. Hallouzi and M. Verhaegen. Reconfigurable fault tolerant control of a boeing 747 using subspace predictive control. Proceedings of the AIAA Guidance, Navigation and Control Conference and Exhibit, USA, 2007. [68] N.A. Mardi and L. Wang. Subspace-based model predictive control of time-varying systems. Joint 48th IEEE Conference on Decision and Control and 28th Chinese Conference, pages 4005­4010, 2009. [69] L. Zhang, S.Z. Xu, and H.T. Zhao. Adaptive subspace predictive control with time-varying forgetting factor. International Journal of Automation and Computing, 11(2):205­209, 2014. [70] R. Hallouzi and M. Verhaegen. Persistency of excitation in subspace predictive control. Proceedings of the 17th World Congress IFAC, 2008. [71] H. Akaike. Information theory and an extension of the maximum likelihood principle. Proceeding of the Second International Symposium Information Theory, pages 267­281, 1973. [72] J. Rissanen. Universal coding, information, prediction and estimation. IEEE Transactions on Information Theory, 30:629­636, 1984.

141

BIBLIOGRAPHY [73] S. Kritchman and B. Nadler. Non-parametric detection of number of signals: Hypothesis testing and random matrix theory. IEEE Transactions on Signal Processing, 57(10):3930­ 3941, 2009. [74] F. Gustaffson. Adaptive Filtering and Change Detection. Wiley, West Sussex, England, 2000. [75] S. Bjorklund and L. Ljung. A review of time-delay estimation techniques. American Control Conference (ACC), Washington, USA, pages 95­100, 2013. [76] J. Shalchian, A. Khaki-Sedigh, and A. Fatehi. A subspace based method for time delay estimation. Proceeding of the 4th International Symposium on Communication, Control and Signal Processing (ISCCSP), Cyprus, 2010. [77] N.K. Sinha and B. Kuszta. Modeling and Identification of Dynamic Systems. Van Nostrand Reinhold Company, USA, 1983. [78] T. Soderstrom and P. Stoica. System Identification. Prentice Hall, London, 1989. [79] K. Godfrey. Perturbation Signals for System Identification. Prentice Hall International, New York, 1993. [80] K. J. Astrom and B. Wittenmark. Computer Controlled Systems. Prentice Hall, New Jersey, 1984. [81] K. J. Astrom and T. Bohlin. Numerical identification of linear dynamic systems from normal operating records. IFAC Symposium on Self-Adaptive Systems, Teddington, England, 1965. [82] G.E. Box and G.M. Jenkins. Time Series Analysis, Forecasting and Control. Holden-Day, San Francisco, CA, 1970. [83] P. V. Kabaila and G. C. Goodwin. On the estimation of the parameters of an optimal interpolator when the class of interpolators is restricted. SIAM Journal on Control and Optimization, 18(2):121­144, 1980. [84] P.E. Wellstead. Non-parametric methods of system identification. Automatica, 17:55­69, 1981. [85] H. Rake. Step response and frequency response methods. Automatica, 16:519­526, 1980. [86] K. R. Godfrey. Correlation methods. Automatica, 16:527­534, 1980. 142

BIBLIOGRAPHY [87] L. Ljung. Prediction error estimation methods. Circuits, Systems and Signal Processing, 21(1):11­21, 2002. [88] T. Soderstrom and P. Stoica. Comparison of some instrumental variable methods: Consistency and accuracy aspects. Automatica, 17:101­115, 1981. [89] T. Soderstrom and P. Stoica. Instrumental Variable Methods for System Identification.

Springer-Verlag, New York, 1983. [90] M. Viberg. Subspace-based methods for the identification of linear time-invariant systems. Automatica, 31(12):1835­1851, 1995. [91] H. Akaike. A new look at the statistical model identification. IEEE Transactions on Automatic Control, 19:716­723, 1974. [92] G. Schwarz. Estimating the dimension of a model. The Annals of Statistics, 6(2):461­464, 1978. [93] S. Beheshti and M.A. Dahleh. LTI systems, additive noise and order estimation. IEEE Conference on Decision and Control, Hawaii USA, 2003. [94] S. Beheshti and M.A. Dahleh. A new information-theoretic approach to signal denoising and best basis selection. IEEE Transactions on Signal Processing, 53(10):3613­3624, 2005. [95] R. E. Kalman and B. L. Ho. Effective construction of linear state-variable models from input/output functions. Regelungstechnik, 14(12):545­548, 1966. [96] H. Akaike. Stochastic theory of minimal realization. IEEE Transactions on Automatic Control, AC-19(6):667­674, 1974. [97] P.V. Overschee and B. De Moor. N4SID: Subspace algorithms for the identification of combined deterministic-stochastic systems. Automatica, 30(1):75­93, 1994. [98] B. Huang and R. Kadali. Dynamic Modeling, Predictive Control and Performance Monitoring - A Data-driven Subspace Approach. Springer, 2008. [99] K. J. Astrom and B. Wittenmark. Adaptive Control. Addison-Wesley Longman Publishing, Boston, MA, USA, 1994.

143

BIBLIOGRAPHY [100] X. Luo and Y. Song. Adaptive predictive control: A data-driven closed-loop subspace identification approach. Abstract and Applied Analysis, 2014. [101] M. Verhaegen and P. Dewilde. Subspace model identification. Part I: the output-error statespace model identification class of algorithms. International Journal of Control, 56(5):1187­ 1210, 1992. [102] M. Verhaegen. Application of a subspace model identification technique to identify LTI systems operating in closed loop. Automatica, 29(4):1027­1040, 1993. [103] E. Fernandez-Camach and C.Bordons-Alba. Model predictive control in the process industry. 1995. [104] J. Richalet and D. O'Donovan. Predictive Functional Control: Principles and Industrial Applications. 2009. [105] D.J. Lamburn, P.W. Gibbens, and S.J. Dumble. Efficient constrained model predictive control. European Journal of Control, 20:301­311, 2014. [106] M. Verhaegen. Identification of the deterministic part of MIMO state space models given in innovations form from input-output data. Automatica, Special Issue on Statistical Signal Processing and Control, 30(1):61­74, 1994. [107] C.S. Chiu, K.Y. Lian, and P. Liu. Fuzzy gain scheduling for parallel parking a car-like robot. IEEE Transactions on Control Systems Technology, 13(6):1084­1092, 2005. [108] H. Kakigano, Y. Miura, and T. Ise. Distribution voltage control for DC micro-grids using fuzzy control and gain scheduling technique. IEEE Transactions on Power Electronics, 28(5):2246­ 2258, 2013. [109] P. Sarma. Multivariable gain-scheduled fuzzy logic control of an exothermic reactor. Engineering Applications of Artificial Intelligence, 14:457­471, 2001. [110] H. Yang, X. Li, Z. Liu, and L. Zhao. Robust fuzzy gain scheduling control for nonlinear systems subject to actuator saturation via delta operator approach. International Journal of Information Science, 272(C):158­172, 2014.

144

BIBLIOGRAPHY [111] K. Bedoud, M. Alirachedi, T. Bahi, and R. Lakel. Adaptive fuzzy gain scheduling of PI controller for control of the wind energy conversion systems. Energy Procedia, 74:211­225, 2015. [112] C.C. Huang and W.H. Yu. Applying a fuzzy gain-scheduled PID controller to dyebath ph. Textile Research Journal, 71(12):1074­1078, Dec. 2001. [113] Y. Kanthaphayao and V. Chunkag. Current-sharing bus and fuzzy gain scheduling of

proportional-integral controller to control a parallel-connected AC/DC converter. IET Power Electronic, 7(10):2525­2532, 2014. [114] A. Rodriguez-Martinez, R. Garduno-Ramirez, and L.G. Vela-Valdes. PI fuzzy gain-scheduling speed control at startup of gas-turbine power plant. IEEE Transactions on Energy Conversion, 26(1):310­317, 2011. [115] T. Chaiyatham and I. Ngamroo. Alleviation of power fluctuation in a microgrid by electrolyzer based on optimal fuzzy gain scheduling PID control. IEEJ Transactions on Electrical and Electronic Engineering, 9(2):158­164, 2014. [116] Z.W. Woo, H.Y. Chung, and J.J. Lin. A PID type fuzzy controller with self-tuning scaling factors. Fuzzy Sets and Systems, 115(2):321­326, 2000. [117] Z.Y. Zhao, M. Tomizuka, and S. Isaka. Fuzzy gain scheduling of PID controllers. IEEE Transactions on Systems, Man and Cybernetics, 23(5):1392­1398, 1993. [118] S. Bouallegue, J. Haggege, M. Ayadi, and M. Benrejeb. PID-type fuzzy logic controller tuning based on particle swarm optimization. Engineering Application of Artificial Intelligence, 25:484­493, 2012. [119] R. Eberhart and J. Kennedy. A new optimizer using particle swarm theory. Proceedings of the 6th International Symposium on Micro Machine and Human Science, pages 39­43, 1995. [120] M. Clerc and J. Kennedy. The particle swarm - explosion, stability, and, convergence in a multidimensional complex space. IEEE Transactions on Evolutionary Computation, 6(1):58­ 73, 2002. [121] Y. Shi and R. Eberhart. A modified particle swarm optimizer. IEEE International Conference on Evolutionary Computation, pages 69­73, 1998. 145

BIBLIOGRAPHY [122] C. Blum and D. Merkle. Swarm Intelligence, Introduction and Applications. Springer, Berlin, Germany, 2008. [123] S.H. Ling, K.Y. Chan, F.H.F. Leung, F. Jiang, and H. Nguyen. Quality and robustness improvement for real world industrial systems using fuzzy particle swarm optimization. Engineering Applications of Artificial Intelligence, 47:68­80, 2016. [124] Y. Marinakis, M. Marinaki, and G. Dounias. A hybrid particle swarm optimization algorithm for the vehicle routing problem. Engineering Applications of Artificial Intelligence, 23(3):463­ 472, 2010. [125] A.P. Engelbrecht. Computational Intelligence: An Introduction, 2nd Edition. John Wiley and Sons, 2007. [126] R. Mendes, J. Kennedy, and J. Neves. The fully informed particle swarm: Simpler, maybe better. IEEE Transactions on Evolutionary Computation, 8:204­210, 2004. [127] P. R. Belanger. On type 1 systems and the Clarke-Gawthrop regulator. Automatica, 19(1):91­ 94, 1983. [128] D.W. Clarke, M.A. D.Phil, and P.J. Gawthrop. Self-tuning controller. IEE Proceedings of the Institution of Electrical Engineering, 122(9):929­934, 1975. [129] S. Sedghizadeh and S. Beheshti. Stability and horizons tuning of subspace predictive control. submitted to: Automatica, 2017. [130] W. Favoreel, B. De Moor, M. Gevers, and P. Van Overschee. Model-free subspace-based LQG-design. Proceeding of the American Control Conference, pages 3372­3376, June, 1999. [131] L.Y. Li, T.T. Dong, S. Zhang, X.X. Zhang, and S.P. Yang. Time-delay identification in dynamic processes with disturbance via correlation analysis. Control Engineering Practice, 62:92­101, 2017. [132] V.D. Hajare, A.A. Khandekar, and B.M. Patre. Discrete sliding mode controller with reaching phase elimination for TITO systems. ISA Transactions, 66:32­45, 2017. [133] R.K. Wood and M.W. Berry. Terminal composition control of a binary distillation column. Chemical Engineering Science, 28(9):1707­1717, 1973.

146

BIBLIOGRAPHY [134] R. Hallouzi and M. Verhaegen. Subspace predictive control applied to fault-tolerant control. Fault Tolerant Flight Control, Springer-Verlag Berlin Heidelberg, LNCIS 399, pages 293­317, 2010. [135] S. Sedghizadeh and S. Beheshti. Fuzzy gain scheduling of subspace predictive controller. American Control Conference, pages 2693­2698, 2016. [136] E. Poulin, A. Pomerleau, A. Desbiens, and D. Hodouin. Development and evaluation of an auto-tuning and adaptive PID controller. Automatica, 32(1):71­82, 1996. [137] I. H. Altas. A fuzzy logic controlled tracking system for moving targets. Proceeding of the 112th IEEE International Symposium on Intelligent Control, pages 43­48, Jul. 1997. [138] Z. Muhammad, Z.M. Yusoff, N. Kasuan, M.N.N. Nordin, M.H.F. Rahiman, and M.N. Taib. Online tuning PID using fuzzy logic controller with self-tuning method. IEEE 3rd International Conference on System Engineering and Technology, pages 94­98, 2013. [139] A. Visioli. Tuning of PID controllers with fuzzy logic. IEEE Proceedings on Control Theory Application, 148(1):1­8, 2001. [140] S. Sedghizadeh and S. Beheshti. Particle swarm optimization based fuzzy gain scheduled subspace predictive control. submitted to: Engineering Applications of Artificial Intelligence, 2017. [141] L. Ljung. System identification toolbox for use with MATLAB. 1991. [142] S. Beheshti, S. Sedghizadeh, and A. Sahebalam. Reconstruction error (RE) based time delay estimation. submitted to: IEEE Signal Processing Letters, 2017. [143] S. Beheshti and S. Sedghizadeh. Number of source signal estimation by mean squared eigenvalue error (MSEE). submitted to: IEEE Transactions on Signal Processing, 2017. [144] S. Beheshti and M.A. Dahleh. Noisy data and impulse response estimation. IEEE Transactions on Signal Processing, 58(2):510­521, 2010. [145] S. Beheshti, M. Hashemi, E. Sejdic, and T. Chau. Mean square error estimation in thresholding. IEEE Signal Processing Letters, 18(2), 2011.

147

